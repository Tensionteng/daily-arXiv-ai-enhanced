<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 77]
- [cs.CL](#cs.CL) [Total: 39]
- [math.NA](#math.NA) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.SD](#cs.SD) [Total: 6]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 30]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Main category: cs.AI

TL;DR: 论文探讨了AI安全的重要性，提出了构建可信生态系统的框架，并通过2025年新加坡AI会议总结了AI安全研究的三大领域。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的快速提升，确保其安全、可靠和可信成为关键问题，需要建立可信的生态系统以促进创新并避免负面影响。

Method: 采用防御深度模型，将AI安全研究分为开发（Development）、评估（Assessment）和控制（Control）三大领域。

Result: 报告总结了国际AI安全研究的重点，并提出了具体的挑战和解决方案。

Conclusion: 构建可信的AI生态系统是确保AI安全的关键，需要国际合作和多领域研究支持。

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [2] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.AI

TL;DR: 论文研究了LLM代理在协作任务中是否理解上下文隐私，并评估了现有模型在隐私保护方面的表现。结果显示当前模型在隐私分类和多轮对话中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的普及，隐私保护在多代理协作任务中变得至关重要，但现有评估基准和模型能力存在不足。

Method: 提出了MAGPIE基准，包含158个高风险场景，评估LLM代理对上下文隐私的理解和协作能力。

Result: 当前模型（如GPT-4o和Claude-2.7-Sonnet）在隐私分类和多轮对话中表现不佳，多代理系统在71%的场景中无法完成任务。

Conclusion: 当前模型在上下文隐私保护和协作任务解决方面尚未达到理想水平。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [3] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Main category: cs.AI

TL;DR: 本文提出了一种动态上下文感知的提示推荐系统，用于领域特定的AI应用，通过结合上下文查询分析、检索增强的知识基础、分层技能组织和自适应技能排名，生成相关且可操作的提示建议。


<details>
  <summary>Details</summary>
Motivation: 由于LLM驱动的应用对用户提示质量高度敏感，而领域特定应用中高质量提示的编写往往具有挑战性，因此需要一种动态的提示推荐系统。

Method: 系统结合上下文查询分析、检索增强的知识基础、分层技能组织和自适应技能排名，利用行为遥测和两阶段分层推理过程动态选择和排名相关技能，并通过预定义和自适应模板生成提示。

Result: 在真实数据集上的实验表明，该方法在自动和专家评估中均表现出高实用性和相关性。

Conclusion: 该动态上下文感知提示推荐系统能有效提升领域特定AI应用中提示的质量和实用性。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [4] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Main category: cs.AI

TL;DR: 提出了一种框架，用于评估语言模型建议的宏观社会影响，并引入了一个间接危害场景数据集，以测试模型对长期安全性的意识。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在高风险社会决策中的影响增加，需要理解其建议的深远影响以确保其有益性。

Method: 提出了一个概念验证框架，用于模拟模型生成建议的宏观社会传播，并引入了一个包含100个间接危害场景的数据集。

Result: 在新数据集上实现了超过20%的改进，并在现有安全基准测试中平均胜率超过70%。

Conclusion: 该框架为构建更安全的语言模型代理提供了有前景的方向。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [5] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）的因果推理能力，发现其仅能进行浅层（level-1）推理，缺乏人类深层（level-2）能力。通过新基准CausalProbe-2024验证，并提出G^2-Reasoner方法提升LLMs的因果推理。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备类似人类的真实因果推理能力，发现其局限性并寻求改进方法。

Method: 分析LLMs的自回归机制，提出新基准CausalProbe-2024，并设计G^2-Reasoner方法结合通用知识和目标导向提示。

Result: LLMs在CausalProbe-2024上表现显著下降，G^2-Reasoner显著提升了其在新鲜和反事实情境中的推理能力。

Conclusion: LLMs当前仅支持浅层因果推理，G^2-Reasoner为迈向深层推理提供了新路径。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [6] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Main category: cs.AI

TL;DR: 论文提出WAP框架，通过四种认知能力增强LVLMs的环境理解，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在复杂场景中表现不佳，依赖环境无关的模仿学习，无法处理上下文敏感指令。

Method: WAP框架通过视觉外观建模、空间推理、功能抽象和语法接地四种能力，结合课程学习，仅使用原始视觉观察。

Result: 在EB-ALFRED基准测试中，Qwen2.5-VL任务成功率提升60.7，常识推理和长程规划分别提升60.0和70.0。

Conclusion: WAP显著提升LVLMs性能，开源模型优于GPT-4o和Claude-3.5-Sonnet。

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [7] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: IXAII是一个交互式可解释AI系统，整合了LIME、SHAP、Anchors和DiCE四种方法，提供定制化视图和用户控制，提升透明度和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法多为静态且忽视用户视角，限制了其有效性。

Method: 开发IXAII系统，整合四种解释方法，提供定制化视图和交互功能。

Result: 专家和普通用户认为IXAII通过多解释和可视化选项提升了透明度。

Conclusion: IXAII为可解释AI实践和人机交互提供了新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [8] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Main category: cs.AI

TL;DR: 论文提出AI驱动的科学发现需解决抽象、推理和现实三大差距，而非依赖模型规模或数据。提出主动推理AI系统架构，强调因果模型、贝叶斯规划、知识图谱和闭环验证的重要性，并指出人类判断不可或缺。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在科学发现中存在局限性，如架构限制、脆弱的推理机制及与实验现实的脱节。论文旨在通过解决三大差距推动AI驱动的科学进步。

Method: 提出主动推理AI系统，包括因果自监督基础模型、贝叶斯规划器、持久知识图谱及闭环验证机制，强调内部模型与外部验证的结合。

Result: 论文未明确具体实验结果，但提出了一种新型AI系统架构，旨在通过闭环交互和人类判断提升科学发现能力。

Conclusion: 科学发现的未来依赖于解决AI的三大差距，并构建结合内部模型与外部验证的主动推理系统，同时需人类判断作为永久组成部分。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [9] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Main category: cs.AI

TL;DR: TableMoE是一种专为多模态表格数据设计的神经符号混合专家架构，通过创新的路由机制和专家系统显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂表格结构（如视觉退化、嵌套布局）下表现不佳，需要更鲁棒的解决方案。

Method: 提出TableMoE，采用神经符号路由机制动态分配任务给专家系统，并引入大规模对齐数据集TableMoE-Align。

Result: TableMoE在多个WildStruct基准测试中显著优于现有模型，验证了其核心组件的有效性。

Conclusion: 神经符号推理的集成显著提升了多模态表格理解的鲁棒性和可解释性。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [10] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: MindCube基准测试揭示现有视觉语言模型（VLM）在构建空间心理模型方面表现接近随机。通过结合认知地图和推理链的协同方法，模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究VLM是否能像人类一样从少量视角想象完整场景，并构建空间心理模型。

Method: 使用MindCube基准测试，评估VLM在认知映射、视角转换和心理模拟方面的能力，并提出“先地图后推理”的协同方法。

Result: 通过协同方法和强化学习，模型准确率从37.8%提升至70.7%。

Conclusion: 构建和利用结构化空间表征能显著提升VLM对不可见空间的理解能力。

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [11] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 论文提出了Ad-Hoc Human-AI Coordination Challenge (AH2AC2)，通过人类代理解决Hanabi游戏中人类评估的高成本和难以复现问题，并开源了有限的人类游戏数据集。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理与人类无缝协调的挑战，尤其是Hanabi游戏中人类评估的高成本和难以复现问题。

Method: 开发人类代理作为评估伙伴，开源有限的人类游戏数据集，并在两到三人Hanabi场景中提供基线结果。

Result: 提出了AH2AC2挑战，开发了人类代理，并开源了3,079局游戏数据集。

Conclusion: AH2AC2为人类-AI协调研究提供了廉价、可复现的评估方法，推动了数据高效方法的发展。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [12] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: Mind2Web 2是一个包含130个高质量、长周期任务的基准测试，用于评估自主浏览网页和信息合成的代理搜索系统。提出了Agent-as-a-Judge框架，通过任务特定的评判代理自动评估答案正确性和来源归属。OpenAI Deep Research表现最佳，达到人类性能的50-70%。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法应对代理搜索的复杂性和开放性，需要新的基准和方法来支持下一代系统的发展。

Method: 构建Mind2Web 2基准，包含130个任务，提出Agent-as-a-Judge框架，基于树状评分设计自动评估答案。

Result: OpenAI Deep Research表现最佳，达到人类性能的50-70%，耗时仅为人类一半。

Conclusion: Mind2Web 2为下一代代理搜索系统的开发和评估提供了严格基础。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [13] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Main category: cs.AI

TL;DR: PsyLite是一个轻量级的心理咨询大语言模型代理，基于InternLM2.5-7B-chat开发，通过两阶段训练策略提升推理能力、心理咨询能力和对话安全性，并在资源受限环境中实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 现有AI心理咨询模型在对话安全、场景处理轻量级部署方面存在不足，PsyLite旨在解决这些问题。

Method: 采用混合蒸馏数据微调和ORPO偏好优化的两阶段训练策略，结合条件RAG引入幽默元素并拒绝危险请求。

Result: 在CEval、CPsyCounE和SafeDialBench评估中表现优异，心理咨询专业性提升47.6%，对话安全性提升2.4%，且仅需5GB内存。

Conclusion: PsyLite为资源受限环境中的心理咨询应用提供了可行解决方案。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Main category: cs.LG

TL;DR: SAFL是一种基于数据集大小特性的自适应联邦学习框架，通过实验揭示了数据集大小对联邦学习效果的影响，并提供了高效的通信和资源利用方案。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注模型异构性和聚合技术，忽视了数据集大小特性对训练动态的根本影响。

Method: 提出SAFL框架，基于数据集大小特性组织联邦学习，并在13个多模态数据集上进行实验评估。

Result: 发现数据集大小在1000-1500样本时效果最佳，结构化数据表现优于非结构化数据，SAFL平均准确率达87.68%，通信效率高。

Conclusion: SAFL填补了数据特性驱动联邦学习策略的空白，为实际部署提供了理论和实践指导。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [15] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: E-ABIN是一个用于生物网络中异常检测的可解释框架，结合了经典机器学习和图深度学习技术，通过用户友好平台提供高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大规模组学数据的增加需要能处理复杂基因表达数据并提供可解释结果的框架。

Method: E-ABIN结合支持向量机、随机森林、图自编码器和图对抗属性网络，提供统一平台。

Result: 在膀胱癌和乳糜泻案例中，E-ABIN成功检测到生物学相关异常并揭示疾病机制。

Conclusion: E-ABIN为生物网络异常检测提供了高效且可解释的解决方案。

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [16] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Main category: cs.LG

TL;DR: 论文提出了上下文-内容不确定性原理（CCUP），并通过分层计算框架推导出操作原则，展示了其在推理和学习中的效率提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大脑和机器如何通过递归的结构-特异性对齐来最小化不确定性。

Method: 方法包括建立分层计算框架，从基础熵最小化到四个层次的操作原则（核心推理约束、资源分配原则、时间引导动态和空间层次组合），并通过形式等价定理和计算模拟验证。

Result: 结果表明CCUP对齐的推理在效率上有显著提升，并提供了统一的理论基础。

Conclusion: 结论是大脑不仅是推理机器，还是通过路径依赖、内容引导的模拟来对齐结构和特异性的循环一致性熵梯度解析器。

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [17] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 提出了一种基于树的扩散模型推理对齐方法（DTS和DTS*），通过重用过去计算提高样本质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高噪声水平下估计不准确且计算效率低，需要改进。

Method: 将推理对齐视为搜索问题，引入树结构方法，通过传播终端奖励和迭代优化值估计。

Result: 在MNIST和CIFAR-10上，DTS计算效率提升10倍；在文本生成任务中，DTS*计算效率提升5倍。

Conclusion: DTS和DTS*通过重用信息提供了一种可扩展的推理对齐方法，显著提升计算效率和样本质量。

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [18] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Main category: cs.LG

TL;DR: 论文证明了FLIPD在现实假设下的正确性，并探讨了高斯卷积替换为均匀卷积时的类似结果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）能够学习低维支持分布，但FLIPD的理论基础在非仿射子流形下尚未完善。本文旨在填补这一理论空白。

Method: 通过形式化证明FLIPD在现实假设下的正确性，并研究高斯卷积替换为均匀卷积时的类似结果。

Result: 证明了FLIPD在现实假设下的正确性，并展示了均匀卷积下的类似结论。

Conclusion: FLIPD在更广泛的条件下有效，为LID估计提供了更坚实的理论基础。

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [19] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Main category: cs.LG

TL;DR: 研究探讨了测试时扩展技术在高级理论物理领域的泛化能力，提出了一种新的符号弱验证框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 验证从数学推理基准（如AIME）中学到的测试时扩展技术是否适用于高级理论物理领域。

Method: 开发了一种符号弱验证框架，利用物理问题的结构优化并行扩展结果。

Result: 新方法在TPBench物理数据集上显著优于现有测试时扩展技术，并在AIME上也表现优异。

Conclusion: 逐步符号验证是解决复杂科学问题的有效方法。

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [20] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 综述介绍了基础模型（FMs）在材料科学中的变革性作用，涵盖其应用领域、技术进展、工具及未来方向。


<details>
  <summary>Details</summary>
Motivation: 材料科学的研究挑战多样且跨领域，传统机器学习模型局限性大，FMs因其通用性和跨领域能力成为解决方案。

Method: 通过任务驱动的分类法，总结了六大应用领域，并讨论了单模态和多模态FMs及LLM代理的进展。

Result: FMs在材料科学中展现出潜力，但仍存在泛化性、可解释性、数据不平衡等局限性。

Conclusion: 未来研究方向包括可扩展预训练、持续学习、数据治理和可信性。

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [21] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Main category: cs.LG

TL;DR: 论文研究了微调后LLM中关系信息的存储与提取方式，提出了动态权重嫁接方法，发现信息通过提取和回忆两种路径处理。


<details>
  <summary>Details</summary>
Motivation: 探索微调后LLM如何存储和提取新学习的关系信息，填补现有定位方法的不足。

Method: 采用动态权重嫁接技术，比较微调与预训练模型的权重变化。

Result: 发现信息通过提取和回忆两种路径处理，部分任务需两者结合，部分仅需其一。

Conclusion: 揭示了微调信息处理的路径机制，为模型行为理解提供了新视角。

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [22] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Main category: cs.LG

TL;DR: 论文研究了在训练大型语言模型时使用块缩放精度格式（如MX格式）的挑战和可行性，发现其在较大计算规模下会导致损失函数的尖锐随机不稳定性，并提出通过调整精度方案来缓解问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决训练大型语言模型的高计算成本问题，研究低精度算术格式（如MX格式）在训练中的可行性和挑战。

Method: 通过近千个语言模型的训练实验，结合控制实验和消融研究，分析块缩放精度格式对训练稳定性的影响。

Result: 发现MX格式在较大计算规模下会导致训练不稳定，并提出通过调整精度方案可以缓解问题，某些混合配置能达到与全精度训练相当的性能。

Conclusion: 块缩放精度格式在训练中具有潜力，但需通过调整精度方案来避免不稳定性，混合配置是一种可行的解决方案。

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [23] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出了一种基于潜在分数的生成AI框架，用于学习非线性动力系统中的随机非局部闭合模型和本构关系，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决复杂多尺度动力系统建模的挑战，特别是在缺乏明确尺度分离的情况下，传统方法的确定性和局部假设过于受限。

Method: 联合训练卷积自编码器和条件扩散模型于潜在空间，降低采样维度并保留物理特性。

Result: 数值结果显示，联合训练方法在潜在空间中既保证了小的重构误差，又确保了扩散模型的良好性能。

Conclusion: 提出的潜在条件扩散模型框架在数值模拟中实现了显著的计算加速，同时保持了与物理空间标准扩散模型相当的预测精度。

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [24] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Main category: cs.LG

TL;DR: 论文提出了一种名为随机参数分解（SPD）的新方法，用于解决现有线性参数分解框架中APD方法的计算成本高和超参数敏感性问题。SPD更具可扩展性和鲁棒性，适用于更大、更复杂的模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法APD在计算成本和超参数敏感性方面存在不足，限制了其在大规模模型中的应用。

Method: 引入随机参数分解（SPD），通过改进计算效率和鲁棒性，实现对更大、更复杂模型的分解。

Result: SPD在实验中表现出更高的可扩展性，避免了参数收缩问题，并在玩具模型中更准确地识别真实机制。

Conclusion: SPD为线性参数分解方法的扩展提供了新途径，推动了机制可解释性研究的进一步发展。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [25] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM的自动化方法（GPU Kernel Scientist），用于迭代优化GPU内核，特别针对新架构如AMD MI300。


<details>
  <summary>Details</summary>
Motivation: 优化GPU内核通常需要深厚的架构知识和反复实验，尤其是在新架构或文档不足的情况下，传统方法效率低下。

Method: 采用多阶段进化过程：选择现有代码版本、生成优化假设、自动实现实验并通过外部评估系统验证。

Result: 由于性能竞赛数据暂未公开，论文主要展示了架构设计和工作流程，定性分析了LLM驱动的潜力。

Conclusion: LLM驱动的自动化方法有望在资源受限或快速发展的硬件环境中加速GPU内核优化。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [26] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Main category: cs.LG

TL;DR: 本文提出了一种基于FINN框架的方法，用于在FPGA上高效部署LSTM模型，解决了现有工具主要针对前馈网络的问题。


<details>
  <summary>Details</summary>
Motivation: LSTM在时间序列任务中表现优异，但其计算复杂度高，难以在资源受限环境中实时部署。FPGA虽适合高效AI加速，但现有工具对LSTM支持不足。

Method: 利用FINN框架和ONNX的Scan操作符建模LSTM的循环计算，支持混合量化和功能验证，并通过自定义转换将量化计算图映射到硬件块。

Result: 生成的量化ConvLSTM加速器在性能和资源消耗间取得平衡，推理精度与现有模型相当或更优。

Conclusion: 该方法为FPGA上资源高效的RNN加速器设计提供了通用解决方案。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [27] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: Hellsemble是一种新颖的集成框架，通过动态分配实例到不同难度的子集，提升分类性能并保持高效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统集成方法计算成本高且对异构数据适应性差，Hellsemble旨在解决这些问题。

Method: Hellsemble通过迭代将误分类实例传递给后续模型，形成专门的基础学习器，并利用路由模型分配新实例。

Result: 在OpenML-CC18和Tabzilla基准测试中，Hellsemble表现优于传统集成方法。

Conclusion: 基于实例难度的动态分配是构建高效稳健集成系统的有效方向。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [28] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 论文提出了一种新颖的对抗样本检测方法，通过分析攻击对不同DNN层的影响差异，训练轻量级回归模型预测深层特征，利用预测误差检测对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本防御方法要么专注于提升DNN鲁棒性，要么依赖次级模型检测对抗数据，但检测方法在实际应用中更具实用性。现有检测方法对最新攻击技术无效或计算效率低。

Method: 提出一种通用高效的方法，通过分析攻击对不同DNN层的影响差异，训练轻量级回归模型预测深层特征，利用预测误差检测对抗样本。

Result: 实验证明该方法高效、适用于实时处理，兼容任何DNN架构，并适用于图像、视频和音频等多领域。

Conclusion: 该方法在对抗样本检测中表现优异，计算高效且通用性强。

Abstract: Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [29] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 该论文研究了分布式图神经网络（GNN）在链接预测任务中的性能下降问题，提出了SpLPG方法，通过图稀疏化减少通信成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 分布式GNN框架在节点分类任务中表现良好，但在链接预测任务中的性能尚未充分探索，尤其是由于图分区和负采样方式导致的性能下降问题。

Method: 论文提出SpLPG方法，通过图稀疏化减少信息损失和通信成本，同时保持链接预测的准确性。

Result: 实验表明，SpLPG在多个真实数据集上减少了约80%的通信开销，同时基本保持了链接预测的准确性。

Conclusion: SpLPG是一种高效的分布式GNN训练方法，适用于链接预测任务，显著降低了通信成本。

Abstract: Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [30] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 提出了一种基于约束深度强化学习（CDRL）的方法，用于优化集成感知与通信系统中的时间资源分配，以提升目标通信质量。


<details>
  <summary>Details</summary>
Motivation: 解决集成感知与通信系统中雷达与通信单元的时间资源分配问题，以在动态环境中最大化通信质量。

Method: 采用约束深度强化学习（CDRL）框架，优化跟踪目标与数据传输的时间分配。

Result: 数值结果表明，CDRL框架在动态环境中能有效提升通信质量，同时满足时间约束。

Conclusion: CDRL方法为集成感知与通信系统提供了一种高效的时间资源分配解决方案。

Abstract: In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [31] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu,Subodh Kalia,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 论文研究了多功能认知雷达系统中的时间分配问题，通过深度强化学习找到帕累托最优解，比较了DDPG和SAC算法，SAC表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决认知雷达系统中新目标扫描与已检测目标跟踪之间的权衡问题。

Method: 将问题建模为多目标优化问题，采用深度强化学习（DDPG和SAC）寻找帕累托最优解，并使用NSGA-II算法估计帕累托前沿上界。

Result: DDPG和SAC均有效适应不同场景，SAC在稳定性和样本效率上优于DDPG。

Conclusion: 该研究为开发更高效、自适应的认知雷达系统提供了方法，能够平衡动态环境中的多目标竞争。

Abstract: The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [32] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 研究发现，LoRA微调显著降低了记忆风险，同时保持任务性能，与全微调相比表现出不同的记忆趋势。


<details>
  <summary>Details</summary>
Motivation: 探索微调（尤其是LoRA微调）中的记忆问题，以填补现有研究的空白。

Method: 使用基于相似性的记忆度量，比较LoRA微调和全微调的记忆风险。

Result: LoRA微调的记忆风险显著低于全微调，且不受模型规模和数据重复的显著影响。

Conclusion: LoRA微调是一种高效且低风险的微调方法，适用于大规模语言模型。

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [33] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang,Cole Ramos,Muhammad A. Awad,Keith Lowery*

Main category: cs.LG

TL;DR: Omniwise是一个端到端的自监督微调管道，首次将大型语言模型（LLMs）应用于GPU内核性能预测，无需执行代码或分析工具即可预测关键性能指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络的快速发展推动了人工智能的进步，但在性能分析领域仍缺乏高效的工具。Omniwise旨在填补这一空白，为开发者提供轻量级且高效的性能预测解决方案。

Method: Omniwise采用自监督微调方法，适用于任何模型，即使是小型3B参数模型也能取得良好效果。它直接从内核代码预测性能指标，如内存带宽、缓存命中率等。

Result: 在AMD MI250和MI300X架构上，Omniwise的预测误差在10%以内的准确率超过90%。此外，还开发了在线推理服务器和Visual Studio Code插件。

Conclusion: Omniwise为GPU内核性能预测提供了一种新颖且高效的解决方案，显著提升了开发者的工作效率。

Abstract: In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [34] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: 提出了一种轻量级的输出重加权遗忘方法RWFT，无需完全重新训练即可从分类器中删除特定类别，解决了现有遗忘方法在预测未学习类别时的不足。


<details>
  <summary>Details</summary>
Motivation: 强制执行用户删除权利和减少有害或偏见预测的需求，现有遗忘方法无法完全模拟重新训练模型的行为。

Method: 通过重新分配预测概率质量的方法，设计了一种对MIA-NN攻击鲁棒的遗忘技术，并引入总变差距离度量来量化残留泄漏。

Result: 实验表明，RWFT在现有评估指标和新提出的TV距离指标上均优于现有方法，分别提升了2.79%和111.45%。

Conclusion: RWFT是一种高效且安全的遗忘方法，能够在不完全重新训练的情况下有效删除特定类别。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [35] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出了一种新的多模型在线共形预测算法，通过动态选择有效模型子集来降低计算复杂度并缩小预测集大小，同时保证覆盖率和实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决多模型在线共形预测中因候选模型集过大或包含低效模型导致的计算复杂性和预测集过大的问题。

Method: 利用二分图反馈动态识别有效模型子集，并结合模型损失和预测集大小作为反馈，选择模型构建预测集。

Result: 实验证明该方法能构建更小的预测集，并在真实和合成数据集上优于现有方法。

Conclusion: 所提算法在保证覆盖率的同时，显著提升了效率和预测集质量。

Abstract: Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [36] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek,Guy Zamir,Yudong Chen*

Main category: cs.LG

TL;DR: 本文研究了平均奖励MDP中的离线强化学习，提出了基于目标策略的复杂性度量，并开发了一种新的算法，首次实现了完全单策略样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 平均奖励MDP中的离线强化学习在分布偏移和非均匀覆盖方面存在挑战，且理论视角相对不足。本文旨在解决这些问题。

Method: 提出了一种基于悲观折扣值迭代的算法，结合新的分位数裁剪技术，使用基于经验跨度的惩罚函数。

Result: 首次实现了完全单策略样本复杂度界限，并适用于一般弱通信MDP。

Conclusion: 本文展示了学习需要超越目标策略的平稳分布的覆盖假设，并通过下界验证了结果的紧致性。

Abstract: We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [37] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 本文提出了一种改进的LIME方法（DL-LIME），通过将深度学习融入采样过程，解决了传统LIME忽略特征相关性的问题，并在雷达资源管理中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在决策过程中表现出色，但其“黑箱”特性限制了可解释性。传统LIME方法在采样过程中忽略特征相关性，影响了解释效果。

Method: 提出DL-LIME方法，将深度学习融入LIME的采样过程，用于雷达资源管理的深度强化学习。

Result: DL-LIME在保真度和任务性能上均优于传统LIME，并揭示了雷达资源管理中决策的关键因素。

Conclusion: DL-LIME不仅提升了可解释性，还优化了任务性能，为雷达资源管理提供了更清晰的决策依据。

Abstract: Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [38] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng,Srivathsan Badrinarayanan,Janghoon Ock,Cheng-Kai Lai,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体LLM框架的化学过程优化方法，通过自主推断操作约束并协作优化，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在操作约束不明确或不可用时效率低下，依赖主观启发式方法。

Method: 采用多智能体框架（AutoGen），包括约束生成、参数验证、模拟执行和优化指导智能体，分两阶段进行自主约束生成和迭代优化。

Result: 在氢化脱烷基过程中，框架表现出与传统方法竞争的性能，计算效率更高，收敛速度提升31倍。

Conclusion: 该方法在操作约束不明确的新兴或改造过程中具有显著潜力。

Abstract: Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [39] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh,Pierre Le Bodic,Geoffrey I. Webb,Mario Boley*

Main category: cs.LG

TL;DR: 论文提出了一种扩展的符号规则集成方法，通过引入可学习的稀疏线性变换，提升模型的表达能力和准确性，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统基于简单阈值命题的规则集成方法依赖于精心设计的输入特征，否则需要增加规则数量和复杂性，从而降低可解释性。本文旨在通过引入可学习的稀疏线性变换，提升模型的表达能力。

Method: 扩展经典规则集成，引入可学习的稀疏线性变换（如$\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$），并提出基于迭代重加权逻辑回归的贪婪优化学习方法。

Result: 实验结果表明，该方法在保持与现有方法相同测试风险的同时，显著降低了模型复杂度。

Conclusion: 通过引入稀疏线性变换，该方法在保持可解释性的同时提升了模型的表达能力和准确性。

Abstract: Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [40] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei,Mehrdad Saberi,Abhilasha Ravichander,Soheil Feizi*

Main category: cs.LG

TL;DR: MSA算法通过利用模型检查点，高效估计并消除数据点对大型语言模型的影响，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中删除问题数据点（如隐私、版权或低质量数据）时完全重新训练的高计算成本问题。

Method: 提出MSA算法，利用模型检查点估计和消除特定数据点的影响。

Result: MSA在多个基准测试中优于现有算法，表明其能有效实现数据擦除。

Conclusion: MSA为大型语言模型提供了一种灵活且高效的数据擦除方法。

Abstract: Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [41] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen,Xiantao Cai,Jia Wu,Wenbin Hu*

Main category: cs.LG

TL;DR: AbMEGD是一个端到端框架，结合多尺度等变图扩散技术，用于抗体序列和结构的协同设计，显著提升了氨基酸恢复率和结构精度。


<details>
  <summary>Details</summary>
Motivation: 当前计算方法在捕捉几何特征和泛化新抗原界面方面存在局限，AbMEGD旨在解决这些问题。

Method: AbMEGD整合了多尺度等变图扩散技术，结合原子级几何特征和残基级嵌入，确保几何精度和计算效率。

Result: 实验显示，AbMEGD在氨基酸恢复率、改进百分比和CDR-H3区域的均方根偏差上均优于DiffAb模型。

Conclusion: AbMEGD在结构完整性和功能优化上取得了显著进展，为抗体设计设定了新标准。

Abstract: Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [42] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为SharpZO的混合方法，通过锐度感知预热训练提升零阶优化的视觉语言模型微调性能，显著提高准确性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有BP-free微调方法依赖高方差进化策略或零阶优化，性能不佳，无法满足边缘设备需求。

Method: SharpZO采用两阶段优化：锐度感知进化策略全局探索和平滑损失景观，随后稀疏零阶优化进行局部搜索。

Result: 在CLIP模型上实验显示，SharpZO比现有方法平均提升7%准确性。

Conclusion: SharpZO为边缘设备提供了一种高效、无需梯度的微调方案。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [43] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton,Valeriy Klyukin,Maksim Artemev,Denis Derkach,Nikita Orlov,Humphrey Shi*

Main category: cs.LG

TL;DR: 本文研究了通过知识蒸馏技术提升小型学生归一化流的采样质量和密度估计能力，探索了其在组合归一化流中的潜力。


<details>
  <summary>Details</summary>
Motivation: 显式密度学习器在生成模型中越来越受欢迎，但其训练难度大且采样质量较低。本文旨在通过知识蒸馏技术解决这些问题。

Method: 提出新颖的知识蒸馏技术，利用归一化流的独特性质，在中间层进行非传统的知识转移。

Result: 通过蒸馏，学生模型可以显著缩小规模，同时在性能上大幅提升，且较小的模型提高了吞吐量。

Conclusion: 知识蒸馏在组合归一化流中具有潜力，能够提升性能并减少模型规模。

Abstract: Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [44] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang,Mangal Prakash,Hehuan Ma,Jianyuan Deng,Yuzhi Guo,Amina Mollaysa,Tommaso Mansi,Rui Liao,Junzhou Huang*

Main category: cs.LG

TL;DR: TRIDENT是一个新颖的多模态框架，整合分子SMILES、文本描述和分类功能注释，通过全局和局部对齐目标学习分子表示，在11个下游任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了分子文本和分类信息，TRIDENT旨在利用这些信息提升分子表示学习。

Method: TRIDENT结合分子SMILES、文本和分类注释，采用体积对齐目标和局部对齐目标，动态平衡全局与局部对齐。

Result: 在11个下游任务中取得最优性能。

Conclusion: 结合SMILES、文本和分类注释能显著提升分子属性预测效果。

Abstract: Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [45] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu,Chongyang Zhao,Jason Xue,Lina Yao,Kristen Moore,Dong Gong*

Main category: cs.LG

TL;DR: MoRA提出了一种细粒度的混合排名自适应学习方法，通过稀疏激活和自适应的排名选择，解决了持续学习中任务干扰和冗余的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有LoRA-based Mixture-of-Experts方法在持续学习中的干扰、冗余和路由模糊问题。

Method: MoRA将每个排名-r更新分解为r个排名-1组件，作为独立专家，通过稀疏激活和自适应选择减少干扰和冗余。

Result: MoRA在CLIP和大型语言模型的持续学习任务中表现出色，显著减少遗忘并提升泛化能力。

Conclusion: MoRA是一种有效的持续学习方法，能够显著提升预训练模型的持续学习能力。

Abstract: Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [46] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习（FL）在概念漂移下的性能，提出了一种基于信息理论的算法来缓解性能下降，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据通常是动态的，而现有FL研究多基于静态数据集，导致性能下降。本文旨在解决FL在概念漂移下的适应性问题。

Method: 将概念漂移建模为马尔可夫链，提出“稳态泛化误差”评估模型能力，并通过KL散度和互信息推导其上界。提出一种基于KL散度和互信息的正则化算法。

Result: 实验结果表明，所提算法在三种漂移模式（周期性、渐变性、随机性）下均优于现有方法，验证了理论分析的正确性。

Conclusion: 本文提出的方法能有效适应FL中的概念漂移，并通过性能-成本权衡分析为实际应用提供了指导。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [47] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的数据选择方法RL-Selector，通过量化样本冗余性（epsilon-sample cover）动态优化选择策略，显著提升了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习依赖大规模数据集，但训练成本高且数据冗余严重，需要更高效的数据选择方法。现有方法忽视样本动态变化，无法全面优化。

Method: 引入epsilon-sample cover量化样本冗余性，将数据选择建模为强化学习问题，通过轻量级RL代理动态优化选择策略。

Result: 在多个基准数据集和架构上，RL-Selector优于现有方法，训练效率提升且模型泛化性能增强。

Conclusion: RL-Selector通过动态数据选择有效减少冗余，显著提升训练效率和模型性能，为数据高效训练提供了新思路。

Abstract: Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [48] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang,Sanghyeon Lee,Jeongmo Kim,Seungyul Han*

Main category: cs.LG

TL;DR: SSE是一种基于图的分层强化学习框架，通过强制单步子目标可达性和改进探索策略，解决了长时程目标任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 长时程目标任务在强化学习中面临奖励稀疏和目标遥远的挑战，现有方法存在子目标不可行和规划效率低的问题。

Method: SSE通过约束高层决策确保子目标可达性，采用解耦探索策略系统探索目标空间，并通过动态调整边成本优化路径规划。

Result: 在多种长时程基准测试中，SSE在效率和成功率上均优于现有方法。

Conclusion: SSE通过结构化约束和动态优化，显著提升了长时程目标任务的性能。

Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [49] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang,Ming Zhou,Shaopeng Zhai,Ying Sun,Hui Xiong*

Main category: cs.LG

TL;DR: 提出了一种基于遗憾感知的无监督技能发现方法，通过对抗性框架提升技能多样性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高维环境中效率不足，需改进技能发现的多样性和效率。

Method: 将技能发现建模为技能生成与策略学习的极小极大博弈，利用遗憾评分指导技能探索。

Result: 在多样性和效率上优于基线方法，高维环境中零样本性能提升15%。

Conclusion: 对抗性框架和遗憾感知机制有效提升了技能发现的性能。

Abstract: Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [50] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng,Ming Tang*

Main category: cs.LG

TL;DR: FedDAA是一个动态聚类联邦学习框架，旨在适应多源概念漂移并保留历史知识，显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注真实漂移，而忽略了虚拟漂移和标签漂移，导致历史知识丢失和性能下降。

Method: FedDAA包含三个模块：聚类数量确定、真实漂移检测和概念漂移适应，以区分并适应不同类型的漂移。

Result: 实验表明，FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100上比现有方法提升了7.84%至8.52%的准确率。

Conclusion: FedDAA通过明确区分和适应多源概念漂移，有效保留了历史知识，显著提升了联邦学习的性能。

Abstract: In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [51] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出一种利用知识图谱生成高质量指令数据的方法，显著提升大语言模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型（LLMs）的工具使用能力，解决现有方法生成指令数据质量不足的问题。

Method: 从知识图谱中提取查询路径，转化为用户查询，并将实体关系转化为可操作工具，生成高质量指令数据。

Result: 实验表明，仅需少量合成数据进行微调即可显著提升LLMs的工具使用能力和整体性能。

Conclusion: 知识图谱生成的指令数据能有效提升LLMs的工具使用能力，为未来研究提供了新方向。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [52] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan,Peng Wang,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: CHOOSE是一种基于浅层Transformer的无线符号检测框架，通过引入自回归潜在推理步骤，显著提升了浅层模型的推理能力，无需增加模型深度。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于ICL的Transformer模型因深度架构导致的存储和计算成本高的问题，适用于资源受限的移动设备。

Method: 提出CHOOSE框架，在隐藏空间中引入自回归潜在推理步骤，增强浅层Transformer（1-2层）的推理能力。

Result: 实验表明，CHOOSE在性能上优于传统浅层Transformer，并与深层Transformer相当，同时保持存储和计算效率。

Conclusion: CHOOSE为资源受限的无线接收器中实现Transformer算法提供了有前景的方向。

Abstract: Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [53] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato,Anna Monreale*

Main category: cs.LG

TL;DR: 论文提出了FeDa4Fair库，用于生成和评估联邦学习中公平性方法的数据集和基准，以解决客户端数据分布不均导致的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性问题因客户端数据分布不均而复杂化，现有方法多关注单一敏感属性，忽视了多样化的公平需求。

Method: 引入FeDa4Fair库，生成适用于评估公平性方法的异构数据集，并提供四个数据集和基准测试。

Result: 提供了可复现的公平性评估工具和数据集，支持全局和客户端层面的公平性研究。

Conclusion: FeDa4Fair为联邦学习中的公平性研究提供了标准化工具，促进了更全面的公平性评估。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [54] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot,Pietro Barbiero,Gabriele Dominici,Giuseppe Marra*

Main category: cs.LG

TL;DR: H-CMR是一种新型概念模型，通过层次化概念图提供概念和任务预测的可解释性，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有概念模型仅对最终任务预测提供可解释性，而概念预测本身仍为黑盒。H-CMR旨在解决这一局限性。

Method: H-CMR使用有向无环图建模概念间关系，通过神经注意力机制选择逻辑规则，层次化预测概念和任务。

Result: H-CMR在性能上达到最先进水平，同时支持概念和模型干预，提升推理准确性和训练效率。

Conclusion: H-CMR在保持高性能的同时，显著提升了概念和任务预测的可解释性和交互性。

Abstract: Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [55] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 提出了一种动态跳过Transformer中间层的新架构，但未能在计算效率与性能之间取得改进。


<details>
  <summary>Details</summary>
Motivation: 基于中间层冗余和早期层信息聚合的观察，试图通过动态跳过中间层来提升效率。

Method: 使用学习门控机制动态跳过对称的中间层块，并采用门控注意力机制防止跳过位置的关注。

Result: 在验证交叉熵与FLOPs的权衡上未优于密集基线。

Conclusion: 方法未达到预期效果，但代码已开源。

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [56] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi,Jun Xia,Yufei Huang,Jingbo Zhou,Siyuan Li,Yunfan Liu,Chang Yu,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出了一种基于双扩散隐式桥（DDIB）的框架，解决单细胞扰动数据未配对问题，结合基因调控网络（GRN）和掩码机制提升生成质量，并引入新评估指标。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序是破坏性过程，无法捕捉同一细胞在扰动前后的表型，导致数据未配对。现有方法或强行配对或忽略未扰动与扰动细胞间关系。

Method: 基于DDIB框架学习数据分布映射，整合GRN信息传播扰动信号，加入掩码机制预测沉默基因，并引入双条件扩散模型（Unlasting）。

Result: 有效解决未配对数据问题，提升生成质量，新评估指标更好地反映单细胞响应的异质性。

Conclusion: Unlasting框架在单细胞扰动数据分析中表现优异，结合GRN和掩码机制为生物医学研究提供新工具。

Abstract: Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [57] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 本文提出了一种抗脆弱的强化学习框架，通过动态选择策略来应对无人机导航中的对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法对分布外偏移的泛化能力有限，无法有效应对动态对抗攻击。

Method: 引入基于折扣汤普森采样的切换机制，动态选择多个鲁棒策略以最小化对抗性分布偏移。

Result: 在复杂导航环境中，该方法表现出更短的路径长度和更高的无冲突导航率。

Conclusion: 抗脆弱强化学习框架在对抗攻击下具有更强的适应性和性能优势。

Abstract: The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [58] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda,Adolfo Perrusquia,Weisi Guo*

Main category: cs.LG

TL;DR: 论文提出了一种抗脆弱强化学习框架，通过逐步增加的对抗扰动训练RL智能体，以应对观察空间中的分布外攻击，提升决策安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 在安全关键系统中，RL策略易受观察空间的分布外对抗攻击影响，导致决策不安全或次优。为解决这一问题，研究旨在设计一种能适应并抵御此类攻击的框架。

Method: 提出抗脆弱RL框架，通过模拟攻击者逐步增加观察空间扰动，利用Wasserstein距离最小化进行专家引导的批评对齐，确保价值函数分布的有界性。

Result: 在无人机避障场景中，抗脆弱策略显著优于标准及鲁棒RL基线，累积奖励提高15%，冲突事件减少30%。

Conclusion: 抗脆弱强化学习在动态威胁环境中具有理论和实践可行性，能提升决策的安全性和适应性。

Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [59] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出一种基于熵分类的高效微调方法，显著优于标准SFT，且数据需求减少62%。


<details>
  <summary>Details</summary>
Motivation: 通过减少对复杂数据的推理调用和降低数据需求，提高LLMs微调效率。

Method: 利用单标记答案熵分类数据，结合SFT和蒸馏进行微调。

Result: 平均准确率0.55，优于标准SFT（0.43），且数据需求减少62%。

Conclusion: 该方法在高效性和性能上均优于传统方法，为后续研究提供了新方向。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [60] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng,Yadan Luo,Liangyu Huo,Yaowei Wang,Xin Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的Norm-Aware Linear Attention机制，解决了线性注意力中查询范数被忽视和负值抑制的问题，通过解耦范数和方向恢复动态稀疏性和范数一致性。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然降低了复杂度，但忽视了查询范数导致熵差距，同时抑制了负值影响了内积交互。

Method: 解耦查询和键矩阵为范数和方向两部分，设计查询范数感知的核函数，并使用范数保持映射确保非负性。

Result: 实验表明NaLaFormer在视觉和语言任务上性能提升高达4.2%。

Conclusion: 提出的机制有效恢复了线性注意力的动态稀疏性和范数一致性，提升了表达能力和效率。

Abstract: Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [61] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Main category: cs.LG

TL;DR: DiLoCoX是一个低通信的大规模去中心化集群训练框架，用于训练超过1000亿参数的模型，显著提升了预训练速度和规模。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中高通信需求对集中式集群的依赖问题，探索在慢速网络上训练大模型的可能性。

Method: 结合Pipeline Parallelism、Dual Optimizer Policy、One-Step-Delay Overlap和Adaptive Gradient Compression Scheme。

Result: 在1Gbps网络上成功预训练107B模型，相比AllReduce提速357倍，且模型收敛几乎无退化。

Conclusion: DiLoCoX是首个成功应用于1000亿参数以上模型的去中心化训练框架。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [62] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang,Hongbin Wang,Tianwang Jia,Xingyi He,Siyang Li,Dongrui Wu*

Main category: cs.LG

TL;DR: DBConformer是一种双分支卷积Transformer网络，用于EEG解码，结合了时间和空间特征建模，显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有CNN-Transformer混合模型在EEG解码中存在局部与全局特征整合不足的问题，且缺乏显式的通道建模。

Method: 提出DBConformer，包含时间分支和空间分支，分别建模长程时间依赖性和通道间关系，并引入轻量级通道注意力模块。

Result: 在五个运动想象数据集和两个癫痫检测数据集上，DBConformer性能优于10个基线模型，参数更少且特征可解释。

Conclusion: DBConformer在性能和可解释性上表现优越，适用于稳健且可解释的EEG解码。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [63] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 提出了一种基于条件生成对抗网络（cGAN）的框架，用于生成能够逃避入侵检测系统（IDS）的隐蔽对抗攻击，并通过条件变分自编码器（CVAE）检测这些攻击。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用空域的广泛应用，传统异常检测方法难以识别新型威胁，且现有方法无法区分隐蔽对抗攻击与真实异常事件。

Method: 设计了一个多类IDS分类器，利用cGAN生成隐蔽对抗样本，并通过CVAE检测这些样本。

Result: CVAE的负对数似然方法在检测隐蔽对抗攻击方面显著优于传统的马氏距离检测器。

Conclusion: 强调了高级概率建模在增强IDS对抗生成模型攻击能力中的重要性。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [64] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Main category: cs.LG

TL;DR: 提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，通过全局和局部提示解决数据异质性问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据、计算和通信异质性方面面临挑战，现有方法仅依赖文本提示且忽略联合标签-域分布变化。

Method: pFedDC框架中，每个客户端维护全局和局部提示（视觉和语言模态），并通过交叉融合模块自适应整合提示。

Result: 在九种异质性数据集上的实验表明，pFedDC优于现有方法。

Conclusion: pFedDC通过双提示和交叉融合有效解决了联邦学习中的异质性问题，具有显著性能优势。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [65] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Latent Prototype Routing（LPR）的新路由框架，通过聚类视角重新审视专家路由，解决了MoE架构中的负载不平衡问题，显著提升了专家利用率和计算资源效率。


<details>
  <summary>Details</summary>
Motivation: 当前MoE系统中存在严重的负载不平衡问题，导致模型容量和计算资源利用不足，影响了性能。

Method: 采用聚类视角设计LPR路由框架，推广现有方法并促进专家负载均衡。

Result: 实验表明，LPR将专家负载的基尼系数从0.70降至0.035，最小-最大专家负载比从1e-6提升至0.70，实现了近乎完美的负载均衡。

Conclusion: LPR是一种高效的路由框架，能够显著改善MoE系统的负载均衡，同时不影响下游任务性能。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [66] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler,Florian Lemmerich*

Main category: cs.LG

TL;DR: 提出了一种基于线性性的神经网络压缩方法，通过合并线性行为神经元实现无损压缩，可将模型大小缩减至1/4。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络压缩方法主要通过测量重要性和冗余性减少参数，但已有优化方案仍有改进空间。

Method: 利用ReLU类激活函数中几乎总是激活的神经元行为线性的特性，提出合并后续层的线性性压缩方法。

Result: 实验表明，该方法在多数测试模型中实现了无损压缩至原始模型大小的1/4，且与重要性剪枝方法兼容。

Conclusion: 该方法为新型压缩技术奠定了基础，可实现更小、更高效的神经网络模型。

Abstract: In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [67] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson,Ola Engkvist,Jon Paul Janet,Christian Tyrchan,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 论文提出了一种基于行列式点过程的多样化小批量选择方法，用于强化学习中的实例评估，特别是在药物发现中，以提高解决方案的多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，评估实例的质量通常成本高昂且耗时，尤其是在强化学习中，需要与环境交互以提供奖励信号。多样化的小批量选择对避免模式崩溃至关重要。

Method: 使用行列式点过程（DPP）进行多样化小批量选择，并在药物发现中进行实验验证。

Result: 实验表明，该方法显著提高了解决方案的多样性，同时保持了高质量。

Conclusion: 该方法在药物发现中具有潜力，可以更快地满足未满足的医疗需求。

Abstract: In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [68] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Main category: cs.LG

TL;DR: 论文提出了一种名为ScalaBL的可扩展贝叶斯低秩适应方法，通过随机变分子空间推理，解决了大语言模型（LLMs）不确定性量化的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在幻觉和校准不足的问题，在高风险领域（如自主系统和医疗）中，量化其不确定性至关重要。现有方法因参数过多难以扩展到大模型。

Method: 在LoRA秩为r的r维子空间中进行贝叶斯推理，利用LoRA参数作为投影矩阵，将子空间样本映射到LLM的完整权重空间，使用随机变分推断学习所有参数。

Result: 尽管子空间维度低，ScalaBL在仅需约1000额外参数的情况下，性能与最先进方法相当，并成功扩展到迄今为止最大的贝叶斯LLM。

Conclusion: ScalaBL是一种高效且可扩展的方法，适用于大语言模型的不确定性量化，解决了现有方法的扩展性问题。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [69] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah,Axel Abels,Ann Nowé,Tom Lenaerts*

Main category: cs.LG

TL;DR: 研究探讨了在部分参与情况下，通过引入人工智能代表（Artificial Delegates）来提升持续性投票系统的公平性和代表性。


<details>
  <summary>Details</summary>
Motivation: 现有持续性投票规则依赖完全参与和完整偏好信息，但实践中部分参与是常态，因此需要解决缺席对公平性的影响。

Method: 研究通过引入偏好学习的人工智能代表，评估其在各种投票方法中对缺席影响的补偿效果。

Result: 研究发现缺席显著影响公平性，但人工智能代表能有效缓解这些影响，并在多种场景中提升系统的鲁棒性。

Conclusion: 人工智能代表是提升持续性投票系统公平性和代表性的有效工具。

Abstract: Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [70] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad,Aya Mrabah,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 提出了一种基于零样本学习和大语言模型的新方法，用于预测电子元件过时风险，解决了数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 电子元件过时问题导致成本增加和系统可用性中断，但缺乏可靠数据阻碍了准确预测。

Method: 利用零样本学习和大语言模型，从表格数据中提取领域知识进行风险预测。

Result: 在两个真实数据集上验证了方法的有效性，并比较了四种大语言模型的性能。

Conclusion: 选择合适的大语言模型对特定预测任务至关重要。

Abstract: Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [71] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière,Frédéric Cazals*

Main category: cs.LG

TL;DR: 论文提出了一种改进的随机种子选择方法，用于k-means聚类和k-GMM，通过前瞻性和多轮策略优化种子选择，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 重新审视随机种子选择技术，分析其关键要素，并提出改进方法以提升聚类和GMM拟合的效果。

Method: 利用前瞻性原则和多轮策略优化种子选择，增强种子与最终评估指标的一致性。

Result: 实验表明，新方法在最终指标（SSE和log-likelihood）上优于现有技术，且计算开销较小。

Conclusion: 新方法有望成为标准技术，并为理论研究开辟新方向。

Abstract: We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [72] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Main category: cs.LG

TL;DR: 本文提出了一种新型的图-时序卷积网络（AGTCNet），用于运动想象脑电图（MI-EEG）分类，显著提升了分类性能并减少了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有BCI系统在跨被试和跨会话的鲁棒性上表现不足，且难以捕捉多通道EEG信号的复杂时空依赖性。

Method: AGTCNet利用EEG电极的拓扑结构作为归纳偏置，结合图卷积注意力网络（GCAT）共同学习EEG的时空表征。

Result: AGTCNet在多个数据集上实现了最先进的性能，模型尺寸减少49.87%，推理时间缩短64.65%，分类准确率显著提升。

Conclusion: AGTCNet为BCI部署提供了一种高效且实用的解决方案，具有广泛的应用潜力。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [73] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li,Hao Sun,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Hong Xu,Jiaya Jia*

Main category: cs.LG

TL;DR: DynamicBench是一个新的基准测试，用于评估大语言模型（LLMs）在实时信息处理中的表现，通过双路径检索管道和领域特定知识，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试无法满足实时信息处理的需求，DynamicBench旨在填补这一空白。

Method: 采用双路径检索管道，结合网络搜索和本地报告数据库，并引入高级报告生成系统。

Result: 实验结果显示，DynamicBench在无文档和有文档辅助场景下分别超越GPT4o 7.0%和5.8%。

Conclusion: DynamicBench有效提升了LLMs在动态信息处理中的能力，代码和数据将公开。

Abstract: Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [74] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh,Mehmet Dik,Nguyen Viet Anh*

Main category: cs.LG

TL;DR: 论文证明了持久拉普拉斯算子的特征值在添加一个单纯形时的变化具有Lipschitz连续性，为谱拓扑数据分析提供了特征值层面的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 持久拉普拉斯算子的特征值是描述数据几何和拓扑特征的重要工具，但其在添加单个单纯形时的变化尚未明确，这对下游应用（如热核签名和谱神经网络）至关重要。

Method: 通过数学证明，建立了持久拉普拉斯算子特征值的Lipschitz界，表明特征值变化不超过单纯形边界欧几里得范数的两倍。

Result: 证明了特征值在局部更新时的稳定性，为动态数据环境中的误差控制提供了可靠保证。

Conclusion: 该结果为谱拓扑数据分析提供了首个特征值层面的鲁棒性保证，支持了其在动态数据中的可靠应用。

Abstract: Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [75] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff,Maya Varma,Ossian Rabow,Subathra Adithan,Julie Kim,Ken Chang,Hannah Lee,Nidhi Rohatgi,Christian Bluethgen,Mohamed S. Muneer,Jean-Benoit Delbrouck,Michael Moor*

Main category: cs.LG

TL;DR: 论文提出了SMMILE，首个专家驱动的多模态ICL医学任务基准，评估了15种MLLM模型，发现其在医学任务中的多模态ICL能力普遍较差，且易受无关示例和顺序偏差影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态ICL在医学领域的潜力，填补现有研究空白，尤其是针对临床任务中从有限示例中学习的需求。

Method: 开发SMMILE和SMMILE++基准，涵盖111个问题和1038个排列问题，由11位医学专家设计，评估15种MLLM模型的多模态ICL能力。

Result: 大多数模型在医学任务中表现中等或较差；ICL仅带来8%-9.4%的性能提升；无关示例和顺序偏差显著影响模型表现。

Conclusion: 当前MLLM模型在多模态医学任务中存在明显局限性和偏差，需进一步优化。

Abstract: Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [76] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman,Jing Bi*

Main category: cs.LG

TL;DR: rQdia通过增强图像正则化Q值分布，提升像素深度强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素深度强化学习中Q值分布不稳定的问题。

Method: 使用辅助损失函数（MSE）均衡Q值分布。

Result: 在MuJoCo和Atari任务中显著提升性能，样本效率和长期训练均有改进。

Conclusion: rQdia首次使无模型连续控制超越状态编码基线。

Abstract: rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [77] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon,Georgios Makris,Sotirios Xydis,Kiamal Pekmestzi,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 论文探讨了在低功耗DNN计算中，通过细粒度错误恢复与硬件近似技术的结合，提升能效。使用ROUP近似乘法器，在ResNet-8模型上验证，实现最高54%的能效提升，仅损失4%精度。


<details>
  <summary>Details</summary>
Motivation: 针对低功耗DNN计算的需求，研究如何通过硬件近似技术与DNN的细粒度错误恢复结合，提高能效。

Method: 采用ROUP近似乘法器，通过层、过滤器和内核级别的细粒度分布策略，评估其对精度和能效的影响。

Result: 在CIFAR-10数据集上，与基线量化模型相比，能效提升54%，精度损失仅4%；相比现有DNN近似方法，能效提升2倍且精度更高。

Conclusion: 提出的方法在低功耗DNN计算中实现了显著的能效提升，同时保持了较高的精度，优于现有近似技术。

Abstract: Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [78] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou,Tom Jacobs,Advait Gadhikar,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 论文提出NANOADAM方法，通过动态更新小幅度权重来优化微调过程，减少资源消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 微调大型预训练神经网络资源消耗高，通过分析梯度与权重关系，发现大幅梯度常与小幅度权重相关，从而提出优化方法。

Method: 提出NANOADAM，动态更新小幅度权重，无需梯度计算，保留大幅度权重以减少灾难性遗忘，并支持更大学习率。

Result: 在NLP和视觉任务中验证，NANOADAM能提升泛化性能并减少资源消耗。

Conclusion: NANOADAM通过优化权重更新策略，有效提升微调效率与性能。

Abstract: Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [79] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng,Bochuan Zhou,Yuping Song*

Main category: cs.LG

TL;DR: 本文提出了一种增强时间感知的图注意力网络（ATGAT），用于加密货币交易欺诈检测，通过多模块设计显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 加密货币交易欺诈检测面临复杂交易模式和严重类别不平衡的双重挑战，传统方法难以捕捉交易网络的时空依赖关系。

Method: ATGAT包含三个模块：(1) 多尺度时间差特征与周期性位置编码融合的时间嵌入模块；(2) 结构、时间和全局上下文注意力联合优化的三重注意力机制；(3) 加权BCE损失解决类别不平衡。

Result: 在Elliptic++数据集上，ATGAT的AUC达到0.9130，优于XGBoost（9.2%）、GCN（12.0%）和标准GAT（10.0%）。

Conclusion: ATGAT验证了时间感知和三重注意力机制对图神经网络的增强效果，为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广至其他时序图异常检测任务。

Abstract: Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [80] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 通过动态评估是否在每一层Transformer编码器后停止上下文学习，提出了一种早期停止方法，显著提升推理速度且性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 解决表格基础模型在推理时的高成本问题，尤其是针对较大数据集。

Method: 动态评估每一层Transformer编码器后是否停止上下文学习，并使用预训练的逐层解码器解码嵌入。

Result: 在34个小分类任务中，推理速度提升至1.3倍，性能损失可忽略；在5个更大分类任务中，速度提升至2.2倍。

Conclusion: 早期停止是一种有效且实用的策略，可显著提升表格上下文学习的效率。

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [81] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris,Isaac Lyngaas,John Lagregren,Mohamed Wahib,Larry York,Prasanna Balaprakash,Dan Lu,Feiyi Wang,Xiao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为D-CHAG的分布式跨通道分层聚合方法，用于处理多通道图像模态数据，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的科学基础模型在处理多源图像数据时存在计算密集的挑战，现有分布式方法未能完全解决。

Method: D-CHAG方法结合张量并行和模型分片，适用于任何模型并行策略和视觉Transformer架构。

Result: 在超光谱成像和天气预报任务中，D-CHAG实现了内存使用减少75%，并在1024个AMD GPU上使持续吞吐量翻倍。

Conclusion: D-CHAG为处理多通道图像数据提供了一种高效的计算方法，具有广泛的应用潜力。

Abstract: Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [82] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala,Cody Fleming*

Main category: cs.LG

TL;DR: 提出了一种名为SSCP的单步生成策略，通过增强的流匹配目标实现高效的单步动作生成，解决了传统生成模型在离线强化学习中的高推理成本和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型（如扩散和流匹配）在离线强化学习中表现优异，但迭代采样导致高推理成本和训练不稳定。SSCP旨在结合生成模型的表达能力和单模态策略的效率。

Method: SSCP通过增强的流匹配目标训练，预测从中间流样本直接完成动作的向量，实现单步动作生成。结合离线演员-评论家框架，避免长反向传播链。

Result: SSCP在离线、离线到在线及在线强化学习场景中均表现优异，速度和适应性优于扩散基线，并在目标条件强化学习中展现优势。

Conclusion: SSCP是一种高效、表达力强的生成策略框架，适用于深度强化学习和序列决策任务。

Abstract: Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [83] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon,Thomas Jack Samuels,Stephan Hau,Lennart Högman*

Main category: cs.LG

TL;DR: 研究探讨了多模态机器学习在检测双向互动中欺骗行为的有效性，结合语音和面部信息表现最佳，准确率达71%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证多模态数据（语音和面部信息）在检测欺骗行为中的效果，并探索双向互动中数据的整合方式。

Method: 采用早期和晚期融合策略，结合音频和视频数据（动作单元和注视信息），分析不同模态和参与者的组合。

Result: 多模态数据（语音和面部信息）结合双向参与者数据显著提升检测准确率，晚期融合策略表现最佳（71%）。

Conclusion: 研究支持心理学理论，为未来双向互动（如心理治疗）的研究奠定了基础。

Abstract: This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [84] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider,Simon Heilig,Asja Fischer,Timm Faulwasser*

Main category: cs.LG

TL;DR: 提出了一种基于最优控制问题的ResNet训练方法，通过惩罚隐藏状态的中间输出来优化网络，并展示了该方法可以自动剪枝不必要的深层残差层。


<details>
  <summary>Details</summary>
Motivation: 将最优控制理论与ResNet训练结合，提供一种理论支持的网络优化方法。

Method: 通过惩罚隐藏状态的中间输出（类似于最优控制中的阶段成本），利用跳跃连接和输出层传播状态。

Result: 训练动态使得不必要的深层残差层权重趋近于零，表明该方法可用于理论支持的层剪枝。

Conclusion: 该方法为ResNet提供了一种理论驱动的优化策略，具有自动剪枝潜力。

Abstract: We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [85] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud,Md Gulzar Hussain,Sumaiya Kabir,Hasnain Ahmad,Mahmudus Sobhan*

Main category: cs.LG

TL;DR: 提出了一种高效评估主观答题脚本的电子化解决方案，通过关键词提取和语法检查实现。


<details>
  <summary>Details</summary>
Motivation: 传统主观答题评估效率低且易受主观影响，需电子化解决方案提高准确性和效率。

Method: 集成系统提取答题脚本关键词，与开放和封闭域关键词对比，并检查语法和拼写错误。

Result: 在100名学生答题脚本测试中，系统精确度达0.91。

Conclusion: 该系统能高效评估主观答题脚本，具有高精确度和实用性。

Abstract: Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [86] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship,Luis Miralles-Pechuan,Stephen O'Sullivan*

Main category: cs.LG

TL;DR: 提出了一种结合遗传算法（GA）和强化学习（RL）的混合方法，用于优化低存储扩展稳定性Runge-Kutta（ESRK）方法，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决高精度、低存储ESRK方法在平衡准确性、稳定性和计算效率方面的挑战。

Method: 采用GA驱动的突变探索搜索空间，并结合RL动态优化启发式选择，实现参数减少和效率提升。

Result: 在基准测试中，最优启发式方法使IPOPT运行时间减少25%，同时保持数值稳定性和精度。

Conclusion: 该方法为高保真模拟提供了资源效率改进的新范式，并拓宽了低存储Runge-Kutta方法的应用范围。

Abstract: Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [87] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad,Vedanti Kshirsagar,Urvi Ranjan,Mahek Nakhua*

Main category: cs.LG

TL;DR: 论文提出了一种基于ML的分类模型，用于预测乳腺癌和宫颈癌的易感性，并结合地理位置提供治疗中心建议，旨在提高癌症意识和筛查率。


<details>
  <summary>Details</summary>
Motivation: 由于Telangana地区女性对乳腺癌和宫颈癌的筛查率极低（2020年分别为0.3%和3.3%），且缺乏相关意识和筛查知识，研究旨在通过技术手段提高癌症意识和早期检测率。

Method: 使用决策树分类和支持向量机分类算法分别预测宫颈癌和乳腺癌的易感性，并开发系统提供就近医院或癌症治疗中心的建议，同时整合健康卡记录医疗数据。

Result: 通过模型预测和系统支持，研究为提升癌症意识和筛查率提供了技术解决方案。

Conclusion: 该研究通过技术手段帮助提高癌症意识和筛查率，有望降低癌症死亡率并提升公众健康素养。

Abstract: According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [88] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale,Nicola Dall'Ora,Sebastiano Gaiardelli,Enrico Fraccaroli,Nicola Mazzocca,Franco Fummi*

Main category: cs.LG

TL;DR: 提出了一种无监督故障诊断方法，结合多变量时间序列分析、过程挖掘和随机模拟，用于CPS中的故障建模与分类。


<details>
  <summary>Details</summary>
Motivation: 手动建模故障行为需专业知识且复杂易错，需更高效、可解释的方法。

Method: 通过多变量时间序列检测异常，转化为事件日志，利用过程挖掘提取Petri网模型，并加入时间分布进行随机模拟。

Result: 在Robotic Arm Dataset上验证有效，能建模、模拟和分类故障行为，支持预测性维护和数字孪生。

Conclusion: 该方法为CPS故障诊断提供了高效、可解释的解决方案。

Abstract: Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [89] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou,Constantin Brif,Ismini Lourentzou*

Main category: cs.LG

TL;DR: mTSBench是当前最大的多元时间序列异常检测（MTS-AD）和无监督模型选择基准，覆盖19个数据集和12个领域，评估了24种方法，发现单一检测器无法在所有数据集上表现优异，且模型选择方法仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列异常检测在多个领域至关重要，但由于复杂的变量依赖、时间动态性和稀疏的异常标签，仍具挑战性。

Method: 引入mTSBench基准，涵盖344个标记时间序列，评估24种异常检测方法（包括基于LLM的检测器），并系统评估无监督模型选择技术。

Result: 结果显示无单一检测器在所有数据集上表现优异，且现有模型选择方法仍有显著不足。

Conclusion: mTSBench提供了统一的评估框架，支持可重复比较，推动自适应异常检测和鲁棒模型选择的未来发展。

Abstract: Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [90] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li,Chenrui Fan,Tianyi Zhou*

Main category: cs.LG

TL;DR: 研究发现大型语言模型（LLM）在预训练过程中会出现“顿悟”（grokking）现象，即测试性能在训练损失收敛后持续提升。通过分析模型内部动态，揭示了从记忆到泛化的转换机制，并提出两种新指标预测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练中“顿悟”现象的机制，特别是在大规模基础模型（如7B参数的OLMoE）预训练中的表现，以理解泛化能力的延迟出现。

Method: 在OLMoE的预训练过程中，计算训练损失并评估其在数学推理、代码生成等任务上的泛化能力，同时分析模型内部动态（如专家选择路径的变化）。

Result: 验证了“顿悟”现象在大规模预训练中仍然存在，且不同数据的“顿悟”阶段异步发生。发现路径结构化和复杂度降低是泛化能力提升的关键。

Conclusion: 研究揭示了“顿悟”现象的机制，提出的新指标能有效预测泛化能力，为预训练提供了实用工具，并理论上证明了结构化路径降低模型复杂度。

Abstract: Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [91] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 论文提出了LUCARIO基准和框架，用于处理表格数据的概率性问答，结合贝叶斯网络和大型语言模型，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答系统（如NL2SQL）在处理需要不确定性推理的概率性问题时表现不佳，因此需要新方法。

Method: 通过从表格中构建贝叶斯网络，将自然语言查询转化为概率查询，并利用大型语言模型生成最终答案。

Result: 实验结果表明，该方法显著优于基线，展示了符号-神经混合推理的优势。

Conclusion: 提出的框架在概率性表格问答中表现优异，证明了符号与神经方法结合的有效性。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [92] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 论文提出了跨语言功能基准测试（CL-GSM Symbolic和CL-IFEval），通过翻译现有英语功能基准模板到五种语言，评估多语言大模型的实用性能和鲁棒性。结果表明，某些静态基准更接近功能性能，且模型在不同语言间的鲁棒性差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准测试（如Belebele、M-MMLU和M-GSM）未能充分评估多语言大模型的实际性能和鲁棒性，因此需要创建更实用的跨语言功能基准。

Method: 通过将英语功能基准模板翻译为法语、西班牙语、印地语、阿拉伯语和约鲁巴语，构建了CL-GSM Symbolic和CL-IFEval两个跨语言功能基准。

Result: 结果显示，某些静态基准（如M-MMLU）与功能性能更接近，而其他基准（如M-GSM和Belebele）则表现较差。模型在不同语言间的鲁棒性差异显著，英语和阿拉伯语表现最稳定。

Conclusion: 跨语言功能基准能更全面地评估多语言大模型的实用性能和鲁棒性，揭示了现有静态基准的局限性。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [93] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: 研究发现，尽管LLM生成的研究想法在初期被认为更具新颖性，但在实际执行后，其评分显著下降，表明LLM生成的想法在执行效果上不如人类专家。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成的研究想法是否能在实际执行中产生更好的研究结果，而不仅仅是表面上的新颖性。

Method: 招募43位专家研究人员，随机分配执行LLM生成或人类专家提出的研究想法，并撰写4页短论文。所有项目由NLP专家盲审评分。

Result: 执行后，LLM生成想法的评分显著下降，人类专家想法的评分更高，揭示了LLM在生成有效研究想法上的局限性。

Conclusion: 当前LLM在生成真正有效的研究想法上存在局限，且缺乏执行结果时难以准确评估研究想法的质量。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [94] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: MultiFinRAG是一个针对金融QA的检索增强生成框架，通过多模态提取和动态检索策略，显著提升了复杂金融文档问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 金融文档通常包含多种模态（文本、表格、图像），传统LLMs和RAG方法在处理跨模态推理时存在局限性，如token限制和上下文碎片化。

Method: MultiFinRAG通过轻量级多模态LLM提取结构化JSON和文本摘要，结合模态感知检索和动态回退策略，实现跨模态推理。

Result: 在复杂金融QA任务中，MultiFinRAG比ChatGPT-4o（免费版）准确率高出19个百分点。

Conclusion: MultiFinRAG有效解决了金融文档多模态问答的挑战，且能在普通硬件上运行。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [95] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在道德模糊场景中的暴力倾向，发现其表面生成与内部偏好不一致，且暴力倾向因人口统计特征而异。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在检测和应对暴力内容时的能力，尤其是在道德模糊的真实场景中的表现。

Method: 使用经过验证的社会科学工具VBVQ，通过基于身份的提示评估六种LLMs的暴力倾向。

Result: LLMs的文本生成与内部暴力偏好不一致，且暴力倾向因人口统计特征而异，与现有研究相矛盾。

Conclusion: LLMs在暴力内容处理中存在潜在偏见，需进一步研究其道德推理能力。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [96] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 论文探讨了自动事实核查在医学领域的应用挑战，指出当前系统未被广泛使用的原因，并提出应将其视为交互式沟通问题而非端到端过程。


<details>
  <summary>Details</summary>
Motivation: 由于医学决策的高风险性和医学文献的复杂性，自动事实核查系统在医学领域的应用需求增长，但实际使用率低。

Method: 通过研究临床专家如何验证社交媒体上的真实医学声明，分析端到端事实核查在医学中的根本挑战。

Result: 揭示了医学事实核查的三大挑战：声明与科学证据的关联困难、模糊声明与意图不匹配、主观真实性标签。

Conclusion: 建议将事实核查视为交互式沟通问题，而非端到端过程，以更有效地应用于医学领域。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [97] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 该论文提出了一系列方法，旨在更高效地适应语言模型（LMs）到下游任务，包括利用未标记数据、参数高效微调和改进监督微调，显著提升了模型的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在NLP中表现出色，但其适应特定任务时仍面临效率低、鲁棒性差和计算成本高的问题，限制了其在实际任务中的应用。

Method: 论文提出三种方法：1）从未标记数据中提取任务相关知识；2）参数高效微调以减少计算成本；3）改进监督微调以提升模型在数据稀缺时的表现。

Result: 通过多种NLP任务的实验验证，这些方法显著提升了语言模型的鲁棒性、效率和泛化能力。

Conclusion: 这些进展为更鲁棒、高效的语言模型奠定了基础，推动了人工通用智能的发展。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [98] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 本文提出了一种基于FineWeb的多语言预训练数据集构建管道，能够自动适应任何语言，并通过实验验证其在多种语言上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大语言模型的预训练面临数据过滤和去重困难，本文旨在解决这一问题。

Method: 设计了一种自动化的数据管道，支持多语言，并通过实验验证其设计选择。

Result: 新管道构建的数据集在多语言任务中表现优于现有数据集，并提出了数据平衡方法。

Conclusion: 最终构建了FineWeb2数据集，覆盖1000多种语言，并公开了相关代码和数据。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [99] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: KaLM-Embedding-V2是一种多功能紧凑型嵌入模型，通过创新训练技术和数据优化，在通用文本嵌入任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升通用文本嵌入任务的性能，同时保持模型紧凑性。

Method: 采用双向Transformer架构和多阶段训练流程，包括预训练、微调和参数平均，并引入焦点式重加权机制和在线硬负样本混合策略。

Result: 在MTEB中英文基准测试中显著优于同类模型，甚至与更大模型竞争。

Conclusion: KaLM-Embedding-V2为紧凑型嵌入模型设定了新标准。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [100] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 论文探讨了通过梯度更新使微调模拟提示效果的方法，展示了梯度下降的表达能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过梯度更新使语言模型的微调能够模拟提示的效果，以提升模型的泛化能力和逻辑推理能力。

Method: 采用基于梯度的元学习方法，利用语言模型自身的提示预测作为目标，无需真实标签，通过梯度下降训练模拟提示效果。

Result: 实验表明，该方法在单次梯度更新后能部分或完全恢复提示模型的性能，尤其在“反转诅咒”任务和文本问答任务中表现显著。

Conclusion: 研究表明，适当的初始化下梯度下降具有强大的表达能力，为长上下文建模提供了新思路，并揭示了梯度学习的泛化潜力。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [101] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 该论文提出了一种扩展的机器性格清单（MPI），结合16PF模型和特定属性控制（SAC）框架，以更精细地控制和评估LLM的性格表达。


<details>
  <summary>Details</summary>
Motivation: 现有LLM性格模型依赖粗粒度的Big Five框架且缺乏强度控制机制，无法满足对更细腻、可控性格表达的需求。

Method: 扩展MPI以纳入16PF模型，开发SAC框架，通过形容词语义锚定和五个强度因子动态控制性格强度。

Result: 实验表明连续强度谱比二元切换更一致可控，且性格强度变化会系统影响相关特质。

Conclusion: 该方法为医疗、教育等领域提供了更细腻的人机交互途径，推动社会机器向更人性化发展。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [102] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 论文提出了一种基于分块的自监督学习（SSL）融合方法，结合CNN-BiLSTM框架，用于非母语者的流利度评估，显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 自动流利度评估（AFA）在捕捉非母语者的语音节奏、停顿和不流畅性方面仍具挑战性，需要更精细的时序分析和多模态特征融合。

Method: 采用分块策略（基于Silero-VAD），融合多种SSL模型（Wav2Vec2、HuBERT、WavLM）的互补优势，结合CNN-BiLSTM框架进行局部和长期依赖建模，并加入分块级流利度标记。

Result: 在Avalinguo和Speechocean762数据集上，F1分数分别提升4.2和2.8，Pearson相关系数分别提升4.0和6.2，优于单SSL模型和Pyannote.audio基线。

Conclusion: 分块多SSL融合方法在流利度评估中表现优异，但未来需进一步研究其在非规则韵律方言中的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [103] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 论文提出CA-Ben基准，评估LLMs在印度财务领域的表现，发现Claude 3.5 Sonnet和GPT-4o表现最佳，但数值计算和法律解释仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在财务领域的能力，填补印度财务背景下LLMs评估的空白。

Method: 使用CA-Ben基准（基于ICAI考试数据）评估六种LLMs，包括GPT-4o、LLAMA等。

Result: Claude 3.5 Sonnet和GPT-4o表现最优，数值计算和法律解释存在不足。

Conclusion: 需改进LLMs的混合推理和检索增强生成方法，以提升定量分析和法律解释能力。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [104] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 提出了一种半监督可扩展的统一框架（SSUF），通过知识增强、标签增强和结构增强模块解决电子商务查询分类中的信息不足和标签依赖问题。


<details>
  <summary>Details</summary>
Motivation: 电子商务查询通常简短且缺乏上下文，现有方法依赖用户点击行为导致马太效应，且缺乏统一框架。

Method: SSUF框架包含知识增强、标签增强和结构增强模块，模块可插拔，适应不同子任务。

Result: 离线与在线A/B实验显示SSUF显著优于现有模型。

Conclusion: SSUF通过统一框架和增强模块有效解决了查询分类中的问题，提升了性能。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [105] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 论文提出MT2-CSD数据集和LLM-CRAN模型，用于多目标多轮对话立场检测，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统立场检测研究局限于单实例分析，缺乏真实社交媒体交互动态的数据集，限制了多轮对话立场检测的发展。

Method: 引入MT2-CSD数据集，提出LLM-CRAN模型，利用大语言模型的推理能力提升对话理解。

Result: 实验表明LLM-CRAN在MT2-CSD数据集上显著优于基线模型。

Conclusion: MT2-CSD和LLM-CRAN为多轮对话立场检测提供了新工具，推动了该领域的发展。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [106] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: DALR方法通过双重对齐学习解决多模态句子表示中的跨模态偏差和模态内语义分歧问题，提升了表示质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在粗粒度上对齐图像和文本，存在跨模态偏差和模态内语义分歧，影响表示质量。

Method: 提出DALR，包含跨模态一致性学习模块（软化负样本并利用辅助任务语义相似性）和模态内排序蒸馏与全局对齐学习。

Result: 在STS和TR任务上验证了DALR的优越性，优于现有基线。

Conclusion: DALR通过精细的双重对齐学习有效提升了多模态句子表示质量。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [107] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: ComRAG是一个用于实时工业社区问答的检索增强生成框架，结合静态知识和动态历史问答对，显著提升性能并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用外部知识或动态历史问答上下文，且缺乏适合工业部署的记忆机制。

Method: 提出ComRAG框架，通过基于质心的记忆机制整合静态知识和动态历史问答对。

Result: 在三个工业CQA数据集上，ComRAG在向量相似度上提升25.9%，延迟降低8.7%-23.3%，块增长从20.23%降至2.06%。

Conclusion: ComRAG有效解决了工业CQA中的知识利用和实时性问题，性能显著优于基线方法。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [108] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: Progtuning是一种结合渐进学习的微调框架，通过逐步减少更新的Transformer块数量，优化资源分配并减少约25%的更新参数，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增大，全参数微调成本高昂，现有参数高效微调方法未能充分利用Transformer块的不均衡贡献，导致资源分配低效。

Method: 提出Progtuning框架，基于贡献逐步减少更新的Transformer块数量。

Result: Progtuning减少约25%的更新参数，优化资源分配，同时保持竞争力性能，并与参数高效微调方法高度适配。

Conclusion: Progtuning在资源效率和性能之间取得了良好平衡，适用于多种适应场景。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [109] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: Cosmos是一种基于压缩潜在空间的扩散模型，用于文本生成，解决了自回归模型的慢速解码和扩散模型的高维度问题，生成质量相当且推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型在文本生成中占主导地位，但其顺序性导致解码速度慢且全局一致性难以保持；扩散模型虽有并行生成优势，但高维度的标记级表示限制了其在文本生成中的应用。

Method: 提出Cosmos，通过在专门为扩散设计的压缩平滑潜在空间中操作，利用同时训练的自编码器实现标记级重建和与预训练语言编码器的对齐。

Result: 实验表明，Cosmos能将文本表示压缩8倍，生成质量与标记级扩散模型相当；增加潜在序列长度后，性能超越扩散和自回归基线模型。

Conclusion: Cosmos在多种生成任务中表现优异，生成质量相当或更优，推理速度提升2倍以上，为文本生成提供了高效替代方案。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [110] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文讨论了MTEB（大规模文本嵌入基准）的工程实践，以确保其可重复性和可扩展性，包括持续集成、数据集验证和社区贡献管理。


<details>
  <summary>Details</summary>
Motivation: MTEB已成为文本嵌入模型的标准评估平台，但需要确保其持续的可重复性和扩展性，以保持其相关性和质量。

Method: 通过设计稳健的持续集成流程、自动化测试执行、验证数据集完整性，并管理社区贡献和新任务的扩展。

Result: 这些工程实践使MTEB更全面且保持高质量，为机器学习评估框架的可重复性和可用性提供了范例。

Conclusion: 本文的经验为面临类似挑战的基准维护者提供了宝贵见解，MTEB的代码库已公开。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [111] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 提出了一种基于文本提示的动态控制对话轮换预测模型，结合Transformer架构，通过指令（如“更快”或“更平静”）动态调整预测行为，实验证明其有效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统缺乏直观且显式的动态控制能力，无法根据对话伙伴和上下文灵活调整轮换预测行为。

Method: 基于Transformer的语音活动投影（VAP）模型，通过文本提示嵌入到通道和跨通道Transformer中，利用LLM生成合成提示数据。

Result: 在950小时的人-人对话数据上验证，模型提高了预测准确性，并能根据文本提示动态调整轮换时机。

Conclusion: 该模型为对话系统提供了直观且动态的轮换控制能力，具有实际应用潜力。

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [112] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了一种基于检索的提示策略，利用句法相似性而非语义相似性在少样本设置下进行自动术语提取（ATE），并在多个领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在多种NLP任务中表现优异，但其在自动术语提取（ATE）中的应用尚未充分探索。本文旨在探索LLM在ATE中的潜力。

Method: 提出了一种基于检索的提示策略，通过句法相似性选择示例，以捕获术语边界，并在少样本设置下进行实验。

Result: 在三个专门的ATE基准测试中，句法检索方法显著提高了F1分数，尤其在跨领域设置中表现稳定。

Conclusion: 句法线索在LLM适应术语提取任务中具有重要作用，为未来研究提供了新方向。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [113] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出了Agent-RewardBench，一个评估多模态大语言模型（MLLMs）奖励建模能力的基准，解决了现有代理在自我纠正和泛化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs代理因缺乏外部反馈而难以自我纠正和泛化，需建立针对代理的奖励模型评估基准。

Method: 设计了Agent-RewardBench，包含多维度评估、步骤级奖励评估及高质量数据采样。

Result: 实验显示即使先进的多模态模型表现有限，突显了代理奖励建模专业训练的必要性。

Conclusion: Agent-RewardBench为代理奖励建模提供了有效评估工具，推动了该领域的发展。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [114] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 研究发现，尽管大型语言模型（LLM）能生成逼真的虚假文本，但简单分类器仍能有效检测，且模型规模的增加可能不会显著提升欺骗性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型生成虚假文本的能力是否会让检测变得困难，以及检测方法是否仍能保持有效性。

Method: 通过统计分类器检测古典侦探小说风格的虚假文本，比较不同版本模型的欺骗性变化。

Result: Gemini在0.5版本升级后欺骗性增强，而GPT未变化，表明检测虚假文本仍可行。

Conclusion: 即使模型规模扩大，可靠检测虚假文本仍可能实现，但新架构可能提升欺骗性。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [115] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: Double-Checker框架通过自我批判和迭代优化提升慢思考LLM的推理能力，显著提高AIME基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 慢思考LLM在生成批判性反馈和优化解决方案方面能力有限，需要一种方法增强其推理能力。

Method: 提出Double-Checker框架，通过微调1,730个自我批判实例，使LLM在推理中迭代批判和优化输出。

Result: Double-Checker将AIME基准测试的pass@1性能从4.4%提升至18.2%。

Conclusion: 自我批判迭代优化是提升LLM推理能力和可信度的有效方向。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [116] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 研究提出了一种轻量级模型，用于检测查询是否基于上下文文档，以减少LLMs的推理时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: LLMs在上下文信息不足时容易生成不可靠的回答，因此需要确保回答的groundedness（基于上下文）。

Method: 使用轻量级编码器模型（如RoBERTa和NomicBERT），在精选数据集上进行微调，以检测查询的groundedness。

Result: 这些轻量级模型在groundedness检测上的准确性与最先进的LLMs（如Llama3 8B和GPT4o）相当，同时显著降低了推理延迟。

Conclusion: 轻量级模型可以有效替代LLMs进行groundedness检测，显著提升效率和资源利用率。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [117] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本文探讨了仅使用文本的自回归语言模型从视觉对话中提取指代表达式的效果，发现仅依赖语言上下文也能有效完成任务，但任务本质是多模态的。


<details>
  <summary>Details</summary>
Motivation: 研究语言上下文是否足以检测视觉对话中具有视觉指涉的提及，并探索文本方法的有效性。

Method: 使用预训练的大型语言模型（LLM），通过下一个词预测标注提及范围，进行参数高效微调。

Result: 即使使用中等规模的LLM和小数据集，仅文本方法也能有效完成任务，表明语言上下文的重要性。

Conclusion: 任务本质是多模态的，单模态方法存在根本性限制。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [118] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 该论文提出了GLASS框架，基于Greimas符号学方阵（GSS），旨在提升大语言模型（LLMs）在深度文学分析中的能力，并提供了首个GSS文学批评数据集和定量评估指标。


<details>
  <summary>Details</summary>
Motivation: LLMs在理解和生成文本方面表现出色，但在对思想深刻、叙事复杂的作品进行专业文学批评方面仍有不足。

Method: 提出GLASS框架，基于GSS进行结构化分析，并构建了包含48部作品的GSS文学批评数据集，采用LLM-as-a-judge范式进行定量评估。

Result: 与专家批评和多部LLMs对比，GLASS表现出高性能，并成功应用于39部经典作品，填补了研究空白。

Conclusion: GLASS为文学研究和教育提供了基于AI的工具，揭示了文学参与的认知机制。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [119] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG是一个新框架，旨在提升实时检索增强生成（RAG）系统对复杂、模糊和多意图查询的鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统在处理噪声、模糊和多意图查询时表现不佳，主要因为训练或评估数据较干净。

Method: Omni-RAG通过三个模块改进RAG：深度查询理解与分解、意图感知知识检索、重排序与生成。

Result: 框架能够更有效地处理复杂查询，满足实际应用需求。

Conclusion: Omni-RAG填补了当前RAG能力与实际需求之间的差距，适用于开放域实时场景。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [120] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 提出了一种结合领域知识的LLM框架，用于检测动态平台上的欺诈对话和概念漂移，显著提高了分类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动态平台上语言模式演变和概念漂移导致欺诈检测困难，现有LLM在风险敏感场景中表现不佳。

Method: 提出DK-Enhanced LLM框架，包含欺诈检测模块、漂移检测单元和漂移分类模块，结合领域知识。

Result: 在SEConvo数据集上实现98%的分类准确率，显著优于零样本基线。

Conclusion: 领域知识和漂移感知显著提升了高风险NLP应用的性能、可解释性和鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [121] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 本文研究了基础大语言模型在多语言Text2Cypher任务中的表现，发现性能从高到低依次为英语、西班牙语和土耳其语，并探讨了提示翻译的影响。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言接口研究主要集中在英语，缺乏对其他语言的评估，本文旨在填补这一空白。

Method: 通过翻译英文问题创建多语言测试集，使用标准化提示和指标评估多个基础模型。

Result: 模型在英语上表现最佳，西班牙语次之，土耳其语最差；提示翻译对性能影响较小。

Conclusion: 研究强调了多语言查询生成中更包容的评估和开发的必要性，未来工作包括模式本地化和多语言微调。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [122] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出了一种新的偏好对齐框架，用于通过用户交互改进实时对话的语音对话模型。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法主要针对文本语言模型，不适用于实时语音交互的复杂动态（如打断、插话）和无明确说话人轮换分割的场景。

Method: 构建了包含15万对偏好标注的大规模数据集，利用离线对齐方法微调全双工自回归语音到语音模型。

Result: 实验表明，通用对话的反馈能有效改进语音对话模型，使其生成更准确、安全和上下文对齐的交互。

Conclusion: 研究发现，良好校准的动态平衡对自然实时语音对话系统至关重要。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [123] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: 论文提出了一种改进的Transformer架构，通过引入TopK激活函数，解决了稀疏自编码器（SAE）在语言模型分析中的局限性，提升了模型的解释性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAE）在分析Transformer语言模型时存在局限性，如特征不稳定性和后训练问题，影响了其内部有效性和实用性。

Method: 在Transformer架构中引入TopK激活函数，使隐藏状态等同于TopK SAE的潜在特征，无需后训练即可提供可解释性。

Result: TopK语言模型在模型大小、计算效率和解释性之间取得了良好平衡，保持了原始能力，同时支持神经元干预和跨检查点分析。

Conclusion: TopK语言模型为理解语言模型的学习和概念表示提供了稳定可靠的工具，有望推动模型解释性和可控性的研究。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [124] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

TL;DR: 研究比较了强化学习方法在微调大语言模型时，从离线到半在线再到完全在线模式的效果，发现在线和半在线方法表现相似且优于离线方法。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习方法在不同任务（可验证和不可验证）和不同训练模式（离线、半在线、在线）中的效果。

Method: 比较了在线和半在线的直接偏好优化（DPO）和组奖励策略优化（GRPO）目标，并分析了训练动态和超参数选择。

Result: 在线和半在线方法表现相似且优于离线方法，多任务联合训练可提升性能。

Conclusion: 在线和半在线方法在微调大语言模型时表现优异，多任务联合训练具有潜力。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [125] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

TL;DR: 论文提出了一种通过未来对话信号学习用户参与度的方法，利用用户反应作为奖励对齐交互式LLMs，并通过i×MCTS和DPO优化模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在社交对话中未能明确保证用户参与度，需更直接的方法提升交互质量。

Method: 采用i×MCTS模拟用户交互，收集高低质量对话数据，并通过DPO对齐模型。

Result: 在情感支持和说服对话场景中，模型显著提升了用户参与度。

Conclusion: 该方法有效提升了交互式LLMs在社交对话中的用户参与度。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [126] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 介绍了首个针对斯洛伐克自然语言理解（NLU）模型的综合基准测试skLEP，涵盖九项任务，并评估了多种预训练语言模型。


<details>
  <summary>Details</summary>
Motivation: 填补斯洛伐克语NLU评估工具的空白，推动该领域的研究发展。

Method: 通过整理新的斯洛伐克语数据集和翻译英语NLU资源构建skLEP，并评估多种语言模型。

Result: 发布了完整的基准数据、开源工具包和公开排行榜，促进可重复性和未来研究。

Conclusion: skLEP为斯洛伐克NLU研究提供了重要工具，有望推动该领域的进一步发展。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [127] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLM）在基准测试中的表现是否真实反映其能力，提出了“Potemkin理解”的概念，并通过两种方法量化其存在。


<details>
  <summary>Details</summary>
Motivation: 基准测试常用于评估LLM，但其有效性依赖于LLM是否以与人类相似的方式误解概念。若LLM的表现与人类理解不符，则可能仅是一种假象。

Method: 提出两种量化方法：一种基于专门设计的基准测试，另一种提供普遍适用的下限估计。

Result: 研究发现Potemkin理解在模型、任务和领域中普遍存在，且反映了概念表征的内部不一致性。

Conclusion: 基准测试可能无法真实反映LLM的理解能力，需更深入的方法评估其概念表征的连贯性。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [128] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 论文分析了用户通过LLMs（大型语言模型）寻求医疗信息的行为，构建了HealthChat-11K数据集，揭示了用户交互模式及其潜在风险。


<details>
  <summary>Details</summary>
Motivation: 研究用户如何通过LLMs获取医疗信息，探索其交互模式及潜在风险。

Method: 通过过滤大规模对话AI数据集，构建HealthChat-11K（11K真实对话），结合临床分类法分析21个医疗领域的用户交互。

Result: 揭示了用户寻求医疗信息的常见模式、不完整上下文、情感行为及可能导致迎合行为的交互（如引导性问题）。

Conclusion: LLMs在医疗支持能力上需改进，以减少潜在风险。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [129] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 论文提出数据效能（Data Efficacy）概念，通过优化训练数据的组织提升语言模型性能，并提出了DELT范式，包含数据评分、选择和排序。实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数据效率（如数据筛选和采样），而数据组织对性能的影响尚未充分探索。

Method: 提出DELT范式，包含数据评分（如LQS）、选择和排序（如FO），以优化数据组织。

Result: DELT在不增加数据量和模型规模的情况下提升了性能，LQS和FO组合效果最佳。

Conclusion: 数据效能是语言模型训练中具有潜力的基础研究方向，可与数据效率结合应用。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [130] [On Uniform Weighted Deep Polynomial approximation](https://arxiv.org/abs/2506.21306)
*Kingsley Yeon,Steven B. Damelin*

Main category: math.NA

TL;DR: 该论文提出了一种加权深度多项式逼近方法，用于处理具有不对称行为的函数，并通过数值实验验证其优于传统逼近方法。


<details>
  <summary>Details</summary>
Motivation: 传统有理逼近方法对非光滑或奇异函数具有根指数收敛性，而多项式逼近仅具有代数收敛性。本文旨在通过加权深度多项式逼近方法提升逼近效率。

Method: 引入一类加权深度多项式逼近器，通过将可学习的深度多项式与单侧权重相乘，捕捉局部非光滑性和全局增长性。

Result: 数值实验表明，该方法在相同参数数量下优于泰勒、切比雪夫和标准深度多项式逼近方法。

Conclusion: 加权深度多项式逼近方法在非光滑函数逼近中具有显著优势，并通过稳定的图参数化策略实现优化。

Abstract: It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [131] [Efficacy of Temporal Fusion Transformers for Runoff Simulation](https://arxiv.org/abs/2506.20831)
*Sinan Rasiya Koya,Tirthankar Roy*

Main category: physics.geo-ph

TL;DR: TFT在降雨径流建模中略优于LSTM，尤其在模拟水文过程的中段和峰值时表现更好，且能处理更长序列。但两者在Caravan数据集上性能均下降，可能因数据质量问题。


<details>
  <summary>Details</summary>
Motivation: 探索TFT在降雨径流建模中是否优于LSTM，并评估其在不同数据集和流域属性下的表现。

Method: 训练10个随机初始化的TFT和LSTM模型，应用于531个美国CAMELS流域及Caravan数据集的5个子集（美国、澳大利亚、巴西、英国、智利）。

Result: TFT略优于LSTM，尤其在模拟水文过程的中段和峰值时；TFT能处理更长序列，适合更大流域。但两者在Caravan数据集上性能显著下降。

Conclusion: TFT在改进水文建模和理解方面具有潜力，但需注意数据质量问题。

Abstract: Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [132] [scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection](https://arxiv.org/abs/2506.20697)
*Zhen Yuan,Shaoqing Jiao,Yihang Xiao,Jiajie Peng*

Main category: q-bio.CB

TL;DR: scMamba是一种无需预先特征选择即可整合单细胞多组学数据的基础模型，通过基于补丁的细胞标记化策略和对比学习，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单细胞多组学技术的发展提供了对细胞身份和疾病机制的新见解，但现有方法因特征选择可能丢失关键信息。

Method: scMamba采用基于补丁的细胞标记化策略，将基因组区域视为标记，并结合对比学习和余弦相似性正则化。

Result: scMamba在保留生物变异、对齐组学层和下游任务（如聚类和细胞类型注释）上显著优于现有方法。

Conclusion: scMamba是处理大规模单细胞多组学数据的强大工具，有望推动生物学发现。

Abstract: The advent of single-cell multi-omics technologies has enabled the
simultaneous profiling of diverse omics layers within individual cells.
Integrating such multimodal data provides unprecedented insights into cellular
identity, regulatory processes, and disease mechanisms. However, it remains
challenging, as current methods often rely on selecting highly variable genes
or peaks during preprocessing, which may inadvertently discard crucial
biological information. Here, we present scMamba, a foundation model designed
to integrate single-cell multi-omics data without the need for prior feature
selection while preserving genomic positional information. scMamba introduces a
patch-based cell tokenization strategy that treats genomics regions as words
(tokens) and cells as sentences. Building upon the concept of state space
duality, scMamba distills rich biological insights from high-dimensional,
sparse single-cell multi-omics data. Additionally, our novel contrastive
learning approach, enhanced with cosine similarity regularization, enables
superior alignment across omics layers compared to traditional methods.
Systematic benchmarking across multiple datasets demonstrates that scMamba
significantly outperforms state-of-the-art methods in preserving biological
variation, aligning omics layers, and enhancing key downstream tasks such as
clustering, cell type annotation, and trajectory inference. Our findings
position scMamba as a powerful tool for large-scale single-cell multi-omics
integration, capable of handling large-scale atlases and advancing biological
discovery.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [133] [Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG](https://arxiv.org/abs/2506.20683)
*Alexander Selivanov,Philip Müller,Özgün Turgut,Nil Stolt-Ansó,Daniel Rückert*

Main category: eess.IV

TL;DR: PTACL是一种多模态对比学习框架，通过整合CMR的时空信息增强ECG表征，提升心脏功能评估能力。


<details>
  <summary>Details</summary>
Motivation: ECG无法直接测量心脏功能参数，而CMR虽精确但昂贵且不易获取，因此需要一种方法弥补这一差距。

Method: PTACL结合全局患者级和局部时间级对比损失，对齐ECG和CMR的表征，无需新增可学习权重。

Result: 在UK Biobank的27,951名受试者数据上，PTACL在患者检索和心脏功能参数预测任务中表现优于基线方法。

Conclusion: PTACL展示了通过ECG增强非侵入性心脏诊断的潜力。

Abstract: An electrocardiogram (ECG) is a widely used, cost-effective tool for
detecting electrical abnormalities in the heart. However, it cannot directly
measure functional parameters, such as ventricular volumes and ejection
fraction, which are crucial for assessing cardiac function. Cardiac magnetic
resonance (CMR) is the gold standard for these measurements, providing detailed
structural and functional insights, but is expensive and less accessible. To
bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive
Learning), a multimodal contrastive learning framework that enhances ECG
representations by integrating spatio-temporal information from CMR. PTACL uses
global patient-level contrastive loss and local temporal-level contrastive
loss. The global loss aligns patient-level representations by pulling ECG and
CMR embeddings from the same patient closer together, while pushing apart
embeddings from different patients. Local loss enforces fine-grained temporal
alignment within each patient by contrasting encoded ECG segments with
corresponding encoded CMR frames. This approach enriches ECG representations
with diagnostic information beyond electrical activity and transfers more
insights between modalities than global alignment alone, all without
introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data
from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL
achieves better performance in two clinically relevant tasks: (1) retrieving
patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac
function parameters, such as ventricular volumes and ejection fraction. Our
results highlight the potential of PTACL to enhance non-invasive cardiac
diagnostics using ECG. The code is available at:
https://github.com/alsalivan/ecgcmr

</details>


### [134] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 论文提出了一种名为U-R-Veda的深度学习模型，结合卷积变换、视觉变换器、残差连接、通道和空间注意力以及边缘检测，用于心脏磁共振图像的语义分割，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 自动化精确的心脏图像分割是诊断和管理心脏疾病的关键步骤，现有方法在准确性和信息保留方面存在不足。

Method: U-R-Veda模型整合了卷积块、视觉变换器、通道和空间注意力机制，以及边缘检测的跳跃连接，以减少信息丢失并提升特征提取能力。

Result: 模型在DSC指标下平均准确率达到95.2%，优于其他模型，尤其在右心室和左心室心肌的分割上表现突出。

Conclusion: U-R-Veda模型显著提升了心脏磁共振图像的语义分割效果，为医学图像分析提供了更准确的工具。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


### [135] [A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation](https://arxiv.org/abs/2506.21162)
*Shuwei Xing,Derek W. Cool,David Tessier,Elvis C. S. Chen,Terry M. Peters,Aaron Fenster*

Main category: eess.IV

TL;DR: 提出了一种将3D超声（US）整合到标准消融工作流程的新框架，包括一种临床可行的2D US-CT/MRI配准方法，并通过多模态图像可视化技术验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 3D超声在肝肿瘤消融中具有显著优势，但肿瘤识别问题阻碍了其广泛应用，因此需要一种有效的整合方法。

Method: 提出了一种利用3D超声作为中介的2D US-CT/MRI配准方法，并开发了多模态图像可视化技术进行验证。

Result: 2D US-CT/MRI配准的标记距离误差为2-4 mm，运行时间为0.22秒/图像对；非刚性配准比刚性配准减少了40%的平均对齐误差。

Conclusion: 该框架提升了3D超声在肿瘤消融中的应用潜力，有望扩展其在临床治疗中的作用。

Abstract: 3D ultrasound (US) imaging has shown significant benefits in enhancing the
outcomes of percutaneous liver tumour ablation. Its clinical integration is
crucial for transitioning 3D US into the therapeutic domain. However,
challenges of tumour identification in US images continue to hinder its broader
adoption. In this work, we propose a novel framework for integrating 3D US into
the standard ablation workflow. We present a key component, a clinically viable
2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to
reduce registration complexity. To facilitate efficient verification of the
registration workflow, we also propose an intuitive multimodal image
visualization technique. In our study, 2D US-CT/MRI registration achieved a
landmark distance error of approximately 2-4 mm with a runtime of 0.22s per
image pair. Additionally, non-rigid registration reduced the mean alignment
error by approximately 40% compared to rigid registration. Results demonstrated
the efficacy of the proposed 2D US-CT/MRI registration workflow. Our
integration framework advanced the capabilities of 3D US imaging in improving
percutaneous tumour ablation, demonstrating the potential to expand the
therapeutic role of 3D US in clinical interventions.

</details>


### [136] [Exploring the Design Space of 3D MLLMs for CT Report Generation](https://arxiv.org/abs/2506.21535)
*Mohammed Baharoon,Jun Ma,Congyu Fang,Augustin Toma,Bo Wang*

Main category: eess.IV

TL;DR: 本文系统研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、LLMs和微调技术，用于3D CT报告生成。通过两种知识增强方法，性能提升10%，在MICCAI 2024挑战赛中排名第二。结果表明，RRG性能与LLM大小无关，且更大体积尺寸不一定提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用MLLMs自动化生成放射学报告，并优化3D CT报告生成的性能。

Method: 研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、LLMs和微调技术；引入两种知识增强方法。

Result: 在1,687例AMOS-MM数据集上，性能提升10%，达到MICCAI 2024挑战赛第二名；RRG性能与LLM大小无关；更大体积尺寸不一定提升性能；使用分割掩码可提升性能。

Conclusion: 3D MLLMs在放射学报告生成中具有潜力，设计选择和知识增强方法对性能有显著影响。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to
automate Radiology Report Generation (RRG). In this work, we systematically
investigate the design space of 3D MLLMs, including visual input
representation, projectors, Large Language Models (LLMs), and fine-tuning
techniques for 3D CT report generation. We also introduce two knowledge-based
report augmentation methods that improve performance on the GREEN score by up
to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our
results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely
independent of the size of LLM under the same training protocol. We also show
that larger volume size does not always improve performance if the original ViT
was pre-trained on a smaller volume size. Lastly, we show that using a
segmentation mask along with the CT volume improves performance. The code is
publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [137] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: DRAGON是一个灵活的框架，用于优化生成模型的目标结果，支持多种奖励函数，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法如RLHF或DPO在优化生成模型时灵活性不足，DRAGON旨在提供更通用的解决方案。

Method: DRAGON通过选择编码器和参考示例构建奖励函数，利用对比集优化奖励。

Result: 在20种奖励函数下，DRAGON平均胜率为81.45%，且无需人类偏好标注即可提升生成质量。

Conclusion: DRAGON为设计和优化奖励函数提供了新方法，显著提升了生成内容的人类感知质量。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


### [138] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 研究了不同适配器配置对MusicGen和Mustango音乐生成模型在低资源音乐类型中的性能影响，发现卷积和变压器适配器各有优势，并分析了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决大规模音乐生成模型微调的高计算成本问题，探索参数高效微调（PEFT）技术在低资源音乐类型中的应用。

Method: 通过实验比较卷积和变压器适配器在不同配置下的性能，分析其对Hindustani Classical和Turkish Makam音乐的影响。

Result: 卷积适配器擅长捕捉局部细节，变压器适配器保留长程依赖；Mustango生成多样但稳定性差，MusicGen训练更快且质量更高。

Conclusion: 适配器设计需权衡细节与长程依赖，Mustango适合多样性需求，MusicGen适合高效高质量生成。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


### [139] [A Hierarchical Deep Learning Approach for Minority Instrument Detection](https://arxiv.org/abs/2506.21167)
*Dylan Sechet,Francesca Bugiotti,Matthieu Kowalski,Edouard d'Hérouville,Filip Langiewicz*

Main category: cs.SD

TL;DR: 论文探讨了在音乐信息检索中识别乐器活动的重要性，提出了一种基于层次分类的方法，并在MedleyDB数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决乐器识别中因数据不足导致的细粒度标注问题，同时提升粗粒度乐器检测的可靠性。

Method: 基于Hornbostel-Sachs分类的层次分类系统，结合MedleyDB数据集，测试了新的层次音乐预测模型。

Result: 展示了层次分类在粗粒度乐器检测中的有效性，为乐器识别领域提供了新思路。

Conclusion: 层次分类方法在乐器识别中具有潜力，为未来研究提供了方向。

Abstract: Identifying instrument activities within audio excerpts is vital in music
information retrieval, with significant implications for music cataloging and
discovery. Prior deep learning endeavors in musical instrument recognition have
predominantly emphasized instrument classes with ample data availability.
Recent studies have demonstrated the applicability of hierarchical
classification in detecting instrument activities in orchestral music, even
with limited fine-grained annotations at the instrument level. Based on the
Hornbostel-Sachs classification, such a hierarchical classification system is
evaluated using the MedleyDB dataset, renowned for its diversity and richness
concerning various instruments and music genres. This work presents various
strategies to integrate hierarchical structures into models and tests a new
class of models for hierarchical music prediction. This study showcases more
reliable coarse-level instrument detection by bridging the gap between detailed
instrument identification and group-level recognition, paving the way for
further advancements in this domain.

</details>


### [140] [Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou](https://arxiv.org/abs/2506.21269)
*Pengfei Fan,Yuli Zhang,Xinheng Wang,Ruiyuan Jiang,Hankang Gu,Dongyao Jia,Shangbo Wang*

Main category: cs.SD

TL;DR: 该研究公开了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种双模态特征融合深度卷积神经网络（BMCNN）来建模车辆噪声与行驶速度的耦合关系。实验表明，BMCNN在两个数据集上表现优异，并验证了各模块对性能提升的贡献。该方法可应用于智能城市交通管理系统。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过声学数据建模车辆噪声与速度的关系，以支持智能交通管理和减少噪声污染。

Method: 提出BMCNN模型，结合自适应去噪和归一化预处理，并行提取MFCC和小波包能量特征，并通过跨模态注意力机制融合。

Result: BMCNN在SZUR-Acoustic Dataset和IDMT-Traffic数据集上的分类准确率分别为87.56%和96.28%。

Conclusion: 该方法在噪声监测和速度估计方面具有实际应用潜力，可优化交通流量控制并支持可持续城市规划。

Abstract: This study presents and publicly releases the Suzhou Urban Road Acoustic
Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive
data-acquisition protocols and annotation guidelines to ensure transparency and
reproducibility of the experimental workflow. To model the coupling between
vehicular noise and driving speed, we propose a bimodal-feature-fusion deep
convolutional neural network (BMCNN). During preprocessing, an adaptive
denoising and normalization strategy is applied to suppress environmental
background interference; in the network architecture, parallel branches extract
Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,
which are subsequently fused via a cross-modal attention mechanism in the
intermediate feature space to fully exploit time-frequency information.
Experimental results demonstrate that BMCNN achieves a classification accuracy
of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic
dataset. Ablation studies and robustness tests on the Suzhou dataset further
validate the contributions of each module to performance improvement and
overfitting mitigation. The proposed acoustics-based speed classification
method can be integrated into smart-city traffic management systems for
real-time noise monitoring and speed estimation, thereby optimizing traffic
flow control, reducing roadside noise pollution, and supporting sustainable
urban planning.

</details>


### [141] [SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture](https://arxiv.org/abs/2506.21478)
*Kehan Sui,Jinxu Xiang,Fang Jin*

Main category: cs.SD

TL;DR: SmoothSinger是一种基于条件扩散模型的歌唱语音合成方法，通过统一框架直接优化低质量音频，减少两阶段流程的失真，并在Opencpop数据集上取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成中表现优异，但在歌唱语音合成中因复杂的声学和音乐特性而面临挑战，导致自然度下降。

Method: 采用参考引导的双分支架构，利用低质量音频作为参考引导去噪过程；改进U-Net，增加低频上采样路径以捕捉音高轮廓和长时频谱依赖；训练时用降级真实音频替代参考音频以解决时序不匹配问题。

Result: 在Opencpop数据集上，SmoothSinger在客观和主观评估中均达到最优效果，显著减少伪影并提升合成语音的自然度。

Conclusion: SmoothSinger通过统一框架和结构改进，有效解决了歌唱语音合成中的自然度和失真问题，为扩散模型在该领域的应用提供了新思路。

Abstract: Singing voice synthesis (SVS) aims to generate expressive and high-quality
vocals from musical scores, requiring precise modeling of pitch, duration, and
articulation. While diffusion-based models have achieved remarkable success in
image and video generation, their application to SVS remains challenging due to
the complex acoustic and musical characteristics of singing, often resulting in
artifacts that degrade naturalness. In this work, we propose SmoothSinger, a
conditional diffusion model designed to synthesize high quality and natural
singing voices. Unlike prior methods that depend on vocoders as a final stage
and often introduce distortion, SmoothSinger refines low-quality synthesized
audio directly in a unified framework, mitigating the degradation associated
with two-stage pipelines. The model adopts a reference-guided dual-branch
architecture, using low-quality audio from any baseline system as a reference
to guide the denoising process, enabling more expressive and context-aware
synthesis. Furthermore, it enhances the conventional U-Net with a parallel
low-frequency upsampling path, allowing the model to better capture pitch
contours and long term spectral dependencies. To improve alignment during
training, we replace reference audio with degraded ground truth audio,
addressing temporal mismatch between reference and target signals. Experiments
on the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that
SmoothSinger achieves state-of-the-art results in both objective and subjective
evaluations. Extensive ablation studies confirm its effectiveness in reducing
artifacts and improving the naturalness of synthesized voices.

</details>


### [142] [Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform](https://arxiv.org/abs/2506.21440)
*Maxime Leiber,Yosra Marnissi,Axel Barrau,Sylvain Meignen,Laurent Massoulié*

Main category: cs.SD

TL;DR: 提出了一种可微分的短时傅里叶变换（STFT）方法，通过梯度优化参数，解决了传统方法依赖离散搜索的问题，并展示了其在提升时频表示和下游任务性能上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统STFT参数调整依赖手动或启发式方法，效果不佳且计算量大。

Method: 提出统一的可微分STFT框架，支持基于梯度的参数优化，并可无缝集成到神经网络中。

Result: 实验表明，该方法能有效优化时频表示并提升下游任务性能。

Conclusion: 可微分STFT为信号分析提供了更高效的参数优化方法，具有广泛的应用潜力。

Abstract: The short-time Fourier transform (STFT) is widely used for analyzing
non-stationary signals. However, its performance is highly sensitive to its
parameters, and manual or heuristic tuning often yields suboptimal results. To
overcome this limitation, we propose a unified differentiable formulation of
the STFT that enables gradient-based optimization of its parameters. This
approach addresses the limitations of traditional STFT parameter tuning
methods, which often rely on computationally intensive discrete searches. It
enables fine-tuning of the time-frequency representation (TFR) based on any
desired criterion. Moreover, our approach integrates seamlessly with neural
networks, allowing joint optimization of the STFT parameters and network
weights. The efficacy of the proposed differentiable STFT in enhancing TFRs and
improving performance in downstream tasks is demonstrated through experiments
on both simulated and real-world data.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [143] [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
*Ghazal Al-Shwayyat,Omer Nezih Gerek*

Main category: eess.AS

TL;DR: 研究提出两种混合模型（MFCC+CNN和DWT+RNN）用于阿拉伯语方言识别，MFCC+CNN表现更优（准确率91.2%），并探讨了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言识别因语言多样性和标注数据稀缺而具有挑战性，尤其在低资源场景下。

Method: 开发并评估了两种混合模型：MFCC+CNN和DWT+RNN，使用Common Voice阿拉伯语数据集的方言子集进行训练。

Result: MFCC+CNN模型表现最佳（准确率91.2%），显著优于DWT+RNN（66.5%）。

Conclusion: 研究为低资源环境下的阿拉伯语方言识别提供了基线，建议未来采用更大标注数据集、自监督学习技术和先进神经网络架构。

Abstract: Arabic dialect recognition presents a significant challenge in speech
technology due to the linguistic diversity of Arabic and the scarcity of large
annotated datasets, particularly for underrepresented dialects. This research
investigates hybrid modeling strategies that integrate classical signal
processing techniques with deep learning architectures to address this problem
in low-resource scenarios. Two hybrid models were developed and evaluated: (1)
Mel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural
Network (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with
a Recurrent Neural Network (RNN). The models were trained on a dialect-filtered
subset of the Common Voice Arabic dataset, with dialect labels assigned based
on speaker metadata. Experimental results demonstrate that the MFCC + CNN
architecture achieved superior performance, with an accuracy of 91.2% and
strong precision, recall, and F1-scores, significantly outperforming the
Wavelet + RNN configuration, which achieved an accuracy of 66.5%. These
findings highlight the effectiveness of leveraging spectral features with
convolutional models for Arabic dialect recognition, especially when working
with limited labeled data. The study also identifies limitations related to
dataset size, potential regional overlaps in labeling, and model optimization,
providing a roadmap for future research. Recommendations for further
improvement include the adoption of larger annotated corpora, integration of
self-supervised learning techniques, and exploration of advanced neural
architectures such as Transformers. Overall, this research establishes a strong
baseline for future developments in Arabic dialect recognition within
resource-constrained environments.

</details>


### [144] [Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4](https://arxiv.org/abs/2506.21174)
*Jongyeon Park,Joonhee Lee,Do-Hyeon Lim,Hong Kook Kim,Hyeongcheol Geum,Jeong Eun Lim*

Main category: eess.AS

TL;DR: 该技术报告介绍了DCASE 2025挑战赛任务4的提交系统，通过结合额外音频特征和标签校正系统，提升了音频分类性能。


<details>
  <summary>Details</summary>
Motivation: 混合音频中的细微线索难以仅通过梅尔频谱捕捉，因此引入额外特征以提供更多视角。

Method: 结合梅尔频谱特征与额外音频特征（如频谱滚降和色度特征），并应用基于代理的标签校正系统和数据集优化。

Result: 实验显示，该系统将CA-SDRi指标相对提升了14.7%。

Conclusion: 通过多特征融合和标签校正，显著提升了音频分类性能。

Abstract: This technical report presents submission systems for Task 4 of the DCASE
2025 Challenge. This model incorporates additional audio features (spectral
roll-off and chroma features) into the embedding feature extracted from the
mel-spectral feature to im-prove the classification capabilities of an
audio-tagging model in the spatial semantic segmentation of sound scenes (S5)
system. This approach is motivated by the fact that mixed audio often contains
subtle cues that are difficult to capture with mel-spectrograms alone. Thus,
these additional features offer alterna-tive perspectives for the model.
Second, an agent-based label correction system is applied to the outputs
processed by the S5 system. This system reduces false positives, improving the
final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.
Finally, we refine the training dataset to enhance the classi-fication accuracy
of low-performing classes by removing irrele-vant samples and incorporating
external data. That is, audio mix-tures are generated from a limited number of
data points; thus, even a small number of out-of-class data points could
degrade model performance. The experiments demonstrate that the submit-ted
systems employing these approaches relatively improve CA-SDRi by up to 14.7%
compared to the baseline of DCASE 2025 Challenge Task 4.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [145] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 提出了首个将脉冲神经网络（SNN）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，填补了当前方法的空白。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量的指数增长（如NISAR任务预计两年内生成100PB数据），高效能处理对可持续数据中心运营至关重要。SNN的事件驱动计算模型有望节省30-100倍能源。

Method: 开发了针对缠绕相位数据的脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并分析了计算复杂性和收敛性。

Result: SNN的时间动态特性可自然建模相位解缠的空间连续性约束，为大规模InSAR处理提供了可持续的补充方法。

Conclusion: 该工作为神经形态计算与SAR干涉测量的交叉领域开辟了新方向。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [146] [Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning](https://arxiv.org/abs/2506.21324)
*Jiechen Chen,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.NE

TL;DR: 提出了一种结合神经形态计算和量子计算优势的随机量子脉冲（SQS）神经元模型，解决了现有量子脉冲神经元的局限性。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算和量子计算各有优势，但现有量子脉冲神经元模型存在依赖经典内存机制和训练方法的问题。

Method: 使用多量子比特电路实现具有内部量子内存的脉冲单元，并通过硬件友好的局部学习规则训练SQS神经网络（SQSNN）。

Result: SQS神经元模型能够在单次操作中实现事件驱动的概率脉冲生成，SQSNN模型结合了神经形态计算的时间序列效率和量子计算的指数级状态空间。

Conclusion: SQSNN模型为模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络开辟了新途径。

Abstract: Neuromorphic and quantum computing have recently emerged as promising
paradigms for advancing artificial intelligence, each offering complementary
strengths. Neuromorphic systems built on spiking neurons excel at processing
time-series data efficiently through sparse, event-driven computation,
consuming energy only upon input events. Quantum computing, on the other hand,
leverages superposition and entanglement to explore feature spaces that are
exponentially large in the number of qubits. Hybrid approaches combining these
paradigms have begun to show potential, but existing quantum spiking models
have important limitations. Notably, prior quantum spiking neuron
implementations rely on classical memory mechanisms on single qubits, requiring
repeated measurements to estimate firing probabilities, and they use
conventional backpropagation on classical simulators for training. Here we
propose a stochastic quantum spiking (SQS) neuron model that addresses these
challenges. The SQS neuron uses multi-qubit quantum circuits to realize a
spiking unit with internal quantum memory, enabling event-driven probabilistic
spike generation in a single shot. Furthermore, we outline how networks of SQS
neurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a
hardware-friendly local learning rule, eliminating the need for global
classical backpropagation. The proposed SQSNN model fuses the time-series
efficiency of neuromorphic computing with the exponentially large inner state
space of quantum computing, paving the way for quantum spiking neural networks
that are modular, scalable, and trainable on quantum hardware.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [147] [Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends](https://arxiv.org/abs/2506.20966)
*Tian-Yu Xiang,Ao-Qun Jin,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Sheng-Bin Duan,Fu-Chao Xie,Wen-Kai Wang,Si-Cheng Wang,Ling-Yun Li,Tian Tu,Zeng-Guang Hou*

Main category: cs.RO

TL;DR: 该论文综述了视觉-语言-动作（VLA）模型的后训练策略，从人类运动学习的角度出发，提出了四个维度的分类方法，并总结了关键挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作任务中展现出泛化能力，但在高精度需求的任务中表现不足，需要通过后训练进一步适配。

Method: 通过人类运动学习的视角，提出环境感知、实体意识、任务理解和多组件整合四个维度的后训练策略。

Result: 论文提出了一个结构化分类法，并总结了VLA模型后训练的关键挑战和趋势。

Conclusion: 该研究为VLA模型的后训练提供了全面的综述和实用指导，为未来研究建立了概念框架。

Abstract: Vision-language-action (VLA) models extend vision-language models (VLM) by
integrating action generation modules for robotic manipulation. Leveraging
strengths of VLM in vision perception and instruction understanding, VLA models
exhibit promising generalization across diverse manipulation tasks. However,
applications demanding high precision and accuracy reveal performance gaps
without further adaptation. Evidence from multiple domains highlights the
critical role of post-training to align foundational models with downstream
applications, spurring extensive research on post-training VLA models. VLA
model post-training aims to address the challenge of improving an embodiment's
ability to interact with the environment for the given tasks, analogous to the
process of humans motor skills acquisition. Accordingly, this paper reviews
post-training strategies for VLA models through the lens of human motor
learning, focusing on three dimensions: environments, embodiments, and tasks. A
structured taxonomy is introduced aligned with human learning mechanisms: (1)
enhancing environmental perception, (2) improving embodiment awareness, (3)
deepening task comprehension, and (4) multi-component integration. Finally, key
challenges and trends in post-training VLA models are identified, establishing
a conceptual framework to guide future research. This work delivers both a
comprehensive overview of current VLA model post-training methods from a human
motor learning perspective and practical insights for VLA model development.
(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)

</details>


### [148] [V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling](https://arxiv.org/abs/2506.21041)
*Junwei You,Pei Li,Zhuoyu Jiang,Zilin Huang,Rui Gan,Haotian Shi,Bin Ran*

Main category: cs.RO

TL;DR: V2X-REALM是一个基于视觉语言模型的框架，用于解决自动驾驶在长尾场景下的鲁棒性问题，通过多模态学习和创新模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在罕见、多样化和视觉退化的长尾场景中的规划与决策问题，特别是在协同环境中。

Method: 提出三个创新：长尾场景生成与评估管道、门控多场景自适应注意力模块、多任务场景感知对比学习目标。

Result: 在复杂驾驶条件下，V2X-REALM在鲁棒性、语义推理、安全性和规划准确性上显著优于现有基线。

Conclusion: V2X-REALM提升了端到端协同自动驾驶的可扩展性，为长尾场景下的鲁棒性提供了有效解决方案。

Abstract: Ensuring robust planning and decision-making under rare, diverse, and
visually degraded long-tail scenarios remains a fundamental challenge for
autonomous driving in urban environments. This issue becomes more critical in
cooperative settings, where vehicles and infrastructure jointly perceive and
reason across complex environments. To address this challenge, we propose
V2X-REALM, a vision-language model (VLM)-based framework with adaptive
multimodal learning for robust cooperative autonomous driving under long-tail
scenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven
long-tail scenario generation and evaluation pipeline that leverages foundation
models to synthesize realistic long-tail conditions such as snow and fog across
vehicle- and infrastructure-side views, enriching training diversity
efficiently; (ii) a gated multi-scenario adaptive attention module that
modulates the visual stream using scenario priors to recalibrate ambiguous or
corrupted features; and (iii) a multi-task scenario-aware contrastive learning
objective that improves multimodal alignment and promotes cross-scenario
feature separability. Extensive experiments demonstrate that V2X-REALM
significantly outperforms existing baselines in robustness, semantic reasoning,
safety, and planning accuracy under complex, challenging driving conditions,
advancing the scalability of end-to-end cooperative autonomous driving.

</details>


### [149] [WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/abs/2506.21539)
*Jun Cen,Chaohui Yu,Hangjie Yuan,Yuming Jiang,Siteng Huang,Jiayan Guo,Xin Li,Yibing Song,Hao Luo,Fan Wang,Deli Zhao,Hao Chen*

Main category: cs.RO

TL;DR: WorldVLA是一个结合视觉-语言-动作（VLA）模型与世界模型的框架，通过联合预测未来图像和生成动作，提升环境物理理解和动作生成能力。研究发现自回归动作生成存在误差传播问题，并提出注意力掩码策略以改善性能。


<details>
  <summary>Details</summary>
Motivation: 统一动作与图像的理解与生成，通过联合建模提升世界模型和动作模型的性能。

Method: 整合VLA模型与世界模型，利用动作和图像理解预测未来图像，同时基于图像观察生成动作。提出注意力掩码策略以减少自回归动作生成的误差传播。

Result: WorldVLA性能优于独立模型，但自回归动作生成存在误差传播问题。注意力掩码策略显著提升了动作块生成任务的表现。

Conclusion: WorldVLA展示了联合建模的优势，但需解决自回归动作生成的局限性。注意力掩码策略为改进方向提供了有效方法。

Abstract: We present WorldVLA, an autoregressive action world model that unifies action
and image understanding and generation. Our WorldVLA intergrates
Vision-Language-Action (VLA) model and world model in one single framework. The
world model predicts future images by leveraging both action and image
understanding, with the purpose of learning the underlying physics of the
environment to improve action generation. Meanwhile, the action model generates
the subsequent actions based on image observations, aiding in visual
understanding and in turn helps visual generation of the world model. We
demonstrate that WorldVLA outperforms standalone action and world models,
highlighting the mutual enhancement between the world model and the action
model. In addition, we find that the performance of the action model
deteriorates when generating sequences of actions in an autoregressive manner.
This phenomenon can be attributed to the model's limited generalization
capability for action prediction, leading to the propagation of errors from
earlier actions to subsequent ones. To address this issue, we propose an
attention mask strategy that selectively masks prior actions during the
generation of the current action, which shows significant performance
improvement in the action chunk generation task.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [150] [Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys](https://arxiv.org/abs/2506.20839)
*Jing Luo,Yejun Gu,Yanfei Wang,Xiaolong Ma,Jaafar. A El-Awady*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种基于混合密度网络（MDN）的方法，用于预测位错密度和应力分布，并通过统计参数改进塑性模型，实现高精度的应力-应变预测和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 机器学习在结构材料领域的应用日益重要，但如何整合现有数据并量化预测模型中的不确定性仍具挑战性。

Method: 利用混合密度网络（MDN）模型，基于大量实验数据预测位错密度分布（作为潜变量）和晶粒级应力分布，并将统计参数融入位错介导的塑性模型。

Result: 该方法显著提高了机械性能预测的准确性和可靠性，并实现了明确的不确定性量化。

Conclusion: 该策略不仅优化了合金设计，还加速了新材料的开发，对快速发展的工业具有重要意义。

Abstract: Machine learning has significantly advanced the understanding and application
of structural materials, with an increasing emphasis on integrating existing
data and quantifying uncertainties in predictive modeling. This study presents
a comprehensive methodology utilizing a mixed density network (MDN) model,
trained on extensive experimental data from literature. This approach uniquely
predicts the distribution of dislocation density, inferred as a latent
variable, and the resulting stress distribution at the grain level. The
incorporation of statistical parameters of those predicted distributions into a
dislocation-mediated plasticity model allows for accurate stress-strain
predictions with explicit uncertainty quantification. This strategy not only
improves the accuracy and reliability of mechanical property predictions but
also plays a vital role in optimizing alloy design, thereby facilitating the
development of new materials in a rapidly evolving industry.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [151] [CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](https://arxiv.org/abs/2506.21085)
*Yangzhe Peng,Kaiyuan Gao,Liang He,Yuheng Cong,Haiguang Liu,Kun He,Lijun Wu*

Main category: q-bio.BM

TL;DR: 论文提出了一个名为CovDocker的共价对接基准，用于解决现有方法在共价键形成和结构变化预测上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有分子对接方法和深度学习模型很少考虑共价键的形成及其相关结构变化，限制了共价药物设计的进展。

Method: 将共价对接过程分解为三个任务：反应位点预测、共价反应预测和共价对接，并采用Uni-Mol和Chemformer等先进模型建立基线性能。

Result: 基准能准确预测相互作用位点并建模共价结合中的分子转化，验证了其作为共价药物设计研究框架的潜力。

Conclusion: CovDocker为共价药物设计提供了数据驱动方法，有望加速选择性共价抑制剂的发现，解决治疗开发中的关键挑战。

Abstract: Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.

</details>


### [152] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: MegaFold是一个跨平台系统，通过优化数据管道、内存管理和算子融合，显著加速AlphaFold3的训练，提升内存效率和训练速度。


<details>
  <summary>Details</summary>
Motivation: AlphaFold3等蛋白质结构预测模型的计算和内存需求高，限制了其训练的可扩展性。MegaFold旨在解决这些瓶颈。

Method: MegaFold采用提前缓存、Triton内核优化和深度融合技术，优化GPU利用率和内存管理。

Result: 在NVIDIA H200和AMD MI250 GPU上，MegaFold将峰值内存使用降低1.23倍，训练速度提升1.73倍和1.62倍，支持更长的序列训练。

Conclusion: MegaFold显著提升了蛋白质折叠模型的可扩展性和训练效率，代码已开源。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [153] [Real-time and personalized product recommendations for large e-commerce platforms](https://arxiv.org/abs/2506.21368)
*Matteo Tolloso,Davide Bacciu,Shahab Mokarizadeh,Marco Varesi*

Main category: cs.IR

TL;DR: 提出了一种基于图神经网络和简约学习方法的实时个性化时尚产品推荐方法，适用于大型电商平台。


<details>
  <summary>Details</summary>
Motivation: 为大型电商平台（尤其是时尚零售）提供实时、个性化的产品推荐，以提升用户满意度。

Method: 利用图神经网络和简约学习方法，实现高准确性和可扩展性的推荐，同时确保低响应时间。

Result: 在大型电商平台数据集上的实验表明，该方法能有效预测购买序列并处理多交互场景，实现高效个性化推荐。

Conclusion: 该方法在真实场景下表现优异，能够满足实时个性化推荐的需求。

Abstract: We present a methodology to provide real-time and personalized product
recommendations for large e-commerce platforms, specifically focusing on
fashion retail. Our approach aims to achieve accurate and scalable
recommendations with minimal response times, ensuring user satisfaction,
leveraging Graph Neural Networks and parsimonious learning methodologies.
Extensive experimentation with datasets from one of the largest e-commerce
platforms demonstrates the effectiveness of our approach in forecasting
purchase sequences and handling multi-interaction scenarios, achieving
efficient personalized recommendations under real-world constraints.

</details>


### [154] [EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora](https://arxiv.org/abs/2506.20963)
*Fangyuan Zhang,Zhengjun Huang,Yingli Zhou,Qintian Guo,Zhixun Li,Wensheng Luo,Di Jiang,Yixiang Fang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: EraRAG提出了一种支持动态更新的多层Graph-RAG框架，通过层次化图结构和LSH技术实现高效数据插入，显著减少更新时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-RAG方法假设静态语料库，动态更新成本高，限制了在动态环境中的可扩展性。

Method: 利用基于超平面的LSH技术将语料库分层组织为图结构，支持局部化数据插入，避免全局重构。

Result: 在大规模基准测试中，EraRAG更新时间和资源消耗降低一个数量级，同时保持高检索精度和低延迟。

Conclusion: EraRAG为持续增长的语料库提供了高效的动态更新方案，平衡了检索效率和适应性。

Abstract: Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [155] [Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market](https://arxiv.org/abs/2506.20930)
*Chi-Sheng Chen,Xinyu Zhang,Ya-Chuan Chen*

Main category: quant-ph

TL;DR: 提出了一种混合量子-经典强化学习框架，用于台湾股市的板块轮动，发现量子增强模型在训练奖励上表现更好，但在实际投资指标上不如经典模型。


<details>
  <summary>Details</summary>
Motivation: 研究量子强化学习在金融领域的实际应用效果，特别是板块轮动策略。

Method: 采用PPO算法，结合经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）作为策略和价值网络，并通过自动化特征工程提取金融指标。

Result: 量子增强模型在训练奖励上表现更优，但在实际投资指标（如累计回报和夏普比率）上不如经典模型。

Conclusion: 当前奖励设计可能导致对短期波动的过拟合，量子电路的表达能力和优化不稳定性加剧了这一问题，未来需改进奖励设计、模型正则化和验证方法。

Abstract: We propose a hybrid quantum-classical reinforcement learning framework for
sector rotation in the Taiwan stock market. Our system employs Proximal Policy
Optimization (PPO) as the backbone algorithm and integrates both classical
architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,
QASA) as policy and value networks. An automated feature engineering pipeline
extracts financial indicators from capital share data to ensure consistent
model input across all configurations. Empirical backtesting reveals a key
finding: although quantum-enhanced models consistently achieve higher training
rewards, they underperform classical models in real-world investment metrics
such as cumulative return and Sharpe ratio. This discrepancy highlights a core
challenge in applying reinforcement learning to financial domains -- namely,
the mismatch between proxy reward signals and true investment objectives. Our
analysis suggests that current reward designs may incentivize overfitting to
short-term volatility rather than optimizing risk-adjusted returns. This issue
is compounded by the inherent expressiveness and optimization instability of
quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We
discuss the implications of this reward-performance gap and propose directions
for future improvement, including reward shaping, model regularization, and
validation-based early stopping. Our work offers a reproducible benchmark and
critical insights into the practical challenges of deploying quantum
reinforcement learning in real-world finance.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [156] [Structural System Identification via Validation and Adaptation](https://arxiv.org/abs/2506.20799)
*Cristian López,Keegan J. Moore*

Main category: math.DS

TL;DR: 提出了一种基于生成模型的结构系统识别方法，结合神经网络和验证数据集，用于参数估计和模型验证。


<details>
  <summary>Details</summary>
Motivation: 通过实验数据与科学理论结合，准确估计复杂系统的动力学参数，以理解和预测系统行为。

Method: 使用神经网络将随机噪声映射为物理参数，通过已知运动方程生成假加速度，并与真实数据对比；利用独立验证数据集和判别网络验证参数。

Result: 分析和实际实验表明，该方法能准确估计非线性结构系统的参数并验证模型。

Conclusion: 该方法为结构系统识别和验证提供了一种有效的新途径。

Abstract: Estimating the governing equation parameter values is essential for
integrating experimental data with scientific theory to understand, validate,
and predict the dynamics of complex systems. In this work, we propose a new
method for structural system identification (SI), uncertainty quantification,
and validation directly from data. Inspired by generative modeling frameworks,
a neural network maps random noise to physically meaningful parameters. These
parameters are then used in the known equation of motion to obtain fake
accelerations, which are compared to real training data via a mean square error
loss. To simultaneously validate the learned parameters, we use independent
validation datasets. The generated accelerations from these datasets are
evaluated by a discriminator network, which determines whether the output is
real or fake, and guides the parameter-generator network. Analytical and real
experiments show the parameter estimation accuracy and model validation for
different nonlinear structural systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [157] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: ClusterRCA是一种用于高性能计算系统网络故障诊断的新框架，通过多模态数据定位故障节点和类型。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统的网络故障诊断面临数据异构性和准确性不足的挑战，现有方法难以直接应用。

Method: ClusterRCA结合分类器和图方法，从拓扑连接的NIC对中提取特征，构建故障图并通过随机游走定位根因。

Result: 实验表明，ClusterRCA在高性能计算系统中诊断网络故障的准确性高，且在不同应用场景下表现稳健。

Conclusion: ClusterRCA为高性能计算系统的网络故障诊断提供了一种有效且鲁棒的解决方案。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [158] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: GPU内存带宽是低延迟大型语言模型（LLM）推理的主要瓶颈。推测解码利用轻量级草案器提出K个令牌，由LLM并行验证，提高令牌吞吐量。然而，对于MoE模型，推测解码因激活更多权重而增加数据移动和验证时间，导致性能下降。Cascade框架通过动态调整K值和选择性启用推测解码，优化MoE模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型中推测解码因权重激活增加导致的性能下降问题，使其在实际应用中可行。

Method: 提出Cascade框架，通过动态调整K值和选择性启用推测解码，基于推测效用（token增益与验证成本的比率）进行决策。

Result: Cascade在vLLM中实现，评估显示其将性能下降限制在5%（相比1.5倍），吞吐量提高7-14%。

Conclusion: Cascade使推测解码在MoE模型中变得实用，显著优化了推理性能。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [159] [Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation](https://arxiv.org/abs/2506.21154)
*He Li,Haoang Chi,Mingyu Liu,Wanrong Huang,Liyang Xu,Wenjing Yang*

Main category: stat.ME

TL;DR: 提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，表现出更强的估计能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有时空维度，而现有基于经典统计模型的方法在性能和泛化能力上存在局限。

Method: 使用Transformer构建框架，提出了一种一致且渐近正态的估计器。

Result: 模拟实验显示该估计器优于基线方法；真实数据实验揭示了冲突对哥伦比亚森林损失的因果效应。

Conclusion: 该框架在时空反事实估计中表现出色，为相关研究提供了新工具。

Abstract: The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [160] [Evaluating PDE discovery methods for multiscale modeling of biological signals](https://arxiv.org/abs/2506.20694)
*Andréa Ducos,Audrey Denizot,Thomas Guyet,Hugues Berry*

Main category: q-bio.QM

TL;DR: 论文提出了一种结合粒子模拟和PDE发现的方法，用于从微观数据推断生物系统的宏观动力学特性，并在钙扩散模拟中验证了五种PDE发现方法的性能。


<details>
  <summary>Details</summary>
Motivation: 生物系统的非线性、多尺度特性以及未知的物理原理使其行为表征极具挑战性，需要跨尺度的机制链接。

Method: 结合粒子模拟和PDE发现，评估五种PDE发现方法在钙扩散模拟中的表现。

Result: 部分方法能准确恢复扩散项，表明PDE发现可从微观数据捕捉宏观动力学。

Conclusion: PDE发现有望成为连接生物系统微观与宏观尺度的有效工具。

Abstract: Biological systems are non-linear, include unobserved variables and the
physical principles that govern their dynamics are partly unknown. This makes
the characterization of their behavior very challenging. Notably, their
activity occurs on multiple interdependent spatial and temporal scales that
require linking mechanisms across scales. To address the challenge of bridging
gaps between scales, we leverage partial differential equations (PDE)
discovery. PDE discovery suggests meso-scale dynamics characteristics from
micro-scale data. In this article, we present our framework combining
particle-based simulations and PDE discovery and conduct preliminary
experiments to assess equation discovery in controlled settings. We evaluate
five state-of-the-art PDE discovery methods on particle-based simulations of
calcium diffusion in astrocytes. The performances of the methods are evaluated
on both the form of the discovered equation and the forecasted temporal
variations of calcium concentration. Our results show that several methods
accurately recover the diffusion term, highlighting the potential of PDE
discovery for capturing macroscopic dynamics in biological systems from
microscopic data.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [161] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: 研究探讨了聊天机器人拟人化如何通过引发人类共情来促进对聊天机器人的亲社会行为和意图。


<details>
  <summary>Details</summary>
Motivation: 填补人们对帮助聊天机器人动机的研究空白，基于CASA框架探索拟人化对共情和亲社会行为的影响。

Method: 通过在线实验（N=244），让聊天机器人在协作任务中犯错并解释原因，测量参与者的亲社会行为和意图。

Result: 聊天机器人的人类身份和情感表达增强了参与者的亲社会行为和意图，共情在其中起中介作用。

Conclusion: 拟人化设计能有效促进人类对聊天机器人的共情和亲社会行为，为未来设计提供指导。

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>


### [162] [A Systematic Review of Human-AI Co-Creativity](https://arxiv.org/abs/2506.21333)
*Saloni Singh,Koen Hndriks,Drik Heylen,Kim Baraka*

Main category: cs.HC

TL;DR: 论文通过系统文献综述分析了62篇关于协同创意系统的研究，总结了设计维度和考虑因素，强调用户控制和系统主动性对创意成果的重要性。


<details>
  <summary>Details</summary>
Motivation: 支持开发更复杂和定制化的协同创意系统，利用先前工作的设计考虑为未来系统提供基础。

Method: 对62篇关于协同创意系统的论文进行系统文献综述，涵盖视觉艺术、设计和写作等领域。

Result: 发现高用户控制的系统能提高满意度、信任感和创意成果的所有权感；主动且适应性强的系统能增强协作。

Conclusion: 尽管有进展，仍存在早期创意阶段支持不足和用户适应AI系统的挑战。

Abstract: The co creativity community is making significant progress in developing more
sophisticated and tailored systems to support and enhance human creativity.
Design considerations from prior work can serve as a valuable and efficient
foundation for future systems. To support this effort, we conducted a
systematic literature review of 62 papers on co-creative systems. These papers
cover a diverse range of applications, including visual arts, design, and
writing, where the AI acts not just as a tool but as an active collaborator in
the creative process. From this review, we identified several key dimensions
relevant to system design: phase of the creative process, creative task,
proactive behavior of the system, user control, system embodiment, and AI model
type. Our findings suggest that systems offering high user control lead to
greater satisfaction, trust, and a stronger sense of ownership over creative
outcomes. Furthermore, proactive systems, when adaptive and context sensitive,
can enhance collaboration. We also extracted 24 design considerations,
highlighting the value of encouraging users to externalize their thoughts and
of increasing the system's social presence and transparency to foster trust.
Despite recent advancements, important gaps remain, such as limited support for
early creative phases like problem clarification, and challenges related to
user adaptation to AI systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [163] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理合成中的时空不一致问题，通过几何感知条件和结构UV扩散策略，生成更平滑、一致的纹理。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解，导致不一致性。视频生成模型的成功启发了利用其解决纹理合成问题。

Method: 结合几何感知条件，利用3D网格结构，提出结构UV扩散策略，增强遮挡区域的生成。

Result: VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法，生成高质量、时间稳定的纹理。

Conclusion: VideoTex为动态实时应用提供了视觉质量和时间一致性的解决方案。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [164] [Control and optimization for Neural Partial Differential Equations in Supervised Learning](https://arxiv.org/abs/2506.20764)
*Alain Bensoussan,Minh-Binh Tran,Bangjie Wang*

Main category: math.OC

TL;DR: 本文提出了一种新的视角，将神经网络视为偏微分方程（PDEs），并研究了抛物型和双曲型算子系数的控制与优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献对抛物型和双曲型系统的控制与优化问题已有广泛研究，但对算子系数的控制与优化问题尚未深入探讨。本文旨在填补这一空白，特别是在神经网络和监督学习的背景下。

Method: 提出将神经网络视为PDEs，并将传统的ODE控制问题重新表述为PDEs的控制问题。针对抛物型PDEs，提出了对偶系统公式，并提供了理论证明。

Result: 证明了抛物型PDEs的控制与优化问题存在极小值，并研究了双曲型PDEs的近似控制问题的解的存在性。

Conclusion: 本文为PDEs算子系数的控制与优化问题奠定了基础，为未来高效数值方法的发展提供了理论支持。

Abstract: Although there is a substantial body of literature on control and
optimization problems for parabolic and hyperbolic systems, the specific
problem of controlling and optimizing the coefficients of the associated
operators within such systems has not yet been thoroughly explored. In this
work, we aim to initiate a line of research in control theory focused on
optimizing and controlling the coefficients of these operators-a problem that
naturally arises in the context of neural networks and supervised learning.
  In supervised learning, the primary objective is to transport initial data
toward target data through the layers of a neural network. We propose a novel
perspective: neural networks can be interpreted as partial differential
equations (PDEs). From this viewpoint, the control problem traditionally
studied in the context of ordinary differential equations (ODEs) is
reformulated as a control problem for PDEs, specifically targeting the
optimization and control of coefficients in parabolic and hyperbolic operators.
To the best of our knowledge, this specific problem has not yet been
systematically addressed in the control theory of PDEs.
  To this end, we propose a dual system formulation for the control and
optimization problem associated with parabolic PDEs, laying the groundwork for
the development of efficient numerical schemes in future research. We also
provide a theoretical proof showing that the control and optimization problem
for parabolic PDEs admits minimizers. Finally, we investigate the control
problem associated with hyperbolic PDEs and prove the existence of solutions
for a corresponding approximated control problem.

</details>


### [165] [Faster Fixed-Point Methods for Multichain MDPs](https://arxiv.org/abs/2506.20910)
*Matthew Zurek,Yudong Chen*

Main category: math.OC

TL;DR: 研究多链马尔可夫决策过程（MDP）的平均奖励问题，提出改进的值迭代算法以解决导航子问题，实现更快收敛。


<details>
  <summary>Details</summary>
Motivation: 多链MDP的平均奖励问题因缺乏收缩性和解的非唯一性而具有理论挑战性，需同时解决导航子问题和长期性能优化。

Method: 开发改进的值迭代算法，结合折扣问题和平均奖励问题的新联系，优化固定点方法，并细化次优分解。

Result: 获得更快的收敛速度和更精确的复杂度度量，扩展了值迭代方法的理论基础。

Conclusion: 算法在多链MDP中表现更优，为折扣和平均奖励问题提供了更快的收敛速度。

Abstract: We study value-iteration (VI) algorithms for solving general (a.k.a.
multichain) Markov decision processes (MDPs) under the average-reward
criterion, a fundamental but theoretically challenging setting. Beyond the
difficulties inherent to all average-reward problems posed by the lack of
contractivity and non-uniqueness of solutions to the Bellman operator, in the
multichain setting an optimal policy must solve the navigation subproblem of
steering towards the best connected component, in addition to optimizing
long-run performance within each component. We develop algorithms which better
solve this navigational subproblem in order to achieve faster convergence for
multichain MDPs, obtaining improved rates of convergence and sharper measures
of complexity relative to prior work. Many key components of our results are of
potential independent interest, including novel connections between
average-reward and discounted problems, optimal fixed-point methods for
discounted VI which extend to general Banach spaces, new sublinear convergence
rates for the discounted value error, and refined suboptimality decompositions
for multichain MDPs. Overall our results yield faster convergence rates for
discounted and average-reward problems and expand the theoretical foundations
of VI approaches.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [166] [Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs](https://arxiv.org/abs/2506.20980)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Weigang Lu*

Main category: cs.SI

TL;DR: 论文提出了一种名为RASH的对比学习框架，用于解决异质图中节点异配性问题，通过双异质超图和动态构建同配/异配图来捕捉高阶语义。


<details>
  <summary>Details</summary>
Motivation: 现实网络中的节点通常具有异配性，但现有方法在异质图中对此问题研究不足，且通常将异质图转换为同质图，导致异质关系中的潜在异配性信息丢失。

Method: RASH框架引入双异质超图编码多关系二分子图，动态构建同配和异配图，并设计多关系对比损失以对齐异质和同配/异配视图。

Result: 实验证明RASH在多种下游任务中有效解决了异质性和异配性问题。

Conclusion: RASH成功解决了异质图中异配性和异质性的双重挑战，代码已开源。

Abstract: Real-world networks usually have a property of node heterophily, that is, the
connected nodes usually have different features or different labels. This
heterophily issue has been extensively studied in homogeneous graphs but
remains under-explored in heterogeneous graphs, where there are multiple types
of nodes and edges. Capturing node heterophily in heterogeneous graphs is very
challenging since both node/edge heterogeneity and node heterophily should be
carefully taken into consideration. Existing methods typically convert
heterogeneous graphs into homogeneous ones to learn node heterophily, which
will inevitably lose the potential heterophily conveyed by heterogeneous
relations. To bridge this gap, we propose Relation-Aware Separation of
Homophily and Heterophily (RASH), a novel contrastive learning framework that
explicitly models high-order semantics of heterogeneous interactions and
adaptively separates homophilic and heterophilic patterns. Particularly, RASH
introduces dual heterogeneous hypergraphs to encode multi-relational bipartite
subgraphs and dynamically constructs homophilic graphs and heterophilic graphs
based on relation importance. A multi-relation contrastive loss is designed to
align heterogeneous and homophilic/heterophilic views by maximizing mutual
information. In this way, RASH simultaneously resolves the challenges of
heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on
benchmark datasets demonstrate the effectiveness of RASH across various
downstream tasks. The code is available at:
https://github.com/zhengziyu77/RASH.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [167] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过系统映射研究总结了敏捷管理在机器学习（ML）系统中的现状，识别了8个关键主题和主要挑战。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的动态性对传统项目管理提出挑战，敏捷方法的灵活性可能适合，但需要定制化应用。

Method: 采用混合搜索策略（数据库搜索与雪球迭代）进行系统映射研究。

Result: 研究识别了27篇论文，归纳出8个框架和主题，主要挑战是ML任务的工作量估算。

Conclusion: 研究填补了领域空白，但需更多实证验证。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [168] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 本文提出了一种基于Python和rdflib库的用户友好方法，用于将Neo4j数据库与OWL本体无缝集成，解决了现有方法需要描述逻辑（DL）语法知识的问题。


<details>
  <summary>Details</summary>
Motivation: 随着数据和知识的快速扩展，系统化的本体生成方法变得至关重要。现有的KNARM方法在Neo4j与OWL集成方面存在挑战，需要更易用的解决方案。

Method: 利用Python和rdflib库开发了一种自动化脚本，从Neo4j数据库（如FDA的FAERS数据）生成OWL本体所需的类和公理。

Result: 成功实现了Neo4j与OWL的无缝集成，并通过FAERS数据集验证了方法的有效性。

Conclusion: 该方法为快速增长的药物不良事件数据集的本体生成提供了实用解决方案，支持药物安全监测和公共卫生决策。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [169] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文介绍了五个基于检索增强生成（RAG）的实际应用案例，并总结了开发过程中的12个关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在事实准确性和上下文相关性方面的局限性，并填补实际应用案例研究的空白。

Method: 开发了五个领域特定的RAG应用，结合多语言OCR、语义检索和领域适应的LLMs，并通过100名用户的网络评估。

Result: 用户评估了六个维度（如易用性、准确性等），并总结了12个影响RAG系统可靠性和可用性的关键挑战。

Conclusion: RAG系统在实际应用中具有潜力，但仍需解决技术、操作和伦理方面的挑战。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [170] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 论文提出了一种结合人类指导的强化学习方法，用于开发复杂的模型转换序列，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 手动开发复杂的模型转换（MT）序列容易出错且不可行，而单纯的强化学习（RL）在复杂问题中表现不佳。人类指导可以弥补这一不足。

Method: 提出了一种技术框架，将用户定义的MT映射到RL原语中，并通过RL程序执行以寻找最优MT序列。

Result: 评估表明，即使人类建议不确定，也能显著提升RL性能，并更高效地开发复杂MT序列。

Conclusion: 该方法通过权衡人类建议的确定性和及时性，为RL驱动的人机协同工程方法迈出了一步。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [171] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 论文提出Synthline v1，通过改进的生成和优化策略提升合成需求数据的质量，评估显示合成数据在某些任务上优于人工数据。


<details>
  <summary>Details</summary>
Motivation: 公开标注的需求数据集稀缺是AI4RE发展的主要障碍，需要系统方法优化合成数据的质量。

Method: 采用增强的Product Line方法（Synthline v1），结合多样本提示、自动提示优化（PACE）和后生成筛选技术。

Result: 多样本提示显著提升数据质量和多样性；PACE优化效果因任务而异；合成数据在安全和缺陷分类任务上优于人工数据。

Conclusion: 合成数据可有效缓解数据集稀缺问题，为AI4RE提供实用解决方案。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [172] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 本文提出了一种名为$T^3$的创新框架，结合大型语言模型（LLMs）的推理能力和树搜索，显著提升了自动程序修复（APR）任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于自动程序修复（APR）任务需要复杂的逻辑和多步推理能力，现有的Chain-of-Thought（CoT）技术在该领域的应用不足。本文旨在通过结合LLMs和树搜索，提升APR任务的性能。

Method: 研究系统评估了多种常见CoT技术在APR任务中的表现，并提出了$T^3$框架，将LLMs的推理能力与树搜索相结合。

Result: $T^3$框架有效提高了生成候选修复方案的准确性，并为优化APR任务中的样本选择和修复策略提供了指导。

Conclusion: $T^3$框架为高效自动化调试建立了坚实基础，展示了LLMs与树搜索结合的潜力。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [173] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: 研究探讨了数据源多样性对加密货币预测模型性能的影响，通过整合多种数据类别，发现多样性显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 理解不同数据源对加密货币市场预测的影响，以开发更准确的预测模型。

Method: 引入Crypto100指数，提出特征降维算法，整合技术指标、链上指标、情感和兴趣指标、传统市场指数及宏观经济指标。

Result: 数据源多样性显著提升模型预测性能，链上指标对短期和长期预测至关重要，传统市场指数和宏观经济指标对长期预测更相关。

Conclusion: 研究揭示了加密货币市场的驱动因素，为开发更准确和稳健的预测模型奠定了基础。

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [174] [IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation](https://arxiv.org/abs/2506.20696)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: physics.med-ph

TL;DR: IMC-PINN-FE是一种结合物理信息神经网络（PINN）和有限元（FE）建模的框架，用于快速、个性化且图像一致的心脏生物力学建模，显著提高了计算效率和运动匹配精度。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法计算成本高且难以准确模拟心脏运动，因此需要一种更高效、精确的方法来模拟心肌的生物力学行为。

Method: 提出IMC-PINN-FE框架，结合图像运动一致性（IMC）和FE建模，通过预训练网络或非监督网络从MRI或超声心动图中估计心脏运动，并快速计算心肌刚度和主动张力。

Result: IMC-PINN-FE将计算时间从小时缩短到秒，运动匹配精度（Dice系数）从0.849提升到0.927，同时保持真实的压力-体积行为。

Conclusion: IMC-PINN-FE提供了一种高效、个性化的心脏生物力学建模方法，显著优于传统方法。

Abstract: Elucidating the biomechanical behavior of the myocardium is crucial for
understanding cardiac physiology, but cannot be directly inferred from clinical
imaging and typically requires finite element (FE) simulations. However,
conventional FE methods are computationally expensive and often fail to
reproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed
neural network (PINN) framework that integrates imaged motion consistency (IMC)
with FE modeling for patient-specific left ventricular (LV) biomechanics.
Cardiac motion is first estimated from MRI or echocardiography using either a
pre-trained attention-based network or an unsupervised cyclic-regularized
network, followed by extraction of motion modes. IMC-PINN-FE then rapidly
estimates myocardial stiffness and active tension by fitting clinical pressure
measurements, accelerating computation from hours to seconds compared to
traditional inverse FE. Based on these parameters, it performs FE modeling
across the cardiac cycle at 75x speedup. Through motion constraints, it matches
imaged displacements more accurately, improving average Dice from 0.849 to
0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE
advances previous PINN-FE models by introducing back-computation of material
properties and better motion fidelity. Using motion from a single subject to
reconstruct shape modes also avoids the need for large datasets and improves
patient specificity. IMC-PINN-FE offers a robust and efficient approach for
rapid, personalized, and image-consistent cardiac biomechanical modeling.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [175] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉语言模型（VLMs）从扩散模型生成的高分辨率图像中选择最可信样本的自动化框架，并通过混合指标（TWS）量化其可靠性。


<details>
  <summary>Details</summary>
Motivation: 超分辨率（SR）问题存在多解性，传统方法在平衡保真度和感知质量时可能引入信息模糊的伪影，而扩散模型生成的多样性样本难以选择最可信解。

Method: 利用VLMs（如BLIP-2、GPT-4o）通过结构化查询评估语义正确性、视觉质量和伪影存在，并通过混合指标TWS（结合CLIP嵌入、SSIM和小波分解）量化可靠性。

Result: 实验表明TWS与人类偏好高度相关，且VLM引导的选择能持续获得高TWS值，优于传统指标如PSNR和LPIPS。

Conclusion: 该方法为生成式SR提供了可扩展、通用的可信解选择方案，设定了新的可信度基准。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [176] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: FixCLR是一种新的半监督域泛化方法，通过对比学习显式正则化域不变表示，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督域泛化中标签稀缺导致的性能不足问题，现有方法未显式正则化域不变表示。

Method: 引入FixCLR，结合伪标签类别信息和排斥项，适配对比学习以实现显式域不变正则化。

Result: FixCLR显著提升性能，尤其在与其他半监督方法结合时表现更佳。

Conclusion: FixCLR是有效的半监督域泛化方法，适用于多领域数据集。

Abstract: Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [177] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Main category: cs.CV

TL;DR: ThirdEye是一种单目深度估计方法，通过显式提供视觉线索（如遮挡边界、阴影和透视）并融合这些线索，生成高分辨率视差图。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖隐式学习，忽略了人类视觉系统依赖的显式线索。ThirdEye旨在通过预训练和冻结的专家网络显式提供这些线索。

Method: 使用预训练和冻结的专家网络提供视觉线索，通过三阶段皮层层次（V1->V2->V3）和键值工作记忆模块融合线索，最终由自适应分箱变换器生成视差图。

Result: ThirdEye继承了外部监督的优势，仅需少量微调。

Conclusion: ThirdEye通过显式线索和分层融合，提高了单目深度估计的性能。

Abstract: Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.

</details>


### [178] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/abs/2506.20960)
*Yiman Zhang,Ziheng Luo,Qiangyu Yan,Wei He,Borui Jiang,Xinghao Chen,Kai Han*

Main category: cs.CV

TL;DR: OmniEval是一个用于评估全模态模型（如MiniCPM-O 2.6）的基准测试，涵盖视觉、听觉和文本输入，具有全模态协作、视频多样性和任务多样性等特点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分评估全模态模型的协作能力，因此设计了OmniEval以填补这一空白。

Method: 设计了包括2617个问答对的评估任务，涵盖3大类任务和12个子任务，特别引入了细粒度的视频定位任务Grounding。

Result: 实验在OmniEval上对多个全模态模型进行了测试，展示了其评估能力。

Conclusion: OmniEval为评估全模态模型的上下文理解和构建能力提供了平台。

Abstract: In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at https://omnieval.github.io/.

</details>


### [179] [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964)
*Chengkuan Chen,Luca L. Weishaupt,Drew F. K. Williamson,Richard J. Chen,Tong Ding,Bowen Chen,Anurag Vaidya,Long Phi Le,Guillaume Jaume,Ming Y. Lu,Faisal Mahmood*

Main category: cs.CV

TL;DR: PathChat+是一种专为病理学设计的新型多模态大语言模型，通过大量病理学指令样本和问答训练，显著优于现有模型，并支持多图像理解和自主诊断推理。


<details>
  <summary>Details</summary>
Motivation: 传统病理学模型缺乏自然语言指令和文本上下文整合，现有MLLMs在训练数据、多图像理解和支持自主诊断推理方面存在不足。

Method: 开发PathChat+，基于超过100万病理学指令样本和550万问答训练，并推出SlideSeek系统，通过分层诊断推理自主评估WSIs。

Result: PathChat+在多个病理学基准测试中显著优于现有模型，SlideSeek在DDxBench上达到高准确性，并能生成可视化报告。

Conclusion: PathChat+和SlideSeek为病理学提供了更强大的多模态理解和诊断推理能力，推动了数字病理学的发展。

Abstract: Pathology is experiencing rapid digital transformation driven by whole-slide
imaging and artificial intelligence (AI). While deep learning-based
computational pathology has achieved notable success, traditional models
primarily focus on image analysis without integrating natural language
instruction or rich, text-based context. Current multimodal large language
models (MLLMs) in computational pathology face limitations, including
insufficient training data, inadequate support and evaluation for multi-image
understanding, and a lack of autonomous, diagnostic reasoning capabilities. To
address these limitations, we introduce PathChat+, a new MLLM specifically
designed for human pathology, trained on over 1 million diverse,
pathology-specific instruction samples and nearly 5.5 million question answer
turns. Extensive evaluations across diverse pathology benchmarks demonstrated
that PathChat+ substantially outperforms the prior PathChat copilot, as well as
both state-of-the-art (SOTA) general-purpose and other pathology-specific
models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI
system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide
images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching
high accuracy on DDxBench, a challenging open-ended differential diagnosis
benchmark, while also capable of generating visually grounded,
humanly-interpretable summary reports.

</details>


### [180] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Main category: cs.CV

TL;DR: DFVEdit是一种高效的无训练视频编辑方法，专为Video DiTs设计，通过流变换直接操作潜在空间，显著提升计算效率和编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在Video DiTs上计算开销大，需要修改注意力机制或微调，DFVEdit旨在解决这一问题。

Method: 提出Conditional Delta Flow Vector (CDFV)和结合Implicit Cross Attention (ICA)与Embedding Reinforcement (ER)，直接在潜在空间操作。

Result: DFVEdit在推理速度上提升20倍，内存减少85%，并在多个指标上达到SOTA性能。

Conclusion: DFVEdit为Video DiTs提供了一种高效、高质量的零样本编辑解决方案。

Abstract: The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in
video generation. However, directly applying existing video editing methods to
Video DiTs often incurs substantial computational overhead, due to
resource-intensive attention modification or finetuning. To alleviate this
problem, we present DFVEdit, an efficient zero-shot video editing method
tailored for Video DiTs. DFVEdit eliminates the need for both attention
modification and fine-tuning by directly operating on clean latents via flow
transformation. To be more specific, we observe that editing and sampling can
be unified under the continuous flow perspective. Building upon this
foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a
theoretically unbiased estimation of DFV -- and integrate Implicit Cross
Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further
enhance editing quality. DFVEdit excels in practical efficiency, offering at
least 20x inference speed-up and 85\% memory reduction on Video DiTs compared
to attention-engineering-based editing methods. Extensive quantitative and
qualitative experiments demonstrate that DFVEdit can be seamlessly applied to
popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art
performance on structural fidelity, spatial-temporal consistency, and editing
quality.

</details>


### [181] [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
*Qize Yang,Shimin Yao,Weixuan Chen,Shenghao Fu,Detao Bai,Jiaxing Zhao,Boyuan Sun,Bowen Yin,Xihan Wei,Jingren Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种通过强化学习增强多模态大语言模型推理能力的方法，解决了全局上下文理解不足和捷径问题，并在多模态基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在理解人类意图时存在全局上下文理解不足和捷径问题，需要改进推理能力。

Method: 引入上下文奖励、格式奖励和准确性奖励，利用大语言模型评估逻辑奖励，并提出多模态基准测试IntentBench。

Result: 所提方法在多模态基准测试中表现优于其他开源模型。

Conclusion: 通过强化学习结合多模态上下文理解，显著提升了模型的推理能力和性能。

Abstract: With the rapid evolution of multimodal large language models, the capacity to
deeply understand and interpret human intentions has emerged as a critical
capability, which demands detailed and thoughtful reasoning. In recent studies,
Reinforcement Learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of Large Language Models (LLMs). Nonetheless, the
challenges associated with adapting RL to multimodal data and formats remain
largely unaddressed. In this paper, we identify two issues in existing
multimodal reasoning models: insufficient global context understanding and
shortcut problems. Insufficient context understanding can happen when a model
misinterprets multimodal context, resulting in incorrect answers. The shortcut
problem occurs when the model overlooks crucial clues in multimodal inputs,
directly addressing the query without considering the multimodal information.
To tackle these issues, we emphasize the necessity for the model to reason with
a clear understanding of the global context within multimodal inputs. This
global context understanding can effectively prevent the model from overlooking
key multimodal cues and ensure a thorough reasoning process. To ensure the
accurate interpretation of multimodal context information, we implement a
context reward judged by a large language model, alongside format and accuracy
rewards. Additionally, to improve complex reasoning capability, we employ the
LLM to assess the logical reward, determining whether the reasoning process
successfully integrates multimodal information with logical methods. We also
introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating
models in understanding complex human intentions and emotions. Our proposed
method demonstrates advanced performance across multiple omni-modal benchmarks
compared to other open-source omni-modal models.

</details>


### [182] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Cradle2Cane的两阶段人脸老化框架，通过自适应噪声注入和身份感知嵌入，解决了年龄准确性和身份保留的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸老化方法在年龄准确性和身份保留之间存在权衡问题，难以实现真实且无缝的跨年龄段转换。

Method: 采用两阶段框架：第一阶段通过自适应噪声注入（AdaNI）实现年龄准确性；第二阶段通过身份感知嵌入（IDEmb）增强身份保留。

Result: 在CelebA-HQ测试集上，Cradle2Cane在年龄准确性和身份一致性方面优于现有方法。

Conclusion: Cradle2Cane通过两阶段设计有效平衡了年龄准确性和身份保留，为人脸老化任务提供了新的解决方案。

Abstract: Face aging has become a crucial task in computer vision, with applications
ranging from entertainment to healthcare. However, existing methods struggle
with achieving a realistic and seamless transformation across the entire
lifespan, especially when handling large age gaps or extreme head poses. The
core challenge lies in balancing age accuracy and identity preservation--what
we refer to as the Age-ID trade-off. Most prior methods either prioritize age
transformation at the expense of identity consistency or vice versa. In this
work, we address this issue by proposing a two-pass face aging framework, named
Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first
pass focuses on solving age accuracy by introducing an adaptive noise injection
(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of
age and gender for the given person as the textual condition. Also, by
adjusting the noise level, we can control the strength of aging while allowing
more flexibility in transforming the face. However, identity preservation is
weakly ensured here to facilitate stronger age transformations. In the second
pass, we enhance identity preservation while maintaining age-specific features
by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace
and Rotate-CLIP. This pass allows for denoising the transformed image from the
first pass, ensuring stronger identity preservation without compromising the
aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive
experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL
protocols, show that our Cradle2Cane outperforms existing face aging methods in
age accuracy and identity consistency.

</details>


### [183] [Segment Anything in Pathology Images with Natural Language](https://arxiv.org/abs/2506.20988)
*Zhixuan Chen,Junlin Hou,Liqi Lin,Yihui Wang,Yequan Bie,Xi Wang,Yanning Zhou,Ronald Cheong Kin Chan,Hao Chen*

Main category: cs.CV

TL;DR: PathSegmentor是一种基于文本提示的病理图像分割基础模型，解决了标注数据有限和类别定义受限的问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 病理图像分割在癌症诊断和预后中至关重要，但现有方法因标注数据不足和类别限制难以临床应用。

Method: 提出PathSegmentor模型和PathSeg数据集，支持自然语言提示的分割，无需空间输入。

Result: PathSegmentor在Dice分数上显著优于现有模型，并提升了诊断模型的可解释性。

Conclusion: PathSegmentor推动了精准肿瘤学中可解释AI的发展，为临床决策提供支持。

Abstract: Pathology image segmentation is crucial in computational pathology for
analyzing histological features relevant to cancer diagnosis and prognosis.
However, current methods face major challenges in clinical applications due to
limited annotated data and restricted category definitions. To address these
limitations, we propose PathSegmentor, the first text-prompted segmentation
foundation model designed specifically for pathology images. We also introduce
PathSeg , the largest and most comprehensive dataset for pathology
segmentation, built from 17 public sources and containing 275k image-mask-label
triples across 160 diverse categories. With PathSegmentor, users can perform
semantic segmentation using natural language prompts, eliminating the need for
laborious spatial inputs such as points or boxes. Extensive experiments
demonstrate that PathSegmentor outperforms specialized models with higher
accuracy and broader applicability, while maintaining a compact architecture.
It significantly surpasses existing spatial- and text-prompted models by 0.145
and 0.429 in overall Dice scores, respectively, showing strong robustness in
segmenting complex structures and generalizing to external datasets. Moreover,
PathSegmentor's outputs enhance the interpretability of diagnostic models
through feature importance estimation and imaging biomarker discovery, offering
pathologists evidence-based support for clinical decision-making. This work
advances the development of explainable AI in precision oncology.

</details>


### [184] [Multimodal Prompt Alignment for Facial Expression Recognition](https://arxiv.org/abs/2506.21017)
*Fuyan Ma,Yiran He,Bin Sun,Shutao Li*

Main category: cs.CV

TL;DR: 提出了一种多模态提示对齐框架MPA-FER，通过细粒度语义指导和跨模态对齐，提升视觉语言模型在面部表情识别中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在面部表情识别中难以捕捉细粒度的文本-视觉关系，导致对细微表情差异的区分能力不足。

Method: 结合多粒度硬提示生成策略和LLM外部知识，通过特征差异最小化和原型引导的视觉特征对齐，优化软提示学习。

Result: 在三个FER基准数据集上表现优于现有方法，同时保持了预训练模型的泛化能力并降低了计算成本。

Conclusion: MPA-FER框架通过细粒度语义指导和跨模态对齐，显著提升了面部表情识别的准确性和可解释性。

Abstract: Prompt learning has been widely adopted to efficiently adapt vision-language
models (VLMs) like CLIP for various downstream tasks. Despite their success,
current VLM-based facial expression recognition (FER) methods struggle to
capture fine-grained textual-visual relationships, which are essential for
distinguishing subtle differences between facial expressions. To address this
challenge, we propose a multimodal prompt alignment framework for FER, called
MPA-FER, that provides fine-grained semantic guidance to the learning process
of prompted visual features, resulting in more precise and interpretable
representations. Specifically, we introduce a multi-granularity hard prompt
generation strategy that utilizes a large language model (LLM) like ChatGPT to
generate detailed descriptions for each facial expression. The LLM-based
external knowledge is injected into the soft prompts by minimizing the feature
discrepancy between the soft prompts and the hard prompts. To preserve the
generalization abilities of the pretrained CLIP model, our approach
incorporates prototype-guided visual feature alignment, ensuring that the
prompted visual features from the frozen image encoder align closely with
class-specific prototypes. Additionally, we propose a cross-modal global-local
alignment module that focuses on expression-relevant facial features, further
improving the alignment between textual and visual features. Extensive
experiments demonstrate our framework outperforms state-of-the-art methods on
three FER benchmark datasets, while retaining the benefits of the pretrained
model and minimizing computational costs.

</details>


### [185] [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
*Perifanos Konstantinos,Goutsos Dionisis*

Main category: cs.CV

TL;DR: 提出了一种针对希腊多音调文本的光学字符识别（OCR）系统，结合卷积层和循环层，显著提升了识别精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统OCR方法在处理希腊多音调文本时存在局限性，需要更高效的解决方案。

Method: 结合卷积层进行特征提取和循环层进行序列学习。

Result: 系统在准确性和效率上有显著提升，并开源了模型和平台。

Conclusion: 该系统为希腊多音调文本的数字化提供了高效工具，并支持学术使用。

Abstract: In this paper, we present an Optical Character Recognition (OCR) system
specifically designed for the accurate recognition and digitization of Greek
polytonic texts. By leveraging the combined strengths of convolutional layers
for feature extraction and recurrent layers for sequence learning, our system
addresses the unique challenges posed by Greek polytonic scripts. This approach
aims to overcome the limitations of traditional OCR methods, offering
significant improvements in accuracy and efficiency. We release the underlying
model as an open-source library and make our OCR platform available for
academic use.

</details>


### [186] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: HalluSegBench是一个新基准，用于评估视觉语言分割模型中的幻觉问题，通过反事实视觉推理揭示模型在视觉驱动幻觉上的普遍性。


<details>
  <summary>Details</summary>
Motivation: 现有评估协议主要关注标签或文本幻觉，缺乏对视觉上下文的操控，无法诊断关键失败。

Method: 提出HalluSegBench，包含1340对反事实实例和281个对象类的新数据集，以及量化幻觉敏感性的新指标。

Result: 实验显示视觉驱动幻觉比标签驱动更普遍，模型常持续错误分割。

Conclusion: 反事实推理对诊断视觉语言分割模型的真实性至关重要。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [187] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 提出了一种名为FGS的方法，通过忠实度引导和调度策略，在图像编辑中平衡可编辑性和忠实度，实现高质量的图像合成。


<details>
  <summary>Details</summary>
Motivation: 解决文本引导扩散模型在图像编辑中可编辑性与忠实度之间的固有权衡问题。

Method: 引入忠实度引导以增强输入图像信息的保留，并采用调度策略解决可编辑性与忠实度的不匹配问题。

Result: 实验证明FGS在保持可编辑性的同时显著提升了忠实度，且兼容多种编辑方法。

Conclusion: FGS是一种高效的方法，适用于多样化的图像编辑任务，能够实现精确且高质量的编辑效果。

Abstract: Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.

</details>


### [188] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Main category: cs.CV

TL;DR: EgoAdapt框架通过跨模态蒸馏和策略学习，显著提升了多感知自我中心任务的效率，同时保持或超越现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现代感知模型在多感知自我中心任务中性能优异，但计算成本高，难以在资源受限环境中部署。

Method: 提出EgoAdapt框架，结合跨模态蒸馏和策略学习，适应不同任务的动作空间。

Result: 在三个数据集上测试，效率显著提升（GMACs减少89.09%，参数减少82.02%，能耗降低9.6倍），性能与或优于现有模型。

Conclusion: EgoAdapt为资源受限环境中的高效多感知任务提供了可行解决方案。

Abstract: Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.

</details>


### [189] [IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes](https://arxiv.org/abs/2506.21116)
*Yujia Liang,Jile Jiao,Zhicheng Wang,Xuetao Feng,Zixuan Ye,Yuan Wang,Hao Lu*

Main category: cs.CV

TL;DR: 论文提出新数据集MultiClip-Bench和新模型IPFormer-VideoLLM，以解决VideoLLMs在多镜头场景中的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs在多镜头场景（如不同视角或场景切换）中表现不佳，主要原因是缺乏相关数据集和模型对实例特征的离散编码。

Method: 引入MultiClip-Bench数据集，并提出IPFormer-VideoLLM模型，通过实例提示和注意力机制聚合跨场景信息。

Result: 实验表明，新数据集和模型显著提升了多场景视频理解能力，并在多个视频基准测试中表现优异。

Conclusion: MultiClip-Bench和IPFormer-VideoLLM为多镜头视频理解提供了有效解决方案，具有广泛的应用潜力。

Abstract: Video Large Language Models (VideoLLMs) have demonstrated remarkable
understanding capabilities, but are found struggling to tackle multi-shot
scenarios,e.g., video clips with varying camera angles or scene changes. This
challenge can render failures such as instance identity forgetting and key
frame negligence. In this work, we first attribute the challenge to the lack of
multi-shot annotations among existing datasets and therefore we introduce a new
dataset termed MultiClip-Bench, featuring dense descriptions and
instruction-based question-answering pairs tailored for multi-shot scenarios.
We empirically find that the training set significantly boosts the multi-shot
performance, while the testing benchmark provides a reliable measure of the
model capability in multi-shot scenarios. By further analyzing and discovering
that current models only encode instance features in a discrete or lossy
manner, at the risk of missing identity information, we then contribute a new
model IPFormer-VideoLLM. Its key idea is the injection of instance-level
features as instance prompts through an efficient attention-based connector.
This allows for the aggregation of instance-specific information across scenes.
Experiments demonstrate that our proposed dataset and model not only enhance
the multi-scene video understanding significantly, but also offer distinct
advantages across various video benchmarks.

</details>


### [190] [Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels](https://arxiv.org/abs/2506.21151)
*Aida Moafi,Danial Moafi,Evgeny M. Mirkes,Gerry P. McCann,Abbas S. Alatrany,Jayanth R. Arnold,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的自动化心肌瘢痕分割方法，通过改进损失函数和数据增强解决标签噪声和数据异质性，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 心肌瘢痕的准确分割对临床评估和治疗规划至关重要，但现有方法面临标签噪声、数据异质性和类别不平衡的挑战。

Method: 通过微调先进模型，使用Kullback-Leibler损失和广泛数据增强，解决标签噪声和数据异质性问题。

Result: 在急性和慢性病例中表现优异，优于nnU-Net等模型，并在分布外测试集上展现强泛化能力。

Conclusion: 该方法为自动化心肌瘢痕量化提供了可靠基础，支持深度学习在心脏影像中的广泛应用。

Abstract: The accurate segmentation of myocardial scars from cardiac MRI is essential
for clinical assessment and treatment planning. In this study, we propose a
robust deep-learning pipeline for fully automated myocardial scar detection and
segmentation by fine-tuning state-of-the-art models. The method explicitly
addresses challenges of label noise from semi-automatic annotations, data
heterogeneity, and class imbalance through the use of Kullback-Leibler loss and
extensive data augmentation. We evaluate the model's performance on both acute
and chronic cases and demonstrate its ability to produce accurate and smooth
segmentations despite noisy labels. In particular, our approach outperforms
state-of-the-art models like nnU-Net and shows strong generalizability in an
out-of-distribution test set, highlighting its robustness across various
imaging conditions and clinical tasks. These results establish a reliable
foundation for automated myocardial scar quantification and support the broader
clinical adoption of deep learning in cardiac imaging.

</details>


### [191] [Task-Aware KV Compression For Cost-Effective Long Video Understanding](https://arxiv.org/abs/2506.21184)
*Minghao Qin,Yan Shu,Peitian Zhang,Kun Lun,Huaying Yuan,Juenjie Zhou,Shitao Xiao,Bo Zhao,Zheng Liu*

Main category: cs.CV

TL;DR: Video-X^2L通过双层KV压缩和选择性KV重加载，高效解决长视频理解中的计算成本问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中因高计算成本导致的性能瓶颈，同时减少信息丢失。

Method: 采用双层KV压缩（生成低压缩和高压缩KV）和选择性KV重加载策略，动态平衡信息保留与计算效率。

Result: 在多个长视频理解基准测试中表现优异，显著节省计算成本。

Conclusion: Video-X^2L无需额外训练，兼容现有模型，是一种高效的长视频理解解决方案。

Abstract: Long-video understanding (LVU) remains a severe challenge for existing
multimodal large language models (MLLMs), primarily due to the prohibitive
computational cost. Recent approaches have explored KV compression to mitigate
this issue, but they often suffer from significant information loss at high
compression ratios. In this paper, we introduce Video-X^2L, which flexibly
preserves critical video information for each LVU task. Video-X^2L involves two
key operations. The first one is called bi-level KV compression. During the
MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:
low-compression KVs (L-KVs) to capture fine-grained video details and
high-compression KVs (H-KVs) to offer compact video representations. The second
one is called selective KV re-loading. During the MLLM's decoding stage,
Video-X^2L selectively re-loads L-KVs for the most critical video chunks while
using H-KVs for other less important ones. This allows the MLLM to fully
utilize task-specific information while maintaining the overall compactness.
Video-X^2L is simple yet effective: it is free from additional training and
directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L
with a variety of popular LVU benchmarks, including VideoMME, MLVU,
LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L
outperforms existing KV-compression methods by a huge advantage while
substantially saving the computation cost.

</details>


### [192] [Transferring disentangled representations: bridging the gap between synthetic and real images](https://arxiv.org/abs/2409.18017)
*Jacopo Dapueto,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 研究探讨了如何利用合成数据学习适用于真实数据的解耦表示，并提出了新的度量方法。


<details>
  <summary>Details</summary>
Motivation: 解耦表示学习在真实图像上未充分发挥潜力，主要由于生成因素的相关性、分辨率和缺乏真实标签。

Method: 通过合成数据学习解耦表示，研究微调效果及解耦属性在迁移后的保留情况，并提出新的干预度量方法。

Result: 实验表明，从合成数据迁移到真实数据的解耦表示是可行且有效的。

Conclusion: 解耦表示的部分属性可以在迁移中保留，为真实数据应用提供了可能性。

Abstract: Developing meaningful and efficient representations that separate the
fundamental structure of the data generation mechanism is crucial in
representation learning. However, Disentangled Representation Learning has not
fully shown its potential on real images, because of correlated generative
factors, their resolution and limited access to ground truth labels.
Specifically on the latter, we investigate the possibility of leveraging
synthetic data to learn general-purpose disentangled representations applicable
to real data, discussing the effect of fine-tuning and what properties of
disentanglement are preserved after the transfer. We provide an extensive
empirical study to address these issues. In addition, we propose a new
interpretable intervention-based metric, to measure the quality of factors
encoding in the representation. Our results indicate that some level of
disentanglement, transferring a representation from synthetic to real data, is
possible and effective.

</details>


### [193] [BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models](https://arxiv.org/abs/2506.21209)
*Louis Kerner,Michel Meintz,Bihe Zhao,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 论文提出了一种名为BitMark的位级水印框架，用于防止文本到图像模型（如Infinity）生成的图像被重复训练导致模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型生成的图像在互联网上广泛传播，这些图像可能被重新用作训练数据，导致模型性能逐渐退化（模型崩溃）。水印技术可以作为一种缓解策略。

Method: BitMark通过在Infinity图像生成过程中，在多个尺度（分辨率）的令牌流中嵌入位级水印，保持视觉保真度和生成速度，同时对多种去除技术具有鲁棒性。

Result: BitMark不仅能够可靠地检测生成内容，还具有高放射性，即使用水印图像训练的新模型也会携带水印。

Conclusion: BitMark为解决图像生成模型的模型崩溃问题提供了一种有效方法，通过可靠检测生成内容来防止性能退化。

Abstract: State-of-the-art text-to-image models like Infinity generate photorealistic
images at an unprecedented speed. These models operate in a bitwise
autoregressive manner over a discrete set of tokens that is practically
infinite in size. However, their impressive generative power comes with a
growing risk: as their outputs increasingly populate the Internet, they are
likely to be scraped and reused as training data-potentially by the very same
models. This phenomenon has been shown to lead to model collapse, where
repeated training on generated content, especially from the models' own
previous versions, causes a gradual degradation in performance. A promising
mitigation strategy is watermarking, which embeds human-imperceptible yet
detectable signals into generated images-enabling the identification of
generated content. In this work, we introduce BitMark, a robust bitwise
watermarking framework for Infinity. Our method embeds a watermark directly at
the bit level of the token stream across multiple scales (also referred to as
resolutions) during Infinity's image generation process. Our bitwise watermark
subtly influences the bits to preserve visual fidelity and generation speed
while remaining robust against a spectrum of removal techniques. Furthermore,
it exhibits high radioactivity, i.e., when watermarked generated images are
used to train another image generative model, this second model's outputs will
also carry the watermark. The radioactive traces remain detectable even when
only fine-tuning diffusion or image autoregressive models on images watermarked
with our BitMark. Overall, our approach provides a principled step toward
preventing model collapse in image generative models by enabling reliable
detection of generated outputs.

</details>


### [194] [Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models](https://arxiv.org/abs/2506.21330)
*Haoyang Wu,Tsun-Hsuan Wang,Mathias Lechner,Ramin Hasani,Jennifer A. Eckhoff,Paul Pak,Ozanan R. Meireles,Guy Rosman,Yutong Ban,Daniela Rus*

Main category: cs.CV

TL;DR: 提出了一种基于层次化状态空间模型的新方法，用于高效处理长时间手术视频，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术的视频分析因时长问题难以全面处理，现有Transformer模型的二次注意力机制效率不足。

Method: 采用层次化状态空间模型，结合局部和全局动态捕捉模块，并使用混合离散-连续监督策略训练。

Result: 在多个数据集上表现优异（Cholec80 +2.8%，MICCAI2016 +4.3%，Heichole +12.9%）。

Conclusion: 该方法显著提升了长时间手术视频分析的效率和准确性，代码将公开。

Abstract: Surgical workflow analysis is essential in robot-assisted surgeries, yet the
long duration of such procedures poses significant challenges for comprehensive
video analysis. Recent approaches have predominantly relied on transformer
models; however, their quadratic attention mechanism restricts efficient
processing of lengthy surgical videos. In this paper, we propose a novel
hierarchical input-dependent state space model that leverages the linear
scaling property of state space models to enable decision making on full-length
videos while capturing both local and global dynamics. Our framework
incorporates a temporally consistent visual feature extractor, which appends a
state space model head to a visual feature extractor to propagate temporal
information. The proposed model consists of two key modules: a
local-aggregation state space model block that effectively captures intricate
local dynamics, and a global-relation state space model block that models
temporal dependencies across the entire video. The model is trained using a
hybrid discrete-continuous supervision strategy, where both signals of discrete
phase labels and continuous phase progresses are propagated through the
network. Experiments have shown that our method outperforms the current
state-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on
MICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available
after paper acceptance.

</details>


### [195] [CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection](https://arxiv.org/abs/2506.21364)
*Zhixin Cheng,Jiacheng Deng,Xinjun Li,Xiaotian Yin,Bohao Liao,Baoqun Yin,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种改进图像到点云配准的方法，通过通道自适应调整模块（CAA）和全局最优选择模块（GOS）解决特征匹配中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有检测自由方法在图像和点云特征匹配中因通道注意力差异和场景结构相似性导致匹配结果下降和冗余对应关系。

Method: 提出CAA模块增强模态内特征并抑制跨模态敏感性，GOS模块用全局优化替代局部选择。

Result: 在RGB-D Scenes V2和7-Scenes数据集上实现最先进的配准性能。

Conclusion: CAA和GOS模块有效提升了图像到点云的配准精度。

Abstract: Detection-free methods typically follow a coarse-to-fine pipeline, extracting
image and point cloud features for patch-level matching and refining dense
pixel-to-point correspondences. However, differences in feature channel
attention between images and point clouds may lead to degraded matching
results, ultimately impairing registration accuracy. Furthermore, similar
structures in the scene could lead to redundant correspondences in cross-modal
matching. To address these issues, we propose Channel Adaptive Adjustment
Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances
intra-modal features and suppresses cross-modal sensitivity, while GOS replaces
local selection with global optimization. Experiments on RGB-D Scenes V2 and
7-Scenes demonstrate the superiority of our method, achieving state-of-the-art
performance in image-to-point cloud registration.

</details>


### [196] [Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](https://arxiv.org/abs/2506.20995)
*Akio Hayakawa,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种逐步生成视频到音频的方法，通过分步生成特定声音事件的音频轨道，模仿传统Foley工作流程，并利用文本提示和先前生成的音频进行引导。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以全面捕捉视频中的所有声音事件，因此需要一种能够分步生成并组合音频的方法。

Method: 采用分步视频到音频合成任务，结合目标文本提示和已生成音频，利用预训练模型避免对专用数据集的需求。

Result: 实验表明，该方法能为单个视频生成多个语义不同的音频轨道，合成质量优于现有基线。

Conclusion: 该方法通过分步生成和引导合成，显著提升了视频到音频生成的质量和多样性。

Abstract: We propose a novel step-by-step video-to-audio generation method that
sequentially produces individual audio tracks, each corresponding to a specific
sound event in the video. Our approach mirrors traditional Foley workflows,
aiming to capture all sound events induced by a given video comprehensively.
Each generation step is formulated as a guided video-to-audio synthesis task,
conditioned on a target text prompt and previously generated audio tracks. This
design is inspired by the idea of concept negation from prior compositional
generation frameworks. To enable this guided generation, we introduce a
training framework that leverages pre-trained video-to-audio models and
eliminates the need for specialized paired datasets, allowing training on more
accessible data. Experimental results demonstrate that our method generates
multiple semantically distinct audio tracks for a single input video, leading
to higher-quality composite audio synthesis than existing baselines.

</details>


### [197] [TITAN: Query-Token based Domain Adaptive Adversarial Learning](https://arxiv.org/abs/2506.21484)
*Tajamul Ashraf,Janibul Bashir*

Main category: cs.CV

TL;DR: 论文提出了一种名为TITAN的方法，用于解决源数据不可用时的无源域自适应目标检测问题，通过将目标图像分为易和难两类，并利用查询令牌对抗模块减少域间差异。


<details>
  <summary>Details</summary>
Motivation: 解决在源数据不可用时，现有基于学生-教师框架的方法因伪标签噪声导致性能下降的问题。

Method: 提出TITAN方法，将目标图像分为易和难两类，估计方差进行划分，并引入查询令牌对抗模块以减少域间差异。

Result: 在多个数据集上验证，TITAN性能显著优于现有方法，mAP提升最高达22.7%。

Conclusion: TITAN通过有效划分目标域和减少域差异，显著提升了无源域自适应目标检测的性能。

Abstract: We focus on the source-free domain adaptive object detection (SF-DAOD)
problem when source data is unavailable during adaptation and the model must
adapt to an unlabeled target domain. The majority of approaches for the problem
employ a self-supervised approach using a student-teacher (ST) framework where
pseudo-labels are generated via a source-pretrained model for further
fine-tuning. We observe that the performance of a student model often degrades
drastically, due to the collapse of the teacher model, primarily caused by high
noise in pseudo-labels, resulting from domain bias, discrepancies, and a
significant domain shift across domains. To obtain reliable pseudo-labels, we
propose a Target-based Iterative Query-Token Adversarial Network (TITAN), which
separates the target images into two subsets: those similar to the source
(easy) and those dissimilar (hard). We propose a strategy to estimate variance
to partition the target domain. This approach leverages the insight that higher
detection variances correspond to higher recall and greater similarity to the
source domain. Also, we incorporate query-token-based adversarial modules into
a student-teacher baseline framework to reduce the domain gaps between two
feature representations. Experiments conducted on four natural imaging datasets
and two challenging medical datasets have substantiated the superior
performance of TITAN compared to existing state-of-the-art (SOTA)
methodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7
percent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,
respectively.

</details>


### [198] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 提出了一种经典-量子混合的生成对抗网络（GAN），能够生成彩色医学图像，解决了量子图像生成的低质量限制，并在性能和效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病检测中，机器学习需要大量高质量数据，但现有数据集存在类别不平衡、隐私问题和对象偏差，数据增强是关键。传统生成模型计算资源消耗大，量子计算虽有潜力但现有方法只能生成低质量灰度图像。

Method: 采用经典-量子潜在空间融合技术，构建首个能生成彩色医学图像的经典-量子GAN。

Result: 模型在图像生成质量和分类性能提升上优于传统深度卷积GAN和现有混合量子GAN，且参数和训练周期大幅减少。在真实IBM量子机器上表现稳健。

Conclusion: 该技术为量子图像生成展示了广阔前景，随着量子硬件发展，其潜力将进一步释放。

Abstract: Machine learning-assisted diagnosis is gaining traction in skin disease
detection, but training effective models requires large amounts of high-quality
data. Skin disease datasets often suffer from class imbalance, privacy
concerns, and object bias, making data augmentation essential. While classical
generative models are widely used, they demand extensive computational
resources and lengthy training time. Quantum computing offers a promising
alternative, but existing quantum-based image generation methods can only yield
grayscale low-quality images. Through a novel classical-quantum latent space
fusion technique, our work overcomes this limitation and introduces the first
classical-quantum generative adversarial network (GAN) capable of generating
color medical images. Our model outperforms classical deep convolutional GANs
and existing hybrid classical-quantum GANs in both image generation quality and
classification performance boost when used as data augmentation. Moreover, the
performance boost is comparable with that achieved using state-of-the-art
classical generative models, yet with over 25 times fewer parameters and 10
times fewer training epochs. Such results suggest a promising future for
quantum image generation as quantum hardware advances. Finally, we demonstrate
the robust performance of our model on real IBM quantum machine with hardware
noise.

</details>


### [199] [Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection](https://arxiv.org/abs/2506.21109)
*Luosheng Xu,Dalin Zhang,Zhaohui Song*

Main category: cs.CV

TL;DR: FlickCD是一种轻量级遥感变化检测模型，通过增强差异模块和多尺度特征融合，在降低计算资源消耗的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型复杂度高但精度提升有限，研究旨在开发高效轻量模型，适用于卫星端处理。

Method: 提出FlickCD，包含增强差异模块（EDM）和局部-全局融合块（SWSA与EGSA），优化特征差异提取和多尺度语义信息捕获。

Result: 在四个基准数据集上，FlickCD显著降低计算和存储开销，同时达到或接近SOTA性能。

Conclusion: FlickCD在性能与资源消耗间取得平衡，为遥感变化检测提供高效解决方案。

Abstract: Remote sensing change detection is essential for monitoring urban expansion,
disaster assessment, and resource management, offering timely, accurate, and
large-scale insights into dynamic landscape transformations. While deep
learning has revolutionized change detection, the increasing complexity and
computational demands of modern models have not necessarily translated into
significant accuracy gains. Instead of following this trend, this study
explores a more efficient approach, focusing on lightweight models that
maintain high accuracy while minimizing resource consumption, which is an
essential requirement for on-satellite processing. To this end, we propose
FlickCD, which means quick flick then get great results, pushing the boundaries
of the performance-resource trade-off. FlickCD introduces an Enhanced
Difference Module (EDM) to amplify critical feature differences between
temporal phases while suppressing irrelevant variations such as lighting and
weather changes, thereby reducing computational costs in the subsequent change
decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion
Blocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global
Self-Attention (EGSA) to efficiently capture semantic information at multiple
scales, preserving both coarse- and fine-grained changes. Extensive experiments
on four benchmark datasets demonstrate that FlickCD reduces computational and
storage overheads by more than an order of magnitude while achieving
state-of-the-art (SOTA) performance or incurring only a minor (<1\% F1)
accuracy trade-off. The implementation code is publicly available at
https://github.com/xulsh8/FlickCD.

</details>


### [200] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 论文提出了一种通过人体动作预测第一人称视角视频（PEVA）的方法，利用3D身体姿态轨迹训练自回归条件扩散变换器。


<details>
  <summary>Details</summary>
Motivation: 研究旨在模拟物理人类行为如何从第一人称视角塑造环境，解决复杂现实环境和具身代理行为的建模挑战。

Method: 基于Nymeria大规模数据集，训练自回归条件扩散变换器，结合人体关节层次结构。

Result: 设计了分层评估协议，全面分析模型的具身预测和控制能力。

Conclusion: 该研究是首次尝试从人类视角通过视频预测建模复杂现实环境和具身代理行为。

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.

</details>


### [201] [A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario](https://arxiv.org/abs/2506.21451)
*Cyrus Addy,Ajay Kumar Gurumadaiah,Yixiang Gao,Kwame Awuah-Offei*

Main category: cs.CV

TL;DR: 本文提出了一种专门用于地下采矿环境的热成像数据集，以支持矿工检测系统的开发和验证，并评估了多种先进目标检测算法的性能。


<details>
  <summary>Details</summary>
Motivation: 地下采矿作业的安全挑战需要可靠的矿工检测能力，但目前缺乏针对该环境的训练数据集。

Method: 通过系统采集采矿活动的热成像数据，构建数据集，并评估YOLOv8、YOLOv10、YOLO11和RT-DETR等算法的性能。

Result: 研究证明了热成像用于矿工检测的可行性，并为未来研究奠定了基础。

Conclusion: 该数据集是开发可靠热成像矿工检测系统的重要第一步，未来可应用于实际紧急情况。

Abstract: Underground mining operations face significant safety challenges that make
emergency response capabilities crucial. While robots have shown promise in
assisting with search and rescue operations, their effectiveness depends on
reliable miner detection capabilities. Deep learning algorithms offer potential
solutions for automated miner detection, but require comprehensive training
datasets, which are currently lacking for underground mining environments. This
paper presents a novel thermal imaging dataset specifically designed to enable
the development and validation of miner detection systems for potential
emergency applications. We systematically captured thermal imagery of various
mining activities and scenarios to create a robust foundation for detection
algorithms. To establish baseline performance metrics, we evaluated several
state-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,
and RT-DETR on our dataset. While not exhaustive of all possible emergency
situations, this dataset serves as a crucial first step toward developing
reliable thermal-based miner detection systems that could eventually be
deployed in real emergency scenarios. This work demonstrates the feasibility of
using thermal imaging for miner detection and establishes a foundation for
future research in this critical safety application.

</details>


### [202] [Evaluation of Traffic Signals for Daily Traffic Pattern](https://arxiv.org/abs/2506.21469)
*Mohammad Shokrolah Shirazi,Hung-Fu Chang*

Main category: cs.CV

TL;DR: 该论文提出三种基于转向运动计数（TMC）的交通信号配置方法（动态、静态和混合），并通过仿真验证其效果。实验表明，90秒和120秒的周期时间表现最佳，混合方法在高峰和非高峰时段适应性更强。


<details>
  <summary>Details</summary>
Motivation: 转向运动计数数据对交通信号设计、交叉口规划和拥堵分析至关重要，但现有方法可能无法适应多样化的交通模式。

Method: 开发了基于视觉的跟踪系统估计TMC，结合仿真工具SUMO评估信号配置。提出动态、静态和混合三种方法，并通过4小时仿真验证。

Result: 90秒和120秒周期时间表现最佳；动态配置在四个交叉口效果更好，混合方法适应高峰和非高峰时段。

Conclusion: 混合方法在交通分布不均时表现优异，静态方法适用于均匀分布。区域交通模式影响信号设计选择。

Abstract: The turning movement count data is crucial for traffic signal design,
intersection geometry planning, traffic flow, and congestion analysis. This
work proposes three methods called dynamic, static, and hybrid configuration
for TMC-based traffic signals. A vision-based tracking system is developed to
estimate the TMC of six intersections in Las Vegas using traffic cameras. The
intersection design, route (e.g. vehicle movement directions), and signal
configuration files with compatible formats are synthesized and imported into
Simulation of Urban MObility for signal evaluation with realistic data. The
initial experimental results based on estimated waiting times indicate that the
cycle time of 90 and 120 seconds works best for all intersections. In addition,
four intersections show better performance for dynamic signal timing
configuration, and the other two with lower performance have a lower ratio of
total vehicle count to total lanes of the intersection leg. Since daily traffic
flow often exhibits a bimodal pattern, we propose a hybrid signal method that
switches between dynamic and static methods, adapting to peak and off-peak
traffic conditions for improved flow management. So, a built-in traffic
generator module creates vehicle routes for 4 hours, including peak hours, and
a signal design module produces signal schedule cycles according to static,
dynamic, and hybrid methods. Vehicle count distributions are weighted
differently for each zone (i.e., West, North, East, South) to generate diverse
traffic patterns. The extended experimental results for 6 intersections with 4
hours of simulation time imply that zone-based traffic pattern distributions
affect signal design selection. Although the static method works great for
evenly zone-based traffic distribution, the hybrid method works well for highly
weighted traffic at intersection pairs of the West-East and North-South zones.

</details>


### [203] [Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection](https://arxiv.org/abs/2506.21486)
*Tobias J. Riedlinger,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 该论文提出了一种基于空间统计学的目标检测模型，旨在解决现有目标检测和分割模型中置信度估计不准确的问题，特别是在未检测到物体的区域。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测模型的置信度估计通常不准确，且无法量化未检测区域的障碍物不确定性，这在自动驾驶等应用中存在安全隐患。

Method: 论文提出了一种基于标记点过程的空间统计学框架，将边界框数据视为空间点事件的实现，并通过似然训练提供明确的置信度估计。

Result: 通过校准评估和性能测试，证明了该方法的有效性。

Conclusion: 该方法为未检测区域的障碍物不确定性提供了可靠的置信度估计，适用于自动驾驶等安全关键应用。

Abstract: Deep neural networks have set the state-of-the-art in computer vision tasks
such as bounding box detection and semantic segmentation. Object detectors and
segmentation models assign confidence scores to predictions, reflecting the
model's uncertainty in object detection or pixel-wise classification. However,
these confidence estimates are often miscalibrated, as their architectures and
loss functions are tailored to task performance rather than probabilistic
foundation. Even with well calibrated predictions, object detectors fail to
quantify uncertainty outside detected bounding boxes, i.e., the model does not
make a probability assessment of whether an area without detected objects is
truly free of obstacles. This poses a safety risk in applications such as
automated driving, where uncertainty in empty areas remains unexplored. In this
work, we propose an object detection model grounded in spatial statistics.
Bounding box data matches realizations of a marked point process, commonly used
to describe the probabilistic occurrence of spatial point events identified as
bounding box centers, where marks are used to describe the spatial extension of
bounding boxes and classes. Our statistical framework enables a
likelihood-based training and provides well-defined confidence estimates for
whether a region is drivable, i.e., free of objects. We demonstrate the
effectiveness of our method through calibration assessments and evaluation of
performance.

</details>


### [204] [Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval](https://arxiv.org/abs/2506.21538)
*Hani Alomari,Anushka Sivakumar,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 论文提出了一种改进的跨模态图像-文本检索方法，通过最大化配对相似性和引入两种损失函数来解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统单向量嵌入方法难以捕捉跨模态的多样关联，而基于集合的方法虽能捕获更丰富关系，但仍面临稀疏监督和集合坍缩问题。

Method: 提出Maximal Pair Assignment Similarity优化嵌入集合的一对一匹配，并引入Global Discriminative Loss和Intra-Set Divergence Loss增强表示。

Result: 在MS-COCO和Flickr30k上实现了最先进的性能，且无需外部数据。

Conclusion: 新方法有效解决了集合表示中的问题，显著提升了跨模态检索性能。

Abstract: Cross-modal image-text retrieval is challenging because of the diverse
possible associations between content from different modalities. Traditional
methods learn a single-vector embedding to represent semantics of each sample,
but struggle to capture nuanced and diverse relationships that can exist across
modalities. Set-based approaches, which represent each sample with multiple
embeddings, offer a promising alternative, as they can capture richer and more
diverse relationships. In this paper, we show that, despite their promise,
these set-based representations continue to face issues including sparse
supervision and set collapse, which limits their effectiveness. To address
these challenges, we propose Maximal Pair Assignment Similarity to optimize
one-to-one matching between embedding sets which preserve semantic diversity
within the set. We also introduce two loss functions to further enhance the
representations: Global Discriminative Loss to enhance distinction among
embeddings, and Intra-Set Divergence Loss to prevent collapse within each set.
Our method achieves state-of-the-art performance on MS-COCO and Flickr30k
without relying on external data.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [205] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič,Martin Vuk,Aljaž Zalar*

Main category: stat.ML

TL;DR: 本文通过解析方法解决了关于多元拟协变量质量分布极值的开放问题，并推翻了相关猜想。


<details>
  <summary>Details</summary>
Motivation: 尽管拟协变量在统计解释上存在不足，但其在依赖建模社区中的重要性日益增加，促使研究者解决相关开放问题。

Method: 采用解析方法，而非之前的线性规划方法，以更全面地回答问题。

Result: 完全解决了开放问题，并推翻了最近的一个猜想。

Conclusion: 解析方法为拟协变量研究提供了更完整的答案，推动了该领域的进展。

Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [206] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica,Kurt Hornik*

Main category: stat.ML

TL;DR: 提出了一种新型变分自编码器（VAE）架构，使用球形柯西（spCauchy）潜在分布，优于传统高斯或von Mises-Fisher（vMF）分布，提供更自然的超球面表示，避免数值不稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统VAE的高斯或vMF潜在分布在表示方向性数据时存在局限性，且vMF因涉及贝塞尔函数的归一化常数计算而数值不稳定。

Method: 采用spCauchy潜在分布，利用其重尾特性避免过正则化，并通过Möbius变换实现高效可微分的重参数化技巧。

Result: spCauchy提供了更灵活和高效的潜在空间表示，避免了vMF的数值不稳定性，且KL散度可通过快速收敛的幂级数计算。

Conclusion: spCauchy是VAE的一种理论优越且实际高效的替代方案，适用于高维生成建模。

Abstract: We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [207] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang,Dan Qiao,Yu-Xiang Wang,Rahul Parhi*

Main category: stat.ML

TL;DR: 研究了平坦性/低曲率在过参数化ReLU网络中的隐式偏差及其对泛化的影响，发现平坦解在高维下泛化性能指数级下降。


<details>
  <summary>Details</summary>
Motivation: 探索平坦性对泛化的影响，尤其是在高维输入下，以解释平坦最小值在高维中泛化失败的原因。

Method: 通过理论分析，针对平坦解的泛化差距和非参数函数估计中的MSE，提出上下界证明。

Result: 平坦解在高维下泛化性能指数级下降，与低范数解形成鲜明对比。

Conclusion: 平坦解在高维中因神经元稀疏激活但权重高而导致性能差，首次系统解释了平坦最小值在高维中泛化失败的原因。

Abstract: We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [208] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng,Lulu Kang,Yiwei Wang,Chun Liu*

Main category: stat.ML

TL;DR: 提出了一种结合流形学习和主动学习的高斯过程回归框架，通过联合优化降维神经网络和潜在空间的高斯过程回归器，提升高维空间中的预测精度。


<details>
  <summary>Details</summary>
Motivation: 高维空间中传统高斯过程回归的预测精度受限，需要结合流形学习和主动学习以提升性能。

Method: 联合优化降维神经网络和潜在空间的高斯过程回归器，采用主动学习准则最小化全局预测误差。

Result: 在合成数据上表现优于随机顺序学习，能高效处理复杂、不连续函数并保持计算可行性。

Conclusion: 该框架在科学和工程应用中具有实用价值，未来将关注可扩展性和不确定性感知的流形学习。

Abstract: This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [209] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn,Frederick Eberhardt,Leonard J. Schulman*

Main category: stat.ML

TL;DR: 论文研究了因果发现算法中马尔可夫等价类的规模问题，发现放宽假设条件会导致等价类规模指数级增长。


<details>
  <summary>Details</summary>
Motivation: 探讨在放宽无环性、因果充分性和均匀模型先验等假设条件下，马尔可夫等价类规模的变化。

Method: 通过理论分析，证明了在稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图三种设置下，马尔可夫等价类的期望规模呈指数级增长。

Result: 在放宽假设条件下，马尔可夫等价类的规模显著增大，表现为指数级下界。

Conclusion: 放宽假设条件会显著增加因果发现的不确定性，导致马尔可夫等价类规模指数级扩大。

Abstract: Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


### [210] [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics](https://arxiv.org/abs/2506.20935)
*Hsin-Hsiung Huang,Hayden Hampton*

Main category: stat.ML

TL;DR: STFT-VNNGP是一种混合架构，结合了TFT和VNNGP，解决了地缘政治冲突预测中的稀疏性和突发性问题，在2023年ATD竞赛中获胜。


<details>
  <summary>Details</summary>
Motivation: 地缘政治冲突预测对国家安全至关重要，但现有深度学习模型（如TFT）在长时预测中不可靠。

Method: 采用两阶段方法：TFT生成多分位数预测，VNNGP进行时空平滑和不确定性量化。

Result: 在中东和美国的冲突动态预测中，STFT-VNNGP表现优于单独TFT，尤其在长时预测中。

Conclusion: STFT-VNNGP为从复杂事件数据中生成可靠情报提供了框架，代码公开以确保可复现性。

Abstract: Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.

</details>


### [211] [Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games](https://arxiv.org/abs/2506.21079)
*Yann Kerzreho*

Main category: stat.ML

TL;DR: 本文提出了一种新方法，通过调整学习率和更新频率来近似多智能体强化学习在有限状态马尔可夫游戏中的动态。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在复杂环境中的动态行为，提供一种可处理的确定性近似方法。

Method: 通过同时降低学习率和增加更新频率，将智能体参数视为慢变量，受快混合游戏状态影响。

Result: 证明了在温和假设下，重缩放过程收敛到一个常微分方程（ODE），该ODE提供了学习动态的确定性近似。

Conclusion: 该方法为多智能体强化学习的动态行为提供了有效的近似工具。

Abstract: This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation

</details>


### [212] [Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)
*Martin J. Wainwright*

Main category: stat.ML

TL;DR: 提出一种高效的重拟合方法，通过残差对称化和缩放，为惩罚非参数估计提供高概率的预测误差上界。


<details>
  <summary>Details</summary>
Motivation: 解决在单一数据集和黑盒预测方法下，如何高效计算预测误差上界的问题。

Method: 基于最小二乘法的惩罚非参数估计，通过残差对称化、缩放和重新定义预测问题来实现。

Result: 在噪声异质性条件下，该方法能提供高概率的预测误差上界。

Conclusion: 该方法适用于多种问题，如非刚性运动恢复和图像修复，具有理论和实践指导意义。

Abstract: We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.

</details>


### [213] [Gaussian Invariant Markov Chain Monte Carlo](https://arxiv.org/abs/2506.21511)
*Michalis K. Titsias,Angelos Alexopoulos,Siran Liu,Petros Dellaportas*

Main category: stat.ML

TL;DR: 论文提出了基于高斯不变性的采样方法（RWM、MALA和二阶MALA），通过精确解析泊松方程提高统计效率，并在高维目标中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统RWM和MALA方法在统计效率上存在不足，高斯不变性为解决这一问题提供了新思路。

Method: 开发高斯不变版本的RWM、MALA和二阶MALA，利用泊松方程的解析解构建控制变量以减少方差。

Result: 新方法在高维目标中表现优异，优于多种先进方法，并提供了几何遍历性和最优接受率的理论分析。

Conclusion: 高斯不变性采样方法显著提升了统计效率，适用于复杂目标，具有理论和实践价值。

Abstract: We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [214] [Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis](https://arxiv.org/abs/2506.20806)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Main category: cs.CR

TL;DR: 提出了一种利用大型语言模型（LLM）增强图神经网络（GNN）在入侵检测中鲁棒性和泛化能力的新方法。


<details>
  <summary>Details</summary>
Motivation: GNN在物联网入侵检测中表现优异，但面临分布漂移和对抗攻击的挑战，现有评估方法不切实际。

Method: 通过LLM代理模拟网络安全专家，分析网络流数据生成的图结构，识别并缓解对抗性扰动。

Result: 实验表明，LLM分析显著提升了GNN在多种对抗攻击下的鲁棒性。

Conclusion: LLM可作为入侵检测架构的补充层，提升系统整体性能。

Abstract: Graph Neural Networks (GNNs) show great promise for Network Intrusion
Detection Systems (NIDS), particularly in IoT environments, but suffer
performance degradation due to distribution drift and lack robustness against
realistic adversarial attacks. Current robustness evaluations often rely on
unrealistic synthetic perturbations and lack demonstrations on systematic
analysis of different kinds of adversarial attack, which encompass both
black-box and white-box scenarios. This work proposes a novel approach to
enhance GNN robustness and generalization by employing Large Language Models
(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These
agents scrutinize graph structures derived from network flow data, identifying
and potentially mitigating suspicious or adversarially perturbed elements
before GNN processing. Our experiments, using a framework designed for
realistic evaluation and testing with a variety of adversarial attacks
including a dataset collected from physical testbed experiments, demonstrate
that integrating LLM analysis can significantly improve the resilience of
GNN-based NIDS against challenges, showcasing the potential of LLM agent as a
complementary layer in intrusion detection architectures.

</details>


### [215] [ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models](https://arxiv.org/abs/2506.20915)
*Mina Namazi,Alexander Nemecek,Erman Ayday*

Main category: cs.CR

TL;DR: ZKPROV是一个新型加密框架，通过零知识证明验证大语言模型（LLM）的数据来源，确保模型训练数据的可靠性而不泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域（如医疗）部署LLM时，确保其计算来源的完整性至关重要，但现有方法要么计算成本高，要么依赖可信执行环境。

Method: ZKPROV通过零知识证明将训练模型与授权数据集绑定，利用数据集签名元数据和紧凑模型参数承诺，避免验证每一步训练过程。

Result: 实验证明ZKPROV在生成和验证证明时高效且可扩展，适用于实际部署。

Conclusion: ZKPROV在保护数据集机密性的同时，提供了可信的数据来源保证。

Abstract: As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.

</details>


### [216] [PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction](https://arxiv.org/abs/2506.21106)
*Felipe Castaño,Eduardo Fidalgo,Enrique Alegre,Rocio Alaiz-Rodríguez,Raul Orduna,Francesco Zola*

Main category: cs.CR

TL;DR: PhishKey是一种新型钓鱼检测方法，结合字符级CNN和CAPE模块，通过软投票集成提高分类准确性，实验显示其F1分数高达98.70%。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击快速演变，绕过检测机制并利用人类弱点，需要一种适应性强、鲁棒且高效的检测方法。

Method: PhishKey结合字符级CNN处理URL分类和CAPE模块处理HTML内容，通过软投票集成预测结果。

Result: 在四个数据集上测试，F1分数达98.70%，对抗攻击表现优异。

Conclusion: PhishKey在钓鱼检测中表现出高效性和鲁棒性，适用于实际应用。

Abstract: Phishing attacks pose a significant cybersecurity threat, evolving rapidly to
bypass detection mechanisms and exploit human vulnerabilities. This paper
introduces PhishKey to address the challenges of adaptability, robustness, and
efficiency. PhishKey is a novel phishing detection method using automatic
feature extraction from hybrid sources. PhishKey combines character-level
processing with Convolutional Neural Networks (CNN) for URL classification, and
a Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at
the word level. CAPE reduces noise and ensures complete sample processing
avoiding crop operations on the input data. The predictions from both modules
are integrated using a soft-voting ensemble to achieve more accurate and
reliable classifications. Experimental evaluations on four state-of-the-art
datasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1
Score and shows strong resistance to adversarial manipulations such as
injection attacks with minimal performance degradation.

</details>


### [217] [Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research](https://arxiv.org/abs/2506.20872)
*Osama Zafar,Rosemarie Santa González,Mina Namazi,Alfonso Morales,Erman Ayday*

Main category: cs.CR

TL;DR: 提出了一种隐私保护框架，结合降维技术和差分隐私，支持农民安全共享数据并促进协作，验证了其隐私保护和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决农民因隐私问题不愿共享数据的障碍，推动数据驱动农业的发展。

Method: 结合主成分分析（PCA）和拉普拉斯噪声的差分隐私技术，支持联邦学习和协作识别。

Result: 在真实数据集上验证了框架的隐私保护和实用性，性能接近集中式系统。

Conclusion: 该框架有助于农民协作和研究人员目标，推动农业数据的安全整合与创新。

Abstract: Data-driven agriculture, which integrates technology and data into
agricultural practices, has the potential to improve crop yield, disease
resilience, and long-term soil health. However, privacy concerns, such as
adverse pricing, discrimination, and resource manipulation, deter farmers from
sharing data, as it can be used against them. To address this barrier, we
propose a privacy-preserving framework that enables secure data sharing and
collaboration for research and development while mitigating privacy risks. The
framework combines dimensionality reduction techniques (like Principal
Component Analysis (PCA)) and differential privacy by introducing Laplacian
noise to protect sensitive information. The proposed framework allows
researchers to identify potential collaborators for a target farmer and train
personalized machine learning models either on the data of identified
collaborators via federated learning or directly on the aggregated
privacy-protected data. It also allows farmers to identify potential
collaborators based on similarities. We have validated this on real-life
datasets, demonstrating robust privacy protection against adversarial attacks
and utility performance comparable to a centralized system. We demonstrate how
this framework can facilitate collaboration among farmers and help researchers
pursue broader research objectives. The adoption of the framework can empower
researchers and policymakers to leverage agricultural data responsibly, paving
the way for transformative advances in data-driven agriculture. By addressing
critical privacy challenges, this work supports secure data integration,
fostering innovation and sustainability in agricultural systems.

</details>
